{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import jieba\n",
    "from jieba import posseg as pseg\n",
    "from collections import defaultdict, Counter\n",
    "import random\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "NEG = 'N'\n",
    "POS = 'P'\n",
    "\n",
    "train_files = {}\n",
    "train_files[NEG] = 'neg_train.txt'\n",
    "train_files[POS] = 'pos_train.txt'\n",
    "\n",
    "test_files = {}\n",
    "test_files[NEG] = 'neg_test.txt'\n",
    "test_files[POS] = 'pos_test.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_file(file_name):\n",
    "    f = open(file_name, 'r')\n",
    "    lines = f.readlines()\n",
    "    f.close()\n",
    "    segs = []        \n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        words = pseg.cut(line)\n",
    "        for (key, flag) in words:\n",
    "            if flag == 'x':\n",
    "                continue           \n",
    "            segs.append(key)            \n",
    "    return segs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def load_train_dataset(input_data):\n",
    "    segs = {}\n",
    "    for k, v in input_data.items():\n",
    "        segs[k] = load_file(v)\n",
    "    return segs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def calc_word_prob(segs):\n",
    "    lm = defaultdict(Counter)\n",
    "    for k,v in segs.items():\n",
    "        for word in v:\n",
    "            lm[k][word] += 1\n",
    "    return lm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def normalize_prob(lm_cnt):\n",
    "   \n",
    "    for key, cnt in lm_cnt.items():\n",
    "        s = float(sum(cnt.values()))\n",
    "        \n",
    "        for word in cnt:\n",
    "            cnt[word] /= s\n",
    "    return lm_cnt    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#计算每种情绪本身的概率 P(emotion)\n",
    "def get_emotion_prob(lm):\n",
    "    prob = {}    \n",
    "    for k, v in lm.items():\n",
    "        prob[k] = len(v)        \n",
    "    \n",
    "    s = float(sum(prob.values()))\n",
    "    for k, v in prob.items():\n",
    "        prob[k] /= s     \n",
    "    return prob  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def create_model(input_data):\n",
    "    print 'Loading training data...'\n",
    "    segs = load_train_dataset(input_data) #读入语料库，建立分词列表\n",
    "    \n",
    "    print 'Training data size by word:'\n",
    "    for k, v in segs.items():\n",
    "        print k, len(v)\n",
    "        \n",
    "    print 'Calculating word prob...'\n",
    "    \n",
    "    lm = calc_word_prob(segs) #统计词频, 计算 P(w|emotion)\n",
    "    \n",
    "    print 'Normalizing...'\n",
    "    lm = normalize_prob(lm)   #归一化\n",
    "    \n",
    "    print 'Calculating emotion prob...'\n",
    "    prob = get_emotion_prob(lm) #计算情绪独立概率 P(emotion)\n",
    "    \n",
    "    print 'Emotion prob:'\n",
    "    for k,v in prob.items():\n",
    "        print k, v\n",
    "        \n",
    "    print '-------- Model created---------'    \n",
    "    for k,v in lm.items():        \n",
    "        print k, len(v.items())\n",
    "        #for word, prob in v.most_common()[:10]:\n",
    "         #   print word, prob\n",
    "    return lm, prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_file_by_lines(file_name):\n",
    "    f = open(file_name, 'r')\n",
    "    lines = f.readlines()\n",
    "    f.close()\n",
    "    return lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calc_line_prob(line_cnt, lm, e_p):\n",
    "    s = [len(lm_dict[NEG].items()), len(lm_dict[POS].items())]\n",
    "    p_neg = 1.0 * e_p[NEG] * s[0]\n",
    "    p_pos = 1.0 * e_p[POS] * s[1]\n",
    "\n",
    "    for word, v in line_cnt.items():        \n",
    "        p_pos *= lm[POS][word] * s[1] * v\n",
    "        p_neg *= lm[NEG][word] * s[0] * v\n",
    "        \n",
    "    if ((p_neg + p_pos) > 1.0e-6):        \n",
    "        p_neg = p_neg / (p_neg + p_pos) \n",
    "        p_pos = 1.0 - p_neg\n",
    "        \n",
    "    return [p_neg, p_pos]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def classify_line(line, lm, e_p):\n",
    "    segs = pseg.cut(line.strip())\n",
    "    cnt = Counter()\n",
    "    for key, flag in segs:\n",
    "        if flag == 'x':\n",
    "            continue\n",
    "        cnt[key] += 1    \n",
    "    \n",
    "    return calc_line_prob(cnt, lm, e_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def classify_lines(lines, lm, e_p):\n",
    "    tags = []\n",
    "    for line in lines:\n",
    "        tags.append(classify_line(line, lm, e_p))\n",
    "    return tags    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calc_confusion_matrix(score_neg, score_pos):\n",
    "    neg_cnt = len(score_neg)\n",
    "    pos_cnt = len(score_pos)\n",
    "    total_cnt = neg_cnt + pos_cnt\n",
    "    \n",
    "    neg = np.argmax(score_neg, 1)\n",
    "    pos = np.argmin(score_pos, 1)\n",
    "    \n",
    "    fn = sum(neg)\n",
    "    fp = sum(pos)\n",
    "    \n",
    "    error = 1.0 * (fn + fp) / total_cnt\n",
    "    accuracy = 1.0 - error\n",
    "    tp = pos_cnt - fp\n",
    "    tn = neg_cnt - fn\n",
    "    tp_rate = 1.0 - 1.0 * fp / pos_cnt\n",
    "    fp_rate = 1.0 * fn / neg_cnt\n",
    "    specity = 1.0 - fp_rate\n",
    "    precision = 1.0 * tp /(pos_cnt + fn)\n",
    "    prevalance = 1.0 * pos_cnt / total_cnt    \n",
    "   \n",
    "    return [total_cnt, neg_cnt, pos_cnt,\n",
    "            tn, fn, tp, fp, accuracy, \n",
    "            tp_rate, fp_rate, specity,\n",
    "            precision, prevalance]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def load_test_dataset(files):\n",
    "    segs = {}\n",
    "    for k, v in files.items():\n",
    "        segs[k] = load_file_by_lines(v)\n",
    "    return segs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def classify_test_data(lm, e_prob, test_data):\n",
    "    probs = {}\n",
    "    for k, v in test_data.items():\n",
    "        probs[k] = classify_lines(test_data[k], lm, e_prob)\n",
    "    return probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def calc_confusion_matrix(scores):\n",
    "    score_neg = scores[NEG]\n",
    "    score_pos = scores[POS]\n",
    "    \n",
    "    neg_cnt = len(score_neg)\n",
    "    pos_cnt = len(score_pos)\n",
    "    total_cnt = neg_cnt + pos_cnt\n",
    "    \n",
    "    neg = np.argmax(score_neg, 1)\n",
    "    pos = np.argmin(score_pos, 1)\n",
    "    \n",
    "    fn = sum(neg)\n",
    "    fp = sum(pos)    \n",
    "    \n",
    "    error = 1.0 * (fn + fp) / total_cnt\n",
    "    accuracy = 1.0 - error\n",
    "    tp = pos_cnt - fp\n",
    "    tn = neg_cnt - fn\n",
    "    tp_rate = 1.0 - 1.0 * fp / pos_cnt\n",
    "    fp_rate = 1.0 * fn / neg_cnt\n",
    "    specity = 1.0 - fp_rate\n",
    "    precision = 1.0 * tp /(pos_cnt + fn)\n",
    "    prevalance = 1.0 * pos_cnt / total_cnt\n",
    "    \n",
    "    matrix = {}\n",
    "    \n",
    "    matrix['Total'] = total_cnt\n",
    "    matrix['Acutal N'] = neg_cnt\n",
    "    matrix['Acutal P'] = pos_cnt    \n",
    "    matrix['TP'] = tp\n",
    "    matrix['TN'] = tn\n",
    "    matrix['FP'] = fp\n",
    "    matrix['FN'] = fn \n",
    "    matrix['Accuracy'] = accuracy\n",
    "    matrix['TP rate'] = tp_rate\n",
    "    matrix['FP rate'] = fp_rate\n",
    "    matrix['Specity'] = specity\n",
    "    matrix['Precision'] = precision\n",
    "    matrix['Prevalance'] = prevalance\n",
    "      \n",
    "    return matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test_model(lm, e_prob, files):\n",
    "    print 'Loading test dataset...'\n",
    "    test_data = load_test_dataset(files)\n",
    "    \n",
    "    print 'Total test data:'\n",
    "    for k,v in test_data.items():\n",
    "        print k, len(v)\n",
    "    \n",
    "    print 'Classifing test data...'\n",
    "    scores = classify_test_data(lm, e_prob, test_data)\n",
    "    \n",
    "    print 'Calcutating confusion matrix...'\n",
    "    matrix = calc_confusion_matrix(scores)\n",
    "    \n",
    "    print '------- Test result --------'\n",
    "    for k ,v in matrix.items():\n",
    "        print k, v\n",
    "    \n",
    "    return matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def print_confusion_matrix(matrix):\n",
    "    print '------ Confusion matrix --------'\n",
    "    tags = ['Total', 'Accuracy', 'Precision', \n",
    "            'TP', 'FP', 'TN', 'FN',\n",
    "            'TP rate', 'FP rate',\n",
    "           'Specity', 'Prevalance']\n",
    "    for tag in tags:\n",
    "        print tag, matrix[tag]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading training data...\n",
      "Training data size by word:\n",
      "P 508469\n",
      "N 449708\n",
      "Calculating word prob...\n",
      "Normalizing...\n",
      "Calculating emotion prob...\n",
      "Emotion prob:\n",
      "P 0.561192278117\n",
      "N 0.438807721883\n",
      "-------- Model created---------\n",
      "P 27733\n",
      "N 21685\n"
     ]
    }
   ],
   "source": [
    "lm_dict, e_prob = create_model(train_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading test dataset...\n",
      "Total test data:\n",
      "P 4965\n",
      "N 5573\n",
      "Classifing test data...\n",
      "Calcutating confusion matrix...\n",
      "------- Test result --------\n",
      "FP 207\n",
      "Prevalance 0.471152021256\n",
      "FP rate 0.22124528979\n",
      "Specity 0.77875471021\n",
      "Acutal N 5573\n",
      "TP rate 0.9583081571\n",
      "TP 4758\n",
      "Precision 0.767666989351\n",
      "TN 4340\n",
      "Acutal P 4965\n",
      "Total 10538\n",
      "FN 1233\n",
      "Accuracy 0.863351679636\n"
     ]
    }
   ],
   "source": [
    "test_result = test_model(lm_dict, e_prob, test_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Confusion matrix --------\n",
      "Total 10538\n",
      "Accuracy 0.863351679636\n",
      "Precision 0.767666989351\n",
      "TP 4758\n",
      "FP 207\n",
      "TN 4340\n",
      "FN 1233\n",
      "TP rate 0.9583081571\n",
      "FP rate 0.22124528979\n",
      "Specity 0.77875471021\n",
      "Prevalance 0.471152021256\n"
     ]
    }
   ],
   "source": [
    "print_confusion_matrix(test_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
