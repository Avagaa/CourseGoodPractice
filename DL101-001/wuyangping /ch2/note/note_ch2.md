# 任务1：贝叶斯公式的运用
- Q:　利用贝叶斯公式，说明为什么 P(y|w1, w2) ≠ P(y|w1)P(y|w2) （即使做了独立假设）
- A: P (y|w1, w2) = P(y, w1, w2) / P(w1, Pw2) = P(w1, w2|y)P(y) / P(w1, w2)

     P(y|w1) = P(y, w1) / P(w1) = P(w1|y) P(y) / P(w1)

     同理，P(y|w2) = P(w2|y) P(y) / P(w2)

     **假设w1, w2为相互独立事件**，则：

     P(w1, w2) = P(w1) P(w2)

     P(w1, w2|y) = P(w1|y) P(w2|y)     

     因此，

     左项为：P(y|w1, w2) = P(w1|y )P(w2|y) P(y) / (P(w1) P(w2))

     右项为：P(y|w1) P(y|w2)　= P(w1|y) P(w2|y) P(y) P(y) / (P(w1) P(w2))

     右项 / 左项 = P(y) ，而P(y) < 1,

     因此 P(y|w1, w2) ≠ P(y|w1)P(y|w2)

     **TODO：w1，w2为非独立事件时的证明**

# 任务2：实现 Naive Bayes 方法
请你用 Python 实现 Naive Bayes 方法，并在给定的数据集上验证数据。具体要求如下：
在「训练数据」上拟合一个 Naive Bayes 模型。在训练时模型不能「看见」任何测试数据的信息。
训练完成后，在测试数据上进行测试。评估标准为你的模型在测试数据上的混淆矩阵（Confusion Matrix）结果。
根据混淆矩阵的结果，分析一下你模型的表现
笔记：
1. Naive Bayes计算量小，效果还不错。
2. 目前实现的模型在做预测时，对于一句话里充分出现的词只计算一次概率。
3. 计算概率P(w|emotion)时，因为每个词的独立出现概率都很小（<1），多个词的概率连成后出现很多很小的数，对P_neg 和 P_pos做了归一化处理。

# 任务2：实现 Gradient Descent 算法
通过梯度下降法，自己实现一种通用的给定数据找到 y = wx + b 中最优的 w 和 b 的程序，并用加噪音数据验证效果。
参考资料：http://blog.csdn.net/jizhg/article/details/47282175
收获：
 - 一开始计算梯度时代码因为对numpy数组的点乘（dot）函数使用有误，梯度计算错误，导致模型无法收敛，更正后收敛很快
 - 通过绘制Cost下降的曲线，可以清晰看到不同Learning rate下cost下降的速度，步长越小收敛越慢，但太大的话可能导致无法收敛
 - 最好的效果是开始时用较大的步长，然后逐步减小步长，如每迭代500次后缩小到0.8倍，效果明显提升，即具备初始时大步长收敛快的优点，又能在后期实现收敛
