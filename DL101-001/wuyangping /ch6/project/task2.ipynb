{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The watermark extension is already loaded. To reload it, use:\n",
      "  %reload_ext watermark\n",
      "2017-04-22T07:57:01\n",
      "\n",
      "CPython 2.7.6\n",
      "IPython 5.2.2\n",
      "\n",
      "compiler   : GCC 4.8.4\n",
      "system     : Linux\n",
      "release    : 4.4.43-boot2docker\n",
      "machine    : x86_64\n",
      "processor  : x86_64\n",
      "CPU cores  : 1\n",
      "interpreter: 64bit\n"
     ]
    }
   ],
   "source": [
    "%load_ext watermark\n",
    "%watermark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "import jieba\n",
    "from jieba import posseg as pseg\n",
    "from collections import Counter, defaultdict\n",
    "import random\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from scipy.spatial.distance import cosine\n",
    "import time\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "RAW_FILE = 'happiness.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Loading model cost 0.314 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "word_cnt = Counter()\n",
    "segs = []\n",
    "ngram = 2\n",
    "DELIMITER = u'.'\n",
    "with open(RAW_FILE) as f:\n",
    "    lines = f.readlines()\n",
    "    for line in lines:\n",
    "        line = line.strip().decode('utf-8')\n",
    "        words = pseg.cut(line)\n",
    "        for word, flag in words:            \n",
    "            if flag == 'x':\n",
    "                segs.append(DELIMITER)\n",
    "            else:\n",
    "                segs.append(word)                \n",
    "                word_cnt[word] += 1 \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "259688\n",
      "15899\n",
      "的 22805\n",
      "是 4335\n",
      "在 3525\n",
      "他 2562\n",
      "了 2273\n",
      "人 2245\n",
      "他们 1811\n",
      "和 1729\n",
      "这 1611\n",
      "有 1534\n"
     ]
    }
   ],
   "source": [
    "print(len(segs))\n",
    "print(len(word_cnt))\n",
    "\n",
    "for (w,v) in word_cnt.most_common()[:10]: \n",
    "    print w,v  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15899\n",
      "3520\n"
     ]
    }
   ],
   "source": [
    "print(len(word_cnt))\n",
    "\n",
    "vocab_dict = []\n",
    "vocab_dict.append('Unknown')\n",
    "\n",
    "for (w,v) in word_cnt.items():\n",
    "    if v> 5:\n",
    "        vocab_dict.append(w)\n",
    "        \n",
    "vocab_size = len(vocab_dict)\n",
    "print(vocab_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unknown\n",
      "居于\n",
      "爱慕\n",
      "出来\n",
      "第二\n",
      "还要\n",
      "肉体上\n",
      "少数几个\n",
      "谈\n",
      "看作\n"
     ]
    }
   ],
   "source": [
    "for w in vocab_dict[:10]:\n",
    "    print w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def get_word_index(word, dicts):\n",
    "    index = -1\n",
    "    if word in dicts:\n",
    "        index = dicts.index(word)\n",
    "    elif not word == DELIMITER:\n",
    "        index = 0 #未命中词\n",
    "    else:\n",
    "        index = -1 #delimiter, context 输入时忽略，delimiter前后的词被切断，不作为context输入    \n",
    "    \n",
    "    return index   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def build_seg_index(raw_segs, dicts):\n",
    "    seg_index = []\n",
    "    for seg in raw_segs:\n",
    "        seg_index.append(get_word_index(seg, dicts))\n",
    "    return seg_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[267, 2237, -1, 1385, 2273, 690, 2988, 3304, 3445, 3467]\n"
     ]
    }
   ],
   "source": [
    "segs_index = build_seg_index(segs, vocab_dict)\n",
    "\n",
    "print(segs_index[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def get_input_data(seg_index):\n",
    "    contexts = []\n",
    "    labels = []\n",
    "    for i in range(len(seg_index)-2):\n",
    "        if (seg_index[i] < 0):\n",
    "            continue\n",
    "        if (seg_index[i+1] < 0):            \n",
    "            continue\n",
    "        contexts.append([seg_index[i], seg_index[i+1]])\n",
    "        labels.append(max(0, seg_index[i+2]))\n",
    "    return (contexts, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "192236\n",
      "192236\n",
      "[[267, 2237], [1385, 2273], [2273, 690], [690, 2988], [2988, 3304], [3304, 3445], [3445, 3467], [3467, 1453], [1453, 2239], [1805, 3165]]\n",
      "[0, 690, 2988, 3304, 3445, 3467, 1453, 2239, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "#input_data = get_input_data(segs_index)\n",
    "(input_context, input_labels) = get_input_data(segs_index)\n",
    "\n",
    "print(len(input_context))\n",
    "print(len(input_labels))\n",
    "\n",
    "print(input_context[:10])\n",
    "print(input_labels[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1923\n"
     ]
    }
   ],
   "source": [
    "train_data_size = len(input_context)\n",
    "batch_size = 100\n",
    "\n",
    "batch_per_round = (train_data_size / batch_size) + 1\n",
    "print batch_per_round"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# 决定了 embedding 的维度 （隐层节点数）\n",
    "word_embedding_dim = 50\n",
    "# 决定了词表数量, 预留一个未登录词\n",
    "\n",
    "UNK_IDX = 0\n",
    "\n",
    "# 这里需要把 Word embedding 放到 Variable 里面。因为 Word embedding 是要随机初始化，跟着数据不断变化的。\n",
    "# 它相当于普通神经网络中的权重。\n",
    "\n",
    "# 在梯度下降时， tensorflow 的 Optimizer 会自动找到 Graph 中的 Variable，计算梯度并进行更新。\n",
    "word_embedding = tf.Variable(tf.random_uniform([vocab_size, word_embedding_dim]))\n",
    "\n",
    "# placeholder 和 variable 基本都可以当做 Tensor 来用\n",
    "# 注意这里的输入是 int32 类型，表示一个词 ID。这里我们需要对数据进行预处理，以把高频词映射到 [1, 80000] 之间，不在词表里面的词设置成 UNK, ID 为 0\n",
    "# 这里我们假设输入是两个词\n",
    "\n",
    "# 这里 Shape 的第一维我们指定为 None，是表示第一维可以根据数据进行变化，因此同样一个程序可以适应梯度下降时不同的 batch_size\n",
    "input_data = tf.placeholder(tf.int32, shape=[None, 2], name='input_data')\n",
    "\n",
    "input_embeds = tf.nn.embedding_lookup(word_embedding, input_data)\n",
    "\n",
    "# reduce 开头的函数一般有一个 axis 参数，决定按行、按列或者按整个矩阵进行 reduce\n",
    "context_embeds = tf.reduce_sum(input_embeds, axis=1)\n",
    "\n",
    "# 激活之前的输出\n",
    "raw_output = tf.layers.dense(context_embeds, vocab_size)\n",
    "# 加 softmax 之后的输出\n",
    "output = tf.nn.softmax(raw_output)\n",
    "\n",
    "# 样本的 labels 也需要用 placeholder 放置\n",
    "labels = tf.placeholder(tf.int32, shape=[None], name='labels')\n",
    "\n",
    "# 因为我们每个样本的 label 只有一个，使用稠密的 softmax 算 cost 及求导太浪费了。这里使用 sparse 版本即可。\n",
    "# 如果你的 label 是完整的 N 个词上的概率分布，这时候可以使用 tf.nn.softmax_cross_entropy_with_logits\n",
    "cost = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=raw_output, labels=labels)\n",
    "\n",
    "train_step = tf.train.GradientDescentOptimizer(0.01).minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0\n",
      "Cost: 7.197510, 3.314429\n",
      "Probability: 0.000083\n",
      "------\n",
      "epoch 1\n",
      "Cost: 6.664127, 2.815690\n",
      "Probability: 0.000041\n",
      "------\n",
      "epoch 2\n",
      "Cost: 6.451363, 2.563116\n",
      "Probability: 0.000023\n",
      "------\n",
      "epoch 3\n",
      "Cost: 6.255911, 2.397075\n",
      "Probability: 0.000014\n",
      "------\n",
      "epoch 4\n",
      "Cost: 6.064359, 2.239144\n",
      "Probability: 0.000010\n",
      "------\n",
      "epoch 5\n",
      "Cost: 5.884868, 2.072106\n",
      "Probability: 0.000008\n",
      "------\n",
      "epoch 6\n",
      "Cost: 5.726837, 1.917528\n",
      "Probability: 0.000007\n",
      "------\n",
      "epoch 7\n",
      "Cost: 5.583937, 1.778972\n",
      "Probability: 0.000007\n",
      "------\n",
      "epoch 8\n",
      "Cost: 5.449971, 1.657970\n",
      "Probability: 0.000006\n",
      "------\n",
      "epoch 9\n",
      "Cost: 5.319761, 1.556361\n",
      "Probability: 0.000005\n",
      "------\n",
      "Softmax Training duration=624.91\n"
     ]
    }
   ],
   "source": [
    "epoches = 10\n",
    "display_steps = 1\n",
    "\n",
    "cost0 = []\n",
    "cost1 = []\n",
    "\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    train_start = time.time()\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for i in range(epoches):        \n",
    "        data_ptr = 0        \n",
    "        for j in range(batch_per_round):\n",
    "            if j == batch_per_round-1:\n",
    "                data_ptr = train_data_size - batch_size\n",
    "            feed_dict = {input_data: input_context[data_ptr:data_ptr+batch_size],\n",
    "                             labels: input_labels[data_ptr:data_ptr+batch_size]}\n",
    "            data_ptr += batch_size\n",
    "            \n",
    "            sess.run(train_step, feed_dict=feed_dict)\n",
    "            \n",
    "        if (i % display_steps) == 0:\n",
    "            print(\"epoch %d\" %i)\n",
    "            cost_train = cost.eval(feed_dict=feed_dict)\n",
    "            print(\"Cost: %f, %f\" % (cost_train[0], cost_train[1]))\n",
    "            cost0.append(cost_train[0])\n",
    "            cost1.append(cost_train[1])\n",
    "            print(\"Probability: %f\" % output.eval(feed_dict=feed_dict)[0, 3])\n",
    "            print(\"------\")            \n",
    "        \n",
    "    word_embedding_final = sess.run(word_embedding)\n",
    "    output_final = output.eval(feed_dict=feed_dict)\n",
    "    \n",
    "    train_end = time.time()\n",
    "    duration = train_end - train_start\n",
    "    print(\"Softmax Training duration=%.2f\" % duration)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f9f7c110c90>]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAW4AAAD8CAYAAABXe05zAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAACoVJREFUeJzt212I5fddx/HPt1kSTZTmaVqrq27qRaERbMtQKUppG/sQ\nH2KpvYgg1orkwiI+UDQlXtSHCxsRqwjKEpCCtqmNBqRCMVUDXlVmk4ipaUyySWhi20xU1KbYUPv1\nYk7I7DqbObszZ2a/2dcLDnPO+f/m8P3tgfce/uc/1d0BYI6XHPYAAJwd4QYYRrgBhhFugGGEG2AY\n4QYYRrgBhhFugGGEG2CYI6t40auvvrqPHTu2ipcGeFE6ceLE0929tszalYT72LFj2djYWMVLA7wo\nVdXjy651qgRgGOEGGEa4AYYRboBhhBtgGOEGGEa4AYYRboBhhBtgGOEGGEa4AYYRboBhhBtgGOEG\nGEa4AYYRboBhhBtgGOEGGEa4AYYRboBhhBtgGOEGGEa4AYYRboBhhBtgGOEGGEa4AYYRboBhhBtg\nGOEGGEa4AYYRboBhhBtgGOEGGEa4AYYRboBhhBtgGOEGGEa4AYYRboBhdg13Vb2qqu7bdvuvqvqF\ngxgOgP/vyG4LuvvBJK9Jkqq6KMmTSe5c8VwAnMHZniq5Lskj3f34KoYBYHdnG+4bk3xspwNVdVNV\nbVTVxubm5t4nA2BHS4e7qi5OckOST+x0vLuPd/d6d6+vra3t13wAnOZsPnFfn+Se7v7SqoYBYHdn\nE+4fzxlOkwBwcJYKd1VdluStSf5iteMAsJtdLwdMku5+JslVK54FgCX4y0mAYYQbYBjhBhhGuAGG\nEW6AYYQbYBjhBhhGuAGGEW6AYYQbYBjhBhhGuAGGEW6AYYQbYBjhBhhGuAGGEW6AYYQbYBjhBhhG\nuAGGEW6AYYQbYBjhBhhGuAGGEW6AYYQbYBjhBhhGuAGGEW6AYYQbYBjhBhhGuAGGEW6AYYQbYBjh\nBhhGuAGGEW6AYZYKd1VdXlV3VNXnquqBqnrDqgcDYGdHllz3e0k+1d3vrqqLk1y6wpkAeAG7hruq\nXprkjUl+Kkm6+9kkz652LADOZJlTJdck2Uzyx1V1b1XdVlWXrXguAM5gmXAfSfK6JH/Y3a9N8kyS\nm09fVFU3VdVGVW1sbm7u85gAPGeZcD+R5Inu/szi8R3ZCvkpuvt4d6939/ra2tp+zgjANruGu7u/\nmOTzVfWqxVPXJfnnlU4FwBkte1XJzyX508UVJSeTvHd1IwHwQpYKd3ffl2R9xbMAsAR/OQkwjHAD\nDCPcAMMIN8Awwg0wjHADDCPcAMMIN8Awwg0wjHADDCPcAMMIN8Awwg0wjHADDCPcAMMIN8Awwg0w\njHADDCPcAMMIN8Awwg0wjHADDCPcAMMIN8Awwg0wjHADDCPcAMMIN8Awwg0wjHADDCPcAMMIN8Aw\nwg0wjHADDCPcAMMIN8Awwg0wjHADDHNkmUVV9ViS/07yv0m+1t3rqxwKgDNbKtwLb+7up1c2CQBL\ncaoEYJhlw91J/rqqTlTVTTstqKqbqmqjqjY2Nzf3b0IATrFsuL+/u1+X5Pok76uqN56+oLuPd/d6\nd6+vra3t65AAPG+pcHf3k4ufTyW5M8nrVzkUAGe2a7ir6rKq+ubn7id5W5L7Vz0YADtb5qqSlye5\ns6qeW//R7v7USqcC4Ix2DXd3n0zyPQcwCwBLcDkgwDDCDTCMcAMMI9wAwwg3wDDCDTCMcAMMI9wA\nwwg3wDDCDTCMcAMMI9wAwwg3wDDCDTCMcAMMI9wAwwg3wDDCDTCMcAMMI9wAwwg3wDDCDTCMcAMM\nI9wAwwg3wDDCDTCMcAMMI9wAwwg3wDDCDTCMcAMMI9wAwwg3wDDCDTCMcAMMI9wAwwg3wDBLh7uq\nLqqqe6vqk6scCIAXdjafuH8+yQOrGgSA5SwV7qo6muSHkty22nEA2M2yn7g/nOSXk3x9hbMAsIRd\nw11VP5zkqe4+scu6m6pqo6o2Njc3921AAE61zCfu70tyQ1U9luT2JG+pqj85fVF3H+/u9e5eX1tb\n2+cxAXjOruHu7g9099HuPpbkxiR/290/sfLJANiR67gBhjlyNou7++4kd69kEgCW4hM3wDDCDTCM\ncAMMI9wAwwg3wDDCDTCMcAMMI9wAwwg3wDDCDTCMcAMMI9wAwwg3wDDCDTCMcAMMI9wAwwg3wDDC\nDTCMcAMMI9wAwwg3wDDCDTCMcAMMI9wAwwg3wDDCDTCMcAMMI9wAw1R37/+LVm0meXzfX3i1rk7y\n9GEPccDs+cJgzzN8Z3evLbNwJeGeqKo2unv9sOc4SPZ8YbDnFx+nSgCGEW6AYYT7eccPe4BDYM8X\nBnt+kXGOG2AYn7gBhrmgwl1VV1bVXVX10OLnFWdY957Fmoeq6j07HP/Lqrp/9RPv3V72XFWXVtVf\nVdXnquqzVfVbBzv92amqd1TVg1X1cFXdvMPxS6rq44vjn6mqY9uOfWDx/INV9faDnPtcnet+q+qt\nVXWiqv5p8fMtBz37udrLe7w4/h1V9eWqev9BzbwS3X3B3JLcmuTmxf2bk3xohzVXJjm5+HnF4v4V\n246/K8lHk9x/2PtZ9Z6TXJrkzYs1Fyf5+yTXH/aezrDPi5I8kuSVi1n/McmrT1vzs0n+aHH/xiQf\nX9x/9WL9JUmuWbzORYe9pxXu97VJvnVx/7uTPHnY+1n1nrcdvyPJJ5K8/7D3s5fbBfWJO8mPJvnI\n4v5HkrxzhzVvT3JXd/97d/9HkruSvCNJquqbkvxSkt88gFn3yznvubu/0t1/lyTd/WySe5IcPYCZ\nz8Xrkzzc3ScXs96erb1vt/3f4o4k11VVLZ6/vbu/2t2PJnl48Xrns3Peb3ff293/unj+s0m+saou\nOZCp92Yv73Gq6p1JHs3Wnke70ML98u7+wuL+F5O8fIc135bk89seP7F4Lkl+I8nvJPnKyibcf3vd\nc5Kkqi5P8iNJ/mYVQ+6DXfewfU13fy3Jfya5asnfPd/sZb/b/ViSe7r7qyuacz+d854XH7p+Jcmv\nHcCcK3fksAfYb1X16STfssOhW7Y/6O6uqqUvqamq1yT5ru7+xdPPmx22Ve152+sfSfKxJL/f3SfP\nbUrON1V1bZIPJXnbYc9yAD6Y5He7+8uLD+CjvejC3d0/cKZjVfWlqnpFd3+hql6R5Kkdlj2Z5E3b\nHh9NcneSNyRZr6rHsvXv9rKquru735RDtsI9P+d4koe6+8P7MO6qPJnk27c9Prp4bqc1Tyz+M3pp\nkn9b8nfPN3vZb6rqaJI7k/xkdz+y+nH3xV72/L1J3l1Vtya5PMnXq+p/uvsPVj/2Chz2SfaDvCX5\n7Zz6Rd2tO6y5Mlvnwa5Y3B5NcuVpa45lzpeTe9pzts7n/3mSlxz2XnbZ55Fsfal6TZ7/4ura09a8\nL6d+cfVni/vX5tQvJ0/m/P9yci/7vXyx/l2HvY+D2vNpaz6Y4V9OHvoAB/zGX5Wtc7QPJfn0tjit\nJ7lt27qfztYXVA8nee8OrzMp3Oe852x9oukkDyS5b3H7mcPe0wvs9QeT/Eu2rjy4ZfHcrye5YXH/\nG7J1RcHDSf4hySu3/e4ti997MOfplTP7td8kv5rkmW3v6X1JXnbY+1n1e7ztNcaH219OAgxzoV1V\nAjCecAMMI9wAwwg3wDDCDTCMcAMMI9wAwwg3wDD/B5vvx8QYqocmAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f9f59b3c750>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(cost0)\n",
    "plt.plot(cost1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def get_random_words(cnt, dicts):\n",
    "    if cnt < 1:\n",
    "        cnt = 1\n",
    "    words = []\n",
    "    index = []\n",
    "    for i in range(cnt):\n",
    "        n = random.randrange(0,len(vocab_dict),1)\n",
    "        words.append(dicts[n])\n",
    "        index.append(n)\n",
    "    return words, index\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def get_cosine_distance_matrix(word_em):\n",
    "    dist_out = 1-pairwise_distances(word_em, metric=\"cosine\")\n",
    "    for i in range(len(dist_out)):\n",
    "        dist_out[i][i] = 0 #将矩阵对角线设为0，不计算每个词与自己的距离\n",
    "    return dist_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "word_dist = get_cosine_distance_matrix(word_embedding_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def get_nearest_word(index, dist_em, top_count):\n",
    "    z = dist_em[index].argsort()[-top_count:]\n",
    "    return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1673 2269 2379 3150 2916 3213 2773 1113 2451 1648]\n",
      "Nearest words to 罪人,31 are:\n",
      "遇到\n",
      "必要\n",
      "各国\n",
      "忌妒\n",
      "怎样\n",
      "假定\n",
      "这样的话\n",
      "文艺复兴\n",
      "并使\n",
      "从而\n",
      "[1925 2439 1433  471 2926 1331 1022 2501  455 2356]\n",
      "Nearest words to 爱情,53 are:\n",
      "深刻\n",
      "鸟儿\n",
      "欢快\n",
      "合理\n",
      "同等\n",
      "持有\n",
      "功能\n",
      "其余\n",
      "贪婪\n",
      "话题\n",
      "[3160 1928 3017 3211 2698 2093 2117 2748 1386 1585]\n",
      "Nearest words to 法官,83 are:\n",
      "白人\n",
      "危险\n",
      "补偿\n",
      "最终\n",
      "虚假\n",
      "成年人\n",
      "繁荣\n",
      "发觉\n",
      "人际关系\n",
      "作\n"
     ]
    }
   ],
   "source": [
    "random_index = [31,53,83]\n",
    "word_dist = get_cosine_distance_matrix(word_embedding_final)\n",
    "\n",
    "for i in range(len(random_index)):\n",
    "    index = get_nearest_word(random_index[i], word_dist, 10)\n",
    "    print index\n",
    "    print(\"Nearest words to %s,%d are:\" % (vocab_dict[random_index[i]], random_index[i]))\n",
    "    for j in index:\n",
    "        print vocab_dict[10-j+1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NCE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def get_input_data_1(seg_index):\n",
    "    contexts = []\n",
    "    labels = []\n",
    "    for i in range(len(seg_index)-1):\n",
    "        if (seg_index[i] < 0):\n",
    "            continue\n",
    "        if (seg_index[i+1] < 0):            \n",
    "            continue\n",
    "        contexts.append(seg_index[i])\n",
    "        labels.append(max(0, seg_index[i+1]))\n",
    "    return (contexts, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "192236\n",
      "192236\n",
      "[267, 1385, 2273, 690, 2988, 3304, 3445, 3467, 1453, 1805]\n",
      "[2237, 2273, 690, 2988, 3304, 3445, 3467, 1453, 2239, 3165]\n"
     ]
    }
   ],
   "source": [
    "#input_data = get_input_data(segs_index)\n",
    "(input_context1, input_labels1) = get_input_data_1(segs_index)\n",
    "\n",
    "print(len(input_context1))\n",
    "print(len(input_labels1))\n",
    "\n",
    "print(input_context1[:10])\n",
    "print(input_labels1[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "graph2 = tf.Graph()\n",
    "with graph2.as_default():\n",
    "    # 决定了 embedding 的维度 （隐层节点数）\n",
    "    word_embedding_dim = 50\n",
    "    # 决定了词表数量, 预留一个未登录词\n",
    "    num_sampled = 25 # Number of negative examples to sample.\n",
    "\n",
    "    UNK_IDX = 0\n",
    "\n",
    "    # 这里需要把 Word embedding 放到 Variable 里面。因为 Word embedding 是要随机初始化，跟着数据不断变化的。\n",
    "    # 它相当于普通神经网络中的权重。\n",
    "\n",
    "    # 在梯度下降时， tensorflow 的 Optimizer 会自动找到 Graph 中的 Variable，计算梯度并进行更新。\n",
    "    word_embedding = tf.Variable(tf.random_uniform([vocab_size, word_embedding_dim]))\n",
    "\n",
    "    # placeholder 和 variable 基本都可以当做 Tensor 来用\n",
    "    # 注意这里的输入是 int32 类型，表示一个词 ID。这里我们需要对数据进行预处理，以把高频词映射到 [1, 80000] 之间，不在词表里面的词设置成 UNK, ID 为 0\n",
    "    # 这里我们假设输入是两个词\n",
    "\n",
    "    # 这里 Shape 的第一维我们指定为 None，是表示第一维可以根据数据进行变化，因此同样一个程序可以适应梯度下降时不同的 batch_size\n",
    "    input_data = tf.placeholder(tf.int32, shape=[None], name='input_data')\n",
    "\n",
    "    input_embeds = tf.nn.embedding_lookup(word_embedding, input_data)\n",
    "\n",
    "\n",
    "    # 样本的 labels 也需要用 placeholder 放置\n",
    "    labels = tf.placeholder(tf.int32, shape=[None,1], name='labels')    \n",
    "\n",
    "    #weights and bias for NCE\n",
    "    W = tf.Variable(\n",
    "        tf.truncated_normal([vocab_size, word_embedding_dim],\n",
    "                            stddev=1.0 / math.sqrt(word_embedding_dim)))\n",
    "    b = tf.Variable(tf.zeros([vocab_size]))\n",
    "\n",
    "    #NCE loss\n",
    "    cost = tf.reduce_mean(\n",
    "        tf.nn.nce_loss(weights=W,\n",
    "                     biases=b,\n",
    "                     labels=labels,\n",
    "                     inputs=input_embeds,\n",
    "                     num_sampled=num_sampled,\n",
    "                     num_classes=vocab_size))\n",
    "    \n",
    "    train_step = tf.train.GradientDescentOptimizer(0.01).minimize(cost)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1\n",
      "Cost: 39.157711\n",
      "------\n",
      "epoch 2\n",
      "Cost: 32.306274\n",
      "------\n",
      "epoch 3\n",
      "Cost: 24.540468\n",
      "------\n",
      "epoch 4\n",
      "Cost: 7.742757\n",
      "------\n",
      "epoch 5\n",
      "Cost: 10.516463\n",
      "------\n",
      "epoch 6\n",
      "Cost: 16.984058\n",
      "------\n",
      "epoch 7\n",
      "Cost: 17.796768\n",
      "------\n",
      "epoch 8\n",
      "Cost: 10.899530\n",
      "------\n",
      "epoch 9\n",
      "Cost: 5.265482\n",
      "------\n",
      "epoch 10\n",
      "Cost: 8.881074\n",
      "------\n",
      "NCE Training duration=17.72\n"
     ]
    }
   ],
   "source": [
    "epoches = 10\n",
    "display_steps = 1\n",
    "\n",
    "cost0 = []\n",
    "cost1 = []\n",
    "\n",
    "with tf.Session(graph=graph2) as sess:\n",
    "    train_start = time.time()\n",
    "    sess.run(tf.global_variables_initializer())    \n",
    "    for i in range(epoches):        \n",
    "        data_ptr = 0        \n",
    "        for j in range(batch_per_round):\n",
    "            if j == batch_per_round-1:\n",
    "                data_ptr = train_data_size - batch_size\n",
    "            feed_dict = {input_data: input_context1[data_ptr:data_ptr+batch_size],\n",
    "                             labels: np.reshape(input_labels1[data_ptr:data_ptr+batch_size],(-1,1))}\n",
    "            data_ptr += batch_size\n",
    "            \n",
    "            sess.run(train_step, feed_dict=feed_dict)\n",
    "            \n",
    "        if ((i+1) % display_steps) == 0:\n",
    "            print(\"epoch %d\" % (i+1))\n",
    "            cost_train = cost.eval(feed_dict=feed_dict)\n",
    "            print(\"Cost: %f\" % (cost_train))\n",
    "            cost0.append(cost_train)           \n",
    "            print(\"------\")            \n",
    "        \n",
    "    word_embedding_final2 = sess.run(word_embedding) \n",
    "    \n",
    "    train_end = time.time()\n",
    "    duration = train_end - train_start\n",
    "    print(\"NCE Training duration=%.2f\" % duration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1289 1958 2747 2196 2031  532 2534 1902 2294 1651]\n",
      "Nearest words to 罪人,31 are:\n",
      "列\n",
      "公司\n",
      "事业\n",
      "显赫\n",
      "就\n",
      "抚育\n",
      "想法\n",
      "多些\n",
      "限制\n",
      "遭受\n",
      "[2951 2589 2120 1140 1084 1846 1598  105 2862 1983]\n",
      "Nearest words to 爱情,53 are:\n",
      "一群\n",
      "下面\n",
      "能够\n",
      "这里\n",
      "不诚实\n",
      "悲观主义\n",
      "一大部分\n",
      "怯懦\n",
      "令\n",
      "期间\n",
      "[2287  152 2300 1161 1649   81  872  570  668 1918]\n",
      "Nearest words to 法官,83 are:\n",
      "男孩子\n",
      "玫瑰花\n",
      "朋友\n",
      "过程\n",
      "的确\n",
      "期待\n",
      "他们\n",
      "美\n",
      "娱乐\n",
      "一句\n"
     ]
    }
   ],
   "source": [
    "random_index = [31,53,83]\n",
    "word_dist2 = get_cosine_distance_matrix(word_embedding_final2)\n",
    "\n",
    "for i in range(len(random_index)):\n",
    "    index = get_nearest_word(random_index[i], word_dist2, 10)\n",
    "    print index\n",
    "    print(\"Nearest words to %s,%d are:\" % (vocab_dict[random_index[i]], random_index[i]))\n",
    "    for j in index:\n",
    "        print vocab_dict[10-j+1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Summary\n",
    "\n",
    "其他参数和输入数据不变的情况下，NCE负例数量25，训练同样的epoch数量，使用softmax和NCE训练时间分别为625秒，17.7秒，NCE获得35倍性能提升。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
