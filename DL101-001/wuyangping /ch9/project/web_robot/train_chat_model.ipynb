{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPython 2.7.6\n",
      "IPython 5.2.2\n",
      "\n",
      "tensorflow 1.0.0\n",
      "numpy 1.12.0\n",
      "\n",
      "compiler   : GCC 4.8.4\n",
      "system     : Linux\n",
      "release    : 4.4.43-boot2docker\n",
      "machine    : x86_64\n",
      "processor  : x86_64\n",
      "CPU cores  : 1\n",
      "interpreter: 64bit\n"
     ]
    }
   ],
   "source": [
    "# 示例代码运行环境\n",
    "%load_ext watermark\n",
    "%watermark -p tensorflow,numpy -v -m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.contrib.legacy_seq2seq import basic_rnn_seq2seq, embedding_rnn_seq2seq, sequence_loss, embedding_attention_seq2seq\n",
    "from tensorflow.python.ops import variable_scope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "import os\n",
    "import time\n",
    "from collections import Counter\n",
    "import math\n",
    "import jieba\n",
    "from jieba import posseg as pseg\n",
    "import numpy as np\n",
    "import datetime\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "LINE_LIMIT = 0\n",
    "VOCAB_SIZE = 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "DICT_FILE = './corpus/dict_2.pkl'\n",
    "RAW_FILE = './corpus/ssa.out'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def load_dict(file_name):\n",
    "    wdict = {}\n",
    "    wcnt = Counter()\n",
    "    with open(file_name, 'rb') as f:\n",
    "        (wdict, wcnt) = pickle.load(f)\n",
    "\n",
    "    rdict = dict(zip(wdict.values(), wdict.keys())) \n",
    "    return wcnt, wdict, rdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0677170753479\n"
     ]
    }
   ],
   "source": [
    "t0 = time.time()\n",
    "word_cnt, train_dict, train_reverse_dict = load_dict(DICT_FILE)\n",
    "t1 = time.time()\n",
    "print(t1-t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def load_train_data(file_name):\n",
    "    with open(file_name, 'rb') as f:\n",
    "        lines = f.readlines()\n",
    "        sentences = []\n",
    "        for line in lines:\n",
    "            line = line.decode('utf8').rstrip()\n",
    "            sentences.append(line)\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000\n"
     ]
    }
   ],
   "source": [
    "raw_sentences = load_train_data(RAW_FILE)\n",
    "print(len(raw_sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "LINE_BREAK = u'<Break>'\n",
    "WORD_DELIMITER = u'/'\n",
    "UNK_WORD = u'<UNK>'\n",
    "PADDING_WORD = u'<PAD>'\n",
    "START_WORD = u'<GO>'\n",
    "END_WORD = u'<EOS>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4997\n",
      "4998\n",
      "4999\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "START_ID = train_dict[START_WORD]\n",
    "END_ID = train_dict[END_WORD]\n",
    "PAD_ID = train_dict[PADDING_WORD]\n",
    "UNK_ID = train_dict[UNK_WORD]\n",
    "print(UNK_ID)\n",
    "print(START_ID)\n",
    "print(END_ID)\n",
    "print(PAD_ID)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def words2id(words, wdict):\n",
    "    out_ = []\n",
    "    for w in words:\n",
    "        if w in wdict.keys():           \n",
    "            out_.append(wdict[w])\n",
    "        else:\n",
    "            out_.append(wdict[UNK_WORD])\n",
    "    return out_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def build_dataset(sentences, wdict):    \n",
    "    out_ = []\n",
    "    for sentence in sentences:\n",
    "        cur_sentence = words2id(sentence, wdict)          \n",
    "        out_.append(cur_sentence)         \n",
    "    return out_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16.6768779755\n",
      "20000\n"
     ]
    }
   ],
   "source": [
    "t0 = time.time()\n",
    "train_sentences = build_dataset(raw_sentences, train_dict)\n",
    "t1 = time.time()\n",
    "print(t1-t0)\n",
    "print(len(train_sentences))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3, 2, 2, 534, 787, 84, 551, 2, 2, 2, 247, 808, 816]\n"
     ]
    }
   ],
   "source": [
    "print(train_sentences[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def pad_sentence(data, length, pad_index, start_index, end_index, is_encode=True):\n",
    "    result_ = []\n",
    "    data_len = len(data)\n",
    "    if (data_len >= length):\n",
    "        result_ = data[:length] #长句做截断处理\n",
    "        if not is_encode:\n",
    "            result_[length-1] = end_index\n",
    "    else:\n",
    "        pad_len = length - data_len\n",
    "        padding = [pad_index] * pad_len\n",
    "        if is_encode:\n",
    "            result_ = padding + data\n",
    "        else:\n",
    "            result_ = [start_index] + data + [end_index] + padding[:-2]\n",
    "    \n",
    "    return result_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def get_batch_data2(offset, size, input_data, input_len, output_len):\n",
    "    total_len = len(input_data)    \n",
    "    if (offset) > total_len:\n",
    "        offset = 0\n",
    "    if (offset+size>total_len):\n",
    "        size = total_len - offset\n",
    "    \n",
    "    \n",
    "    input_ = []\n",
    "    output_ = []\n",
    "    \n",
    "    index = offset\n",
    "    while(len(input_) < size):\n",
    "        if (index >= total_len-1):\n",
    "            index = 0\n",
    "        if (len(input_data[index])>1):\n",
    "            encode_data = pad_sentence(input_data[index], input_len, 0, START_ID, END_ID)\n",
    "            decode_data = pad_sentence(input_data[index+1], output_len, 0, START_ID, END_ID, False)\n",
    "            input_.append(encode_data)\n",
    "            output_.append(decode_data)\n",
    "        index += 1    \n",
    "\n",
    "    return input_, output_  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "3\n",
      "[[0, 0, 3, 2, 2, 534, 787, 84, 551, 2, 2, 2, 247, 808, 816], [0, 18, 1284, 199, 1698, 198, 924, 2, 5, 857, 392, 1, 174, 116, 86], [0, 878, 13, 774, 51, 135, 316, 315, 128, 245, 1, 944, 423, 140, 55]]\n",
      "[[4998, 18, 1284, 199, 1698, 198, 924, 2, 5, 857, 392, 1, 174, 116, 86, 4999, 0, 0, 0, 0], [4998, 878, 13, 774, 51, 135, 316, 315, 128, 245, 1, 944, 423, 140, 55, 4999, 0, 0, 0, 0], [4998, 117, 5, 101, 463, 6, 315, 128, 1, 3, 4999, 0, 0, 0, 0, 0, 0, 0, 0, 0]]\n"
     ]
    }
   ],
   "source": [
    "a,b = get_batch_data2(0,3, train_sentences, 15, 20)\n",
    "print(len(a))\n",
    "print(len(b))\n",
    "\n",
    "print(a)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def left_shift(decoder_inputs, pad_idx):\n",
    "    # for generating targets\n",
    "    return [list(input_[1:]) + [pad_idx] for input_ in decoder_inputs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def generate_feed_dict(batch_encoder_inputs, batch_decoder_inputs, pad_index):\n",
    "    encoder_inputs_ = list(zip(*batch_encoder_inputs))  \n",
    "\n",
    "    target_inputs_ = list(zip(*left_shift(batch_decoder_inputs, pad_index)))\n",
    "    decoder_inputs_ = list(zip(*batch_decoder_inputs))\n",
    "\n",
    "    feed_dict = dict()\n",
    "    # Prepare input data    \n",
    "    for (i, placeholder) in enumerate(encoder_placeholders):\n",
    "        # 这里用 placeholder 或者 placeholder.name 都可以       \n",
    "        feed_dict[placeholder.name] = np.asarray(encoder_inputs_[i], dtype=int)       \n",
    "        for i in range(len(decoder_placeholders)):           \n",
    "            feed_dict[decoder_placeholders[i].name] = np.asarray(decoder_inputs_[i], dtype=int)\n",
    "            feed_dict[target_placeholders[i].name] = np.asarray(target_inputs_[i], dtype=int)        \n",
    "            # 这里使用 weights 把 <PAD> 的损失屏蔽了\n",
    "            feed_dict[target_weights_placeholders[i].name] = np.asarray([float(idx != pad_index) for idx in target_inputs_[i]],\n",
    "                                                              dtype=float)\n",
    "    return feed_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Attenion\n",
    "tf.reset_default_graph()\n",
    "\n",
    "RNN_CELL_TYPE = 'LSTMCell_Attention'\n",
    "learning_rate = 1.0\n",
    "\n",
    "encoder_length = 15\n",
    "decoder_length = 20\n",
    "embed_dim = 128\n",
    "\n",
    "cell = tf.contrib.rnn.LSTMCell(embed_dim)\n",
    "num_encoder_symbols = VOCAB_SIZE\n",
    "num_decoder_symbols = VOCAB_SIZE\n",
    "embedding_size = embed_dim\n",
    "\n",
    "encoder_len_placeholder = tf.placeholder(tf.int32)\n",
    "\n",
    "encoder_placeholders = [tf.placeholder(tf.int32, shape=[None],\n",
    "                                       name=\"encoder_%d\" % i) for i in range(encoder_length)]\n",
    "decoder_placeholders = [tf.placeholder(tf.int32, shape=[None],\n",
    "                                       name=\"decoder_%d\" % i) for i in range(decoder_length)]\n",
    "target_placeholders = [tf.placeholder(tf.int32, shape=[None],\n",
    "                                       name=\"target_%d\" % i) for i in range(decoder_length)]\n",
    "target_weights_placeholders = [tf.placeholder(tf.float32, shape=[None],\n",
    "                                       name=\"decoder_weight_%d\" % i) for i in range(decoder_length)]\n",
    "outputs, states = embedding_attention_seq2seq(\n",
    "    encoder_placeholders, decoder_placeholders, cell,\n",
    "    num_encoder_symbols, num_decoder_symbols,\n",
    "    embedding_size, output_projection=None,\n",
    "    feed_previous=False)\n",
    "\n",
    "loss = sequence_loss(outputs, target_placeholders, target_weights_placeholders)\n",
    "#train_step = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)\n",
    "#train_step = tf.train.AdamOptimizer(learning_rate).minimize(loss)\n",
    "train_step = tf.train.AdagradOptimizer(learning_rate).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000\n",
      "2\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "epoch_num = 100\n",
    "batch_size = 100\n",
    "sample_ratio = 100 #为节约训练时间，只用 1/ Sample_ratio的样本数据训练\n",
    "\n",
    "train_data_size = len(train_sentences)\n",
    "iteration_num = int(train_data_size / batch_size / sample_ratio)\n",
    "#if train_data_size % batch_size > 0:\n",
    "#    iteration_num += 1\n",
    "\n",
    "display_step = int(iteration_num / 10)\n",
    "\n",
    "print(train_data_size)\n",
    "print(iteration_num)\n",
    "print(display_step)\n",
    "\n",
    "save_epoch = 1\n",
    "\n",
    "pad_index = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-05-15 14:44:53 Start training, Cell type=LSTMCell_Attention, Learning rate=1.000000\n"
     ]
    }
   ],
   "source": [
    "saver = tf.train.Saver()\n",
    "sess = tf.Session()\n",
    "\n",
    "sess.run(tf.global_variables_initializer())\n",
    "#saved_model = './../models/bot_model_0026'\n",
    "#print('Loading model from:', saved_model)        \n",
    "#saver.restore(sess, saved_model)\n",
    "\n",
    "\n",
    "print(\"%s Start training, Cell type=%s, Learning rate=%f\" % \n",
    "      (time.strftime(\"%Y-%m-%d %H:%M:%S\"), RNN_CELL_TYPE, learning_rate))\n",
    "\n",
    "costs = []\n",
    "t0 = time.time()\n",
    "for epoch in range(epoch_num):\n",
    "    #print(\"Start epoch %d\" % epoch)\n",
    "        \n",
    "    offset = 0\n",
    "    #iteration_num = 1\n",
    "    for i in range(iteration_num):\n",
    "        encoder_inputs, decoder_inputs = get_batch_data2(\n",
    "            offset, batch_size, \n",
    "            train_sentences, \n",
    "            encoder_length, decoder_length)\n",
    "        offset += batch_size\n",
    "\n",
    "        feed_dict1 = generate_feed_dict(encoder_inputs, decoder_inputs, pad_index)\n",
    "\n",
    "        sess.run(train_step, feed_dict1)\n",
    "        \n",
    "        if i % display_step == 0:\n",
    "            print(\"%s %d, %f\" % (time.strftime(\"%Y-%m-%d %H:%M:%S\"), i, sess.run(loss, feed_dict1)))\n",
    "        \n",
    "    c = sess.run(loss, feed_dict1)\n",
    "    costs.append(c)\n",
    "    print(\"%s epoch %d, cost=%f\" %(time.strftime(\"%Y-%m-%d %H:%M:%S\"), epoch, c))\n",
    "    \n",
    "    if ((epoch+1) % save_epoch) == 0:\n",
    "        model_path = './../models/bot_model_attention_%04d' % (epoch) \n",
    "        print(model_path)\n",
    "        saved_model = saver.save(sess, model_path)\n",
    "        print(\"%s Modeled saved to: %s\" % (time.strftime(\"%Y-%m-%d %H:%M:%S\"), saved_model))\n",
    "            \n",
    "\n",
    "t1 = time.time()\n",
    "print(\"Training duration:%d\" % (t1-t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Loading model from:', './../models/bot_model_attention_0039')\n",
      "1.66039395332\n"
     ]
    }
   ],
   "source": [
    "saver = tf.train.Saver()\n",
    "sess = tf.Session()\n",
    "\n",
    "#sess.run(tf.global_variables_initializer())\n",
    "saved_model = './model/bot_model_attention_0039'\n",
    "print('Loading model from:', saved_model)   \n",
    "\n",
    "t0 = time.time()\n",
    "saver.restore(sess, saved_model)\n",
    "t1 = time.time()\n",
    "print(t1-t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def decode_data(session, offset, size, encode_input, decode_input):\n",
    "    # Decoding\n",
    "    with variable_scope.variable_scope(variable_scope.get_variable_scope(), reuse=True):\n",
    "        #outputs, states = embedding_rnn_seq2seq(\n",
    "        outputs, states = embedding_attention_seq2seq(\n",
    "             encoder_placeholders, decoder_placeholders, cell,\n",
    "            num_encoder_symbols, num_decoder_symbols,\n",
    "            embedding_size, output_projection=None,\n",
    "            feed_previous=True)\n",
    "  \n",
    "        \n",
    "        test_encoder_inputs, test_decoder_inputs = get_batch_data2(\n",
    "                offset, size, \n",
    "                encode_input[:-1], \n",
    "                encoder_length, \n",
    "                decoder_length)\n",
    "\n",
    "        feed_dict_test = generate_feed_dict(test_encoder_inputs, test_decoder_inputs, pad_index)        \n",
    "      \n",
    "        result = []\n",
    "        for o in outputs:\n",
    "            # 注意这里也需要提供 feed_dict\n",
    "            m = np.argmax(o.eval(feed_dict_test, session=sess), axis=1)\n",
    "            result.append(m[0])\n",
    "\n",
    "        return result            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def index_to_words(data, dictionary):\n",
    "    text = ''\n",
    "    for w in data:\n",
    "        if (w==END_ID):\n",
    "            break\n",
    "        if (w!=PAD_ID):\n",
    "            text += dictionary[w] + ' '\n",
    "    #text = ' '.join([dictionary[i] for i in data])\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "拥 有 史 上 最 强 魔 法 师 的 遗 传 因 子 \n"
     ]
    }
   ],
   "source": [
    "print(index_to_words(train_sentences[2], train_reverse_dict))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def decode_line(line_no, offset, dataset, rdict):\n",
    "    index = line_no+offset\n",
    "    while (len(dataset[index])<=1):\n",
    "        index += 1\n",
    "        if (index>=len(dataset)):\n",
    "            index=0\n",
    "            break\n",
    "    encoder_input = list(dataset[index])\n",
    "\n",
    "   \n",
    "    output = decode_data(sess, index, 1, dataset, dataset)\n",
    "    print(\"Line#: %d\" % (index+1))\n",
    "    print(\"Input: %s\" % index_to_words(dataset[index], rdict)) \n",
    "    print(\"Result: %s\" % index_to_words(output, rdict))\n",
    "    print(\"Expect: %s\" % index_to_words(dataset[index+1], rdict)) \n",
    "    print(\"----------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Line#: 101\n",
      "Input: 快 给 我 出 去 \n",
      "Result: 仪 ！ 救 他 你 跟 办 \n",
      "Expect: 不 可 以 \n",
      "----------------------------------\n"
     ]
    }
   ],
   "source": [
    "decode_line(0,100,train_sentences, train_reverse_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def decode_lines(line_count, offset, encode_input, decode_input, encode_dict, decode_dict):\n",
    "    for i in range(line_count):\n",
    "        index = offset+i\n",
    "        output = decode_data(sess, index, 1, encode_input, decode_input)\n",
    "        print(\"Line#: %d\" % (index+1))\n",
    "        print(\"Input: %s\" % index_to_words(encode_input[index], encode_dict))\n",
    "        print(\"Result: %s\" % index_to_words(output, decode_dict))\n",
    "        print(\"Expect: %s\" % index_to_words(decode_input[index+1], decode_dict)) \n",
    "        print(\"----------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Line#: 101\n",
      "Input: 快 给 我 出 去 \n",
      "Result: 仪 ！ 救 他 你 跟 办 \n",
      "Expect: 不 可 以 \n",
      "----------------------------------\n",
      "Line#: 102\n",
      "Input: 不 可 以 \n",
      "Result: ！ 救 才 ！ 救 丹 才 ！ 救 丹 才 ！ 救 丹 去 吧 \n",
      "Expect: 不 管 是 幽 灵 也 好     什 么 也 好 \n",
      "----------------------------------\n",
      "Line#: 103\n",
      "Input: 不 管 是 幽 灵 也 好     什 么 也 好 \n",
      "Result: 他 们 上 他 \n",
      "Expect: 和 树 还 是 和 树 \n",
      "----------------------------------\n",
      "Line#: 104\n",
      "Input: 和 树 还 是 和 树 \n",
      "Result: 没 ！ 救 救 他 诉 他 你 跟 放 听 \n",
      "Expect: 是 我 最 重 要 　 最 重 要 的 丈 夫 \n",
      "----------------------------------\n",
      "Line#: 105\n",
      "Input: 是 我 最 重 要 　 最 重 要 的 丈 夫 \n",
      "Result: 上 他 来 他 你 俊 \n",
      "Expect: 怎 么 会       夕 菜 \n",
      "----------------------------------\n",
      "Line#: 106\n",
      "Input: 怎 么 会       夕 菜 \n",
      "Result: 仪 ！ 去 吧 他   ！ 救 救 去 权 仪 ！ 去 吧 他   ！ 救 救 \n",
      "Expect: 如 果 有 利 用 价 值 的 话 \n",
      "----------------------------------\n",
      "Line#: 107\n",
      "Input: 如 果 有 利 用 价 值 的 话 \n",
      "Result: ！ 救 丹 才 ！ 去 吧 他 一 ！ 救 丹 才 ！ 去 体 味 上 历 仪 \n",
      "Expect: 要 留 他 下 来 也 可 以 \n",
      "----------------------------------\n",
      "Line#: 108\n",
      "Input: 要 留 他 下 来 也 可 以 \n",
      "Result: 上 他 没 ！ 救 他 \n",
      "Expect: 可 是 对 方 是 幽 灵 啊 \n",
      "----------------------------------\n",
      "Line#: 109\n",
      "Input: 可 是 对 方 是 幽 灵 啊 \n",
      "Result: ！ 救 他 们 ！ 救 爬 ！ 片 仪 上 他 你 跟 才 ！ 救 去 权 ！ \n",
      "Expect: 能 有 什 么 用 处 呢 \n",
      "----------------------------------\n",
      "Line#: 110\n",
      "Input: 能 有 什 么 用 处 呢 \n",
      "Result: 上 他 上 他 上 \n",
      "Expect: 是 不 是 幽 灵 还 不 清 楚     会 让 人 认 为 是 冒 牌 的 \n",
      "----------------------------------\n"
     ]
    }
   ],
   "source": [
    "#训练数据集翻译测试\n",
    "decode_lines(10, 100, train_sentences, train_sentences, train_reverse_dict, train_reverse_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "decode_line(1,307,train_sentences, train_reverse_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def generate_response(session, test_sentence, wdict, rdict, encoder_len):\n",
    "    data = build_test_dataset(test_sentence, wdict, encoder_len)\n",
    "\n",
    "    output = decode_text(session, data, PAD_ID)\n",
    "    \n",
    "    print(\"Raw:%s\" % test_sentence)\n",
    "    print(\"Input:%s\" % index_to_words(data, rdict))\n",
    "    print(\"Output:%s\" % index_to_words(output, rdict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def build_test_dataset(test_text, wdict, encoder_len):\n",
    "    #words = pseg.cut(test_text)\n",
    "    #words = [w for (w,v) in words]\n",
    "    words = test_text\n",
    "    \n",
    "    ids = words2id(words, wdict)\n",
    "    if (len(ids)>encoder_len):\n",
    "        ids = ids[:encoder_len]\n",
    "    return pad_sentence(ids, encoder_len, 0, START_ID, END_ID)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def words2id(words, wdict):\n",
    "    id_ = []\n",
    "    for w in words:\n",
    "        index = wdict[UNK_WORD]\n",
    "        if w in wdict.keys():\n",
    "            id_.append(wdict[w])\n",
    "        else:\n",
    "            id_.append(wdict[UNK_WORD])\n",
    "    return id_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def decode_text(session, encode_input, pad_index):    \n",
    "      \n",
    "        decode_input = encode_input + [pad_index]*(decoder_length-len(encode_input))\n",
    "        feed_dict_test = generate_feed_dict([encode_input], [decode_input], pad_index)        \n",
    "      \n",
    "        result = []\n",
    "        for o in outputs:\n",
    "            # 注意这里也需要提供 feed_dict\n",
    "            m = np.argmax(o.eval(feed_dict_test, session=sess), axis=1)\n",
    "            result.append(m[0])\n",
    "\n",
    "        return result "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw:我该怎么办\n",
      "Input:我 该 怎 么 办 \n",
      "Output:藤 硫 良 救 良 良 良 ！ 案 际 他 起 ！ ！ v \n"
     ]
    }
   ],
   "source": [
    "input_text = u'我该怎么办'\n",
    "generate_response(sess, input_text, train_dict, train_reverse_dict, encoder_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
