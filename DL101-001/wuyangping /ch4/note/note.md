# 任务1：基于矩阵乘法用 Tensorflow 实现 ch3 作业的单隐层神经网络
- Ch3 task3已经是用矩阵乘法实现。     

# 任务2：使用 tf.layers.dense 替换原有的矩阵乘法
- 使用tf.layers.dense,代码显得更简洁，不用自己初始化w和b。
- 换成dense后遇到一个坑，训练数据收敛的很好，精度达到1，但画出来的分界面效果不理想，形状类似tanh函数。答疑时问了老师，估计是分界面画错了，我重新检查代码，发现我在生成测试输出时，重新用tf.layers定义了新的输入层，隐层和输出层，跟训练数据集的layers是独立的。因此我在生成分界面的测试结果时，用的是未经训练的layers在做预测，因此看起来输出结果就是tanh函数，因为对应的w和b都还是初始值。修改了test_prediction的定义，使用跟训练数据同样的layers后问题解决，分界面和之前的矩阵乘法是一致的。

# 任务3：构建 word embedding
- 使用你的语料库进行训练，构建 nn language model
- 完成 3 个名词各自最相近的 Top 10 个词的检索

收获：
 - 对word embedding最朴素的理解就是神经网络的权重，词向量就是字典中的每个词对应的weights矩阵中的一行。
 - 对于大语料库的语言模型的训练，如果每次载入太多训练数据集，会导致矩阵太大，内存不够用，可以使用mini batch方法分批输入。
 - 对于大型模型训练，可使用tf.train.Saver保存训练结果到文件中，后续可用tf.train.restore()读入训练好的模型，在此基础上直接做预测或继续训练。

# 小作业：为什么取这个矩阵的某一行，和这个词出现的时候做矩阵乘法，结果是一样的呢？
- 因为这个词出现时，输入数据的向量只在这个词对应的位置为1，其他位置的元素都为0，做矩阵乘法后，得到行向量中每个元素的值，只有该词对应的w会保留，其他元素的乘积都为0，因此与直接取这个矩阵的对应行是等价的，这样可以不做矩阵乘法，节约计算时间。
