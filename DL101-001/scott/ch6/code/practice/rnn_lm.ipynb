{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import itertools\n",
    "import operator\n",
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import jieba\n",
    "import tensorflow as tf\n",
    "import zhon.hanzi as zh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 数据清理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_data(filename):\n",
    "    \"\"\"Read and cut sequences, return a long string.\"\"\"\n",
    "    jieba.setLogLevel(20)\n",
    "    jieba.enable_parallel(4)\n",
    "    with open(filename, 'r') as file:\n",
    "        data = file.read()\n",
    "    # Split sequences with zh.punctuation or 'new line'\n",
    "    sequences = re.split(r'[{}|\\n]'.format(zh.punctuation), data)\n",
    "    # Remove empty string\n",
    "    sequences = (sequence for sequence in sequences if sequence)\n",
    "    sequences = (list(jieba.cut(sequence)) for sequence in sequences)\n",
    "    return sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_common_words(words, n):\n",
    "    count = Counter(words)\n",
    "    count_dict = {i: j for i, j in count.items() if j > n}\n",
    "    word_counts = sorted(count_dict.items(), key=operator.itemgetter(1), reverse=True)\n",
    "    return word_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_dict(word_counts):\n",
    "    count = [['UNK', -1]]\n",
    "    count.extend(word_counts)\n",
    "    word_dict = {key: value for value, (key, _) in enumerate(count)}\n",
    "    reversed_dict = {key: value for value, key in word_dict.items()}\n",
    "    return word_dict, reversed_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def word_to_number(sequences, word_dict):\n",
    "    data = []\n",
    "    for sequence in sequences:\n",
    "        sequence_data = []\n",
    "        for word in sequence:\n",
    "            if word in word_dict:\n",
    "                index = word_dict[word]\n",
    "            else:\n",
    "                index = 0\n",
    "            sequence_data.append(index)\n",
    "        data.append(sequence_data)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = list(read_data('YGZ-rain.md'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "words = list(itertools.chain.from_iterable(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_counts = get_common_words(words, 0)  # 小语料训练，所以不做截断了"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_dict, reversed_dict = build_dict(word_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text_data = word_to_number(data, word_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. 定义参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD8CAYAAAB5Pm/hAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fae84a2b7f0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "length = [len(i) for i in text_data]\n",
    "plt.hist(length)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "大部分句子长度都在10以下"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocab_size = len(word_counts) + 1  # 1214\n",
    "hidden_size = 128  # The same as embedding dim\n",
    "batch_size = 20\n",
    "max_words = 10\n",
    "number_steps = max_words - 1\n",
    "epochs = 1000 + 1\n",
    "print_loss_every = 100\n",
    "lr = 0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 定义测试数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_padding = np.asarray([x[:max_words] for x in [y + [0] * max_words \n",
    "                                         for y in text_data]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = data_padding[:, :number_steps]\n",
    "y_ = data_padding[:, 1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data = np.concatenate((x, y_), axis=1)  # 方便 shuffle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 2. 定义神经网络结构以及前向传播过程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Placeholder for input, output\n",
    "# input_x 不取最后一词，input_y 不取第一词，所以两者的维度是一样的\n",
    "input_x = tf.placeholder(tf.int32, shape=[None, number_steps], name='input_x')\n",
    "input_y = tf.placeholder(tf.int32, shape=[None, number_steps], name='input_y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 先把非零元素转为 1，然后求和，即句子长度，当然原先句子中的一些 unk 也被忽略了\n",
    "sequence_length = tf.reduce_sum(tf.sign(input_x), 1)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Embedding layer\n",
    "with tf.name_scope('embedding'):\n",
    "    word_embedding = tf.Variable(tf.random_uniform([vocab_size, hidden_size]))\n",
    "    embeds = tf.nn.embedding_lookup(word_embedding, input_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# RNN layer\n",
    "with tf.name_scope('rnn'):\n",
    "    cell = tf.contrib.rnn.BasicRNNCell(128)\n",
    "    outputs, states = tf.nn.dynamic_rnn(\n",
    "        cell, embeds, dtype=tf.float32, sequence_length=sequence_length)\n",
    "    # Flat outputs\n",
    "    output_flat = tf.reshape(outputs, [-1, hidden_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "logits = tf.layers.dense(output_flat, vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "probs = tf.nn.softmax(logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "labels_flat = tf.reshape(input_y, [-1])\n",
    "mask = tf.cast(tf.sign(labels_flat), dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cost = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=labels_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loss = tf.reduce_sum(cost * mask) / batch_size  # 每个句子的平均损失"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. 训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After    0 steps, perplexity is 11.08\n",
      "After  100 steps, perplexity is 10.97\n",
      "After  200 steps, perplexity is 9.97\n",
      "After  300 steps, perplexity is 8.36\n",
      "After  400 steps, perplexity is 7.17\n",
      "After  500 steps, perplexity is 6.06\n",
      "After  600 steps, perplexity is 5.16\n",
      "After  700 steps, perplexity is 4.41\n",
      "After  800 steps, perplexity is 3.83\n",
      "After  900 steps, perplexity is 3.39\n",
      "After 1000 steps, perplexity is 3.05\n"
     ]
    }
   ],
   "source": [
    "train_step = tf.train.AdamOptimizer(lr).minimize(loss)\n",
    "saver = tf.train.Saver()\n",
    "with tf.Session() as sess:\n",
    "    costs = 0.0\n",
    "    iters = 0\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for i in range(epochs): \n",
    "        batch_data = train_data[np.random.randint(\n",
    "            train_data.shape[0], size=batch_size), :]\n",
    "        X = batch_data[:, :number_steps]\n",
    "        Y = batch_data[:, number_steps:]\n",
    "        feed_dict = {input_x: X, input_y: Y}\n",
    "        sess.run(train_step, feed_dict=feed_dict)\n",
    "        cross_entropy = sess.run(loss, feed_dict=feed_dict)\n",
    "        costs += cross_entropy\n",
    "        iters += number_steps\n",
    "        perplexity = np.exp(costs / iters)\n",
    "        if i % print_loss_every == 0:\n",
    "            print('After {:4d} steps, perplexity is {:.2f}'.format(i, perplexity))\n",
    "    # After training, save the final sess\n",
    "    saver.save(sess, 'save_model/rnn-model.ckpt', global_step=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checkpoint\t\t\t\t rnn-model.ckpt-1001.index\r\n",
      "rnn-model.ckpt-1001.data-00000-of-00001  rnn-model.ckpt-1001.meta\r\n"
     ]
    }
   ],
   "source": [
    "!ls save_model/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Refrences:\n",
    "\n",
    "* [assignment2.pdf](http://web.stanford.edu/class/cs224n/assignment2/assignment2.pdf)\n",
    "* [The Unreasonable Effectiveness of Recurrent Neural Networks](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "87px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": false,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
