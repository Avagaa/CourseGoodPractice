{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 使用 RNN 构建语言模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 环境说明："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scott Ming 2017-04-23 \n",
      "\n",
      "CPython 3.6.0\n",
      "IPython 6.0.0\n",
      "\n",
      "numpy 1.12.1\n",
      "pandas 0.19.2\n",
      "matplotlib 2.0.0\n",
      "tensorflow 1.0.1\n",
      "\n",
      "compiler   : GCC 4.9.2\n",
      "system     : Linux\n",
      "release    : 3.16.0-4-amd64\n",
      "machine    : x86_64\n",
      "processor  : \n",
      "CPU cores  : 4\n",
      "interpreter: 64bit\n"
     ]
    }
   ],
   "source": [
    "%load_ext watermark\n",
    "%watermark -a 'Scott Ming' -v -m -d -p numpy,pandas,matplotlib,tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import itertools\n",
    "import operator\n",
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import jieba\n",
    "import tensorflow as tf\n",
    "import zhon.hanzi as zh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 数据清理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_data(filename):\n",
    "    \"\"\"Read and cut sequences, return a long string.\"\"\"\n",
    "    jieba.setLogLevel(20)\n",
    "    jieba.enable_parallel(4)\n",
    "    with open(filename, 'r') as file:\n",
    "        data = file.read()\n",
    "    # Split sequences with zh.punctuation or 'new line'\n",
    "    sequences = re.split(r'[{}|\\n]'.format(zh.punctuation), data)\n",
    "    # Remove empty string\n",
    "    sequences = (sequence for sequence in sequences if sequence)\n",
    "    sequences = (list(jieba.cut(sequence)) for sequence in sequences)\n",
    "    return sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_common_words(words, n):\n",
    "    count = Counter(words)\n",
    "    count_dict = {i: j for i, j in count.items() if j > n}\n",
    "    word_counts = sorted(count_dict.items(), key=operator.itemgetter(1), reverse=True)\n",
    "    return word_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_dict(word_counts):\n",
    "    count = [['UNK', -1]]\n",
    "    count.extend(word_counts)\n",
    "    word_dict = {key: value for value, (key, _) in enumerate(count)}\n",
    "    reversed_dict = {key: value for value, key in word_dict.items()}\n",
    "    return word_dict, reversed_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def word_to_number(sequences, word_dict):\n",
    "    data = []\n",
    "    for sequence in sequences:\n",
    "        sequence_data = []\n",
    "        for word in sequence:\n",
    "            if word in word_dict:\n",
    "                index = word_dict[word]\n",
    "            else:\n",
    "                index = 0\n",
    "            sequence_data.append(index)\n",
    "        data.append(sequence_data)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = list(read_data('YGZ-rain.md'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "words = list(itertools.chain.from_iterable(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_counts = get_common_words(words, 0)  # 小语料训练，所以不做截断了"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_dict, reversed_dict = build_dict(word_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text_data = word_to_number(data, word_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. 定义参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD8CAYAAAB5Pm/hAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fc07a41b438>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "length = [len(i) for i in text_data]\n",
    "plt.hist(length)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "大部分句子长度都在10以下"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocab_size = len(word_counts) + 1  # 1214\n",
    "hidden_size = 128  # The same as embedding dim\n",
    "batch_size = 20\n",
    "max_words = 10\n",
    "number_steps = max_words - 1\n",
    "epochs = 1000 + 1\n",
    "print_loss_every = 100\n",
    "eval_every = 200\n",
    "lr = 0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 定义测试数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_padding = np.asarray([x[:max_words] for x in [y + [0] * max_words \n",
    "                                         for y in text_data]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = data_padding[:, :number_steps]\n",
    "y_ = data_padding[:, 1:]\n",
    "train_data = np.concatenate((x, y_), axis=1)  # 方便 shuffle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 2. 定义 RNN 类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class BasicRNN(object):\n",
    "    def __init__(self, number_steps, vocab_size, hidden_size, batch_size, infer_sample=False):\n",
    "        self.number_steps = number_steps\n",
    "        self.vocab_size = vocab_size\n",
    "        self.hidden_size = hidden_size  # Equal to embeddings dim\n",
    "        self.batch_size = batch_size \n",
    "        \n",
    "        if infer_sample:\n",
    "            self.batch_size = 1\n",
    "            self.number_steps = 1\n",
    "        else:\n",
    "            self.batch_size = batch_size\n",
    "            self.number_steps = number_steps\n",
    "\n",
    "        # Placeholder for input, output\n",
    "        self.input_x = tf.placeholder(tf.int32, shape=[None, self.number_steps], name='input_x')\n",
    "        self.input_y = tf.placeholder(tf.int32, shape=[None, self.number_steps], name='input_y')\n",
    "        # 先把非零元素转为 1，然后求和，即句子长度，当然原先句子中的一些 unk 也被忽略了\n",
    "        self.sequence_length = tf.reduce_sum(tf.sign(self.input_x), 1)\n",
    "        \n",
    "        # Embedding layer\n",
    "        with tf.name_scope('embedding'):\n",
    "            word_embedding = tf.get_variable('word_embedding', [self.vocab_size, self.hidden_size],\n",
    "                                            tf.float32, tf.random_normal_initializer())\n",
    "            embeds = tf.nn.embedding_lookup(word_embedding, self.input_x)\n",
    "        \n",
    "        # RNN layer\n",
    "        with tf.name_scope('rnn'):\n",
    "            self.basic_cell = tf.contrib.rnn.BasicRNNCell(self.hidden_size)\n",
    "            self.initial_state = self.basic_cell.zero_state(self.batch_size, tf.float32)\n",
    "            outputs, states = tf.nn.dynamic_rnn(\n",
    "                self.basic_cell, embeds, dtype=tf.float32, sequence_length=self.sequence_length)\n",
    "            # Flat outputs\n",
    "            output_flat = tf.reshape(outputs, [-1, self.hidden_size])\n",
    "            self.final_state = states\n",
    "        \n",
    "        # Softmax output\n",
    "        with tf.name_scope('softmax_output'):\n",
    "            W = tf.get_variable('W', [self.hidden_size, self.vocab_size], \n",
    "                                tf.float32, tf.random_normal_initializer())\n",
    "            b = tf.get_variable('b', [self.vocab_size], tf.float32, \n",
    "                                tf.constant_initializer(0.0))\n",
    "            self.logits = tf.matmul(output_flat, W) + b\n",
    "            self.model_output = tf.nn.softmax(self.logits)\n",
    "        \n",
    "        # Cross-entropy loss\n",
    "        with tf.name_scope('loss'):\n",
    "            labels_flat = tf.reshape(self.input_y, [-1])\n",
    "            mask = tf.cast(tf.sign(labels_flat), dtype=tf.float32)\n",
    "            cost = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=self.logits, labels=labels_flat)\n",
    "            self.loss = tf.reduce_sum(cost * mask) / batch_size  # 每个句子的平均损失\n",
    "\n",
    "    def sample(self, sess, words=reversed_dict, vocab=word_dict, num=10, prime_text='听听 那冷雨'):\n",
    "        state = sess.run(self.basic_cell.zero_state(1, tf.float32))\n",
    "        word_list = prime_text.split()\n",
    "        for word in word_list[:-1]:\n",
    "            x = np.zeros((1, 1))\n",
    "            x[0, 0] = vocab[word]\n",
    "            feed_dict = {self.input_x: x, self.initial_state:state}\n",
    "            [state] = sess.run([self.final_state], feed_dict=feed_dict)\n",
    "\n",
    "        out_sentence = prime_text\n",
    "        word = word_list[-1]\n",
    "        for n in range(num):\n",
    "            x = np.zeros((1, 1))\n",
    "            x[0, 0] = vocab[word]\n",
    "            feed_dict = {self.input_x: x, self.initial_state: state}\n",
    "            [model_output, state] = sess.run([self.model_output, self.final_state], feed_dict=feed_dict)\n",
    "            sample = np.argmax(model_output[0])\n",
    "            if sample == 0:\n",
    "                break\n",
    "            word = words[sample]\n",
    "            out_sentence = out_sentence + ' ' + word\n",
    "        return out_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "rnn = BasicRNN(number_steps, vocab_size, hidden_size, batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. 训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After  100 steps, perplexity is 14.38\n",
      "After  200 steps, perplexity is 4.67\n",
      "听听 那冷雨 雨滴 椽 的 意趣 两次 神州 的 意趣 两次 神州\n",
      "=-==-==-==-==-==-==-==-==-==-==-==-=\n",
      "After  300 steps, perplexity is 2.97\n",
      "After  400 steps, perplexity is 2.34\n",
      "听听 那冷雨 在 日式 的 境界 水迢 的 境界 水迢 的 境界\n",
      "=-==-==-==-==-==-==-==-==-==-==-==-=\n",
      "After  500 steps, perplexity is 2.03\n",
      "After  600 steps, perplexity is 1.84\n",
      "听听 那冷雨 在 旭日 在 旭日 在 旭日 在 旭日 在 旭日\n",
      "=-==-==-==-==-==-==-==-==-==-==-==-=\n",
      "After  700 steps, perplexity is 1.71\n",
      "After  800 steps, perplexity is 1.62\n",
      "听听 那冷雨 着 星期三 飘云 在 古老 的 意趣 毕竟 是 一种\n",
      "=-==-==-==-==-==-==-==-==-==-==-==-=\n",
      "After  900 steps, perplexity is 1.56\n",
      "After 1000 steps, perplexity is 1.50\n",
      "听听 那冷雨 在 石 秋 雨潇潇 台风 台雨 在 石 秋 雨潇潇\n",
      "=-==-==-==-==-==-==-==-==-==-==-==-=\n"
     ]
    }
   ],
   "source": [
    "train_step = tf.train.AdamOptimizer(lr).minimize(rnn.loss)\n",
    "saver = tf.train.Saver()\n",
    "with tf.Session() as sess:\n",
    "    with tf.variable_scope(tf.get_variable_scope(), reuse=True):\n",
    "        test_basic_model = BasicRNN(\n",
    "            number_steps, vocab_size, hidden_size, \n",
    "            batch_size, infer_sample=True)\n",
    "    costs = 0.0\n",
    "    iters = 0\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for i in range(epochs): \n",
    "        batch_data = train_data[np.random.randint(\n",
    "            train_data.shape[0], size=batch_size), :]\n",
    "        X = batch_data[:, :number_steps]\n",
    "        Y = batch_data[:, number_steps:]\n",
    "        feed_dict = {rnn.input_x: X, rnn.input_y: Y}\n",
    "        sess.run(train_step, feed_dict=feed_dict)\n",
    "        cross_entropy = sess.run(rnn.loss, feed_dict=feed_dict)\n",
    "        costs += cross_entropy\n",
    "        iters += number_steps\n",
    "        perplexity = np.exp(costs / iters)\n",
    "        if i % print_loss_every == 0 and i != 0:\n",
    "            print('After {:4d} steps, perplexity is {:.2f}'.format(i, perplexity))\n",
    "        if i % eval_every == 0 and i != 0:\n",
    "            print(test_basic_model.sample(sess, num=10))\n",
    "            print('=-=' * 12)\n",
    "    # After training, save the final sess\n",
    "    saver.save(sess, 'save_model/rnn-model.ckpt', global_step=epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Refrences:\n",
    "\n",
    "* [assignment2.pdf](http://web.stanford.edu/class/cs224n/assignment2/assignment2.pdf)\n",
    "* [The Unreasonable Effectiveness of Recurrent Neural Networks](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "87px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": false,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
