{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 环境说明："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scott Ming 2017-04-01 \n",
      "\n",
      "CPython 3.5.2\n",
      "IPython 5.2.2\n",
      "\n",
      "numpy 1.12.1\n",
      "pandas 0.19.2\n",
      "matplotlib 2.0.0\n",
      "tensorflow 1.0.1\n",
      "\n",
      "compiler   : GCC 4.9.2\n",
      "system     : Linux\n",
      "release    : 3.16.0-4-amd64\n",
      "machine    : x86_64\n",
      "processor  : \n",
      "CPU cores  : 4\n",
      "interpreter: 64bit\n"
     ]
    }
   ],
   "source": [
    "%load_ext watermark\n",
    "%watermark -a 'Scott Ming' -v -m -d -p numpy,pandas,matplotlib,tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 4. 神经网络语言模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import jieba\n",
    "import random\n",
    "import os\n",
    "import re\n",
    "import string\n",
    "import requests\n",
    "import collections\n",
    "import io\n",
    "import tarfile\n",
    "import gzip\n",
    "import zhon.hanzi as zh\n",
    "from nltk.corpus import stopwords\n",
    "from tensorflow.python.framework import ops\n",
    "ops.reset_default_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 4.1 读取并清理数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def read_data(filename):\n",
    "    \"\"\"Read and cut texts, return a long string.\"\"\"\n",
    "    jieba.setLogLevel(20)\n",
    "    jieba.enable_parallel(4)\n",
    "    with open(filename, 'r') as f:\n",
    "        data = f.read()\n",
    "    texts = ' '.join(jieba.cut(data))\n",
    "#     texts = re.split(r'[{}|\\n]'.format(zh.punctuation), texts) # 按标点分会太稀疏\n",
    "    texts = texts.split('\\n')\n",
    "    return texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "读取 stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "with open('stop_words_chinese.txt') as f:\n",
    "    chinese_stops = f.read()\n",
    "    chinese_stops = chinese_stops.split('\\n')\n",
    "    \n",
    "english_stops = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.1 s, sys: 104 ms, total: 1.21 s\n",
      "Wall time: 1.76 s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "texts = read_data('tst_jin.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.13 s, sys: 4 ms, total: 2.14 s\n",
      "Wall time: 2.13 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def clean_text(texts, chinese_stops, english_stops=None):\n",
    "    # 把大写单词转为小写\n",
    "    texts = [x.lower() for x in texts]\n",
    "    # 去除英文标点\n",
    "    texts = [''.join(c for c in x if c not in string.punctuation) for x in texts]\n",
    "    # 去除中文标点符号, 上面以句子作为切分所以不需要再清楚中文标点了\n",
    "    texts = [''.join(c for c in x if c not in zh.punctuation) for x in texts]\n",
    "    # 去除中文停止词(停止词里面有数字，就不另外清理数字了)\n",
    "    texts = [' '.join([word for word in x.split() if word not in (chinese_stops)]) for x in texts]\n",
    "    # 去除英文停止词\n",
    "    texts = [' '.join([word for word in x.split() if word not in (english_stops)]) for x in texts]\n",
    "    # 清楚多余的空格\n",
    "    texts = [' '.join(x.split()) for x in texts]\n",
    "    return texts\n",
    "\n",
    "texts = clean_text(texts, chinese_stops, english_stops)\n",
    "\n",
    "\n",
    "# Texts must contain at least 3 words\n",
    "texts = [x for x in texts if len(x.split()) > 2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 4.2 定义模型参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "batch_size = 50         # 每次训练的 batch_size\n",
    "embedding_size = 200    # 即将单词转为稠密向量的维度\n",
    "vocabulary_size = 10000 # 预计进入训练的单词，这里设的是 top 10000\n",
    "generations = 5000     # 训练次数\n",
    "print_loss_every = 500  # 迭代 500 次，打印一次 loss\n",
    "\n",
    "num_sampled = int(batch_size/2)  # 负样本的噪声单词熟练\n",
    "window_size = 2         # How many words to consider left and right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "print_valid_every = 2000\n",
    "valid_words = '说 杀 内力'.split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 4.3 生成字典"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Build dictionary of words\n",
    "def build_dictionary(sentences, vocabulary_size):\n",
    "    # Turn sentences (list of strings) into lists of words\n",
    "    split_sentences = [s.split() for s in sentences]\n",
    "    words = [x for sublist in split_sentences for x in sublist]\n",
    "    \n",
    "    # Initialize list of [word, word_count] for each word, starting with unknown\n",
    "    count = [['RARE', -1]]\n",
    "    \n",
    "    # Now add most frequent words, limited to the N-most frequent (N=vocabulary size)\n",
    "    count.extend(collections.Counter(words).most_common(vocabulary_size-1))\n",
    "    \n",
    "    # Now create the dictionary\n",
    "    word_dict = {}\n",
    "    # For each word, that we want in the dictionary, add it, then make it\n",
    "    # the value of the prior dictionary length\n",
    "    for word, word_count in count:\n",
    "        word_dict[word] = len(word_dict)\n",
    "    \n",
    "    return word_dict, count  # 返回 count 方便查看频率和验证词"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 4.4 单词转为数字"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def text_to_numbers(sentences, word_dict):\n",
    "    # Initialize the returned data\n",
    "    data = []\n",
    "    for sentence in sentences:\n",
    "        sentence_data = []\n",
    "        # For each word, either use selected index or rare word index\n",
    "        for word in sentence:\n",
    "            if word in word_dict:\n",
    "                word_ix = word_dict[word]\n",
    "            else:\n",
    "                word_ix = 0\n",
    "            sentence_data.append(word_ix)\n",
    "        data.append(sentence_data)\n",
    "    return(data)\n",
    "\n",
    "# Build our data set and dictionaries\n",
    "word_dictionary, count = build_dictionary(texts, vocabulary_size)\n",
    "word_dictionary_rev = dict(zip(word_dictionary.values(), word_dictionary.keys()))\n",
    "text_data = text_to_numbers(texts, word_dictionary)\n",
    "\n",
    "# Get validation word keys\n",
    "valid_examples = [word_dictionary[x] for x in valid_words]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 4.5 生成 word2vec 训练样本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Generate data randomly (N words behind, target, N words ahead)\n",
    "def generate_batch_data(sentences, batch_size, window_size, method='skip_gram'):\n",
    "    # Fill up data batch\n",
    "    batch_data = []\n",
    "    label_data = []\n",
    "    while len(batch_data) < batch_size:\n",
    "        # select random sentence to start\n",
    "        rand_sentence = np.random.choice(sentences)\n",
    "        # Generate consecutive windows to look at\n",
    "        window_sequences = [rand_sentence[max((ix-window_size),0):(ix+window_size+1)] for ix, x in enumerate(rand_sentence)]\n",
    "        # Denote which element of each window is the center word of interest\n",
    "        label_indices = [ix if ix<window_size else window_size for ix,x in enumerate(window_sequences)]\n",
    "        \n",
    "        # Pull out center word of interest for each window and create a tuple for each window\n",
    "        if method=='skip_gram':\n",
    "            batch_and_labels = [(x[y], x[:y] + x[(y+1):]) for x,y in zip(window_sequences, label_indices)]\n",
    "            # Make it in to a big list of tuples (target word, surrounding word)\n",
    "            tuple_data = [(x, y_) for x,y in batch_and_labels for y_ in y]\n",
    "        elif method=='cbow':\n",
    "            batch_and_labels = [(x[:y] + x[(y+1):], x[y]) for x,y in zip(window_sequences, label_indices)]\n",
    "            # Make it in to a big list of tuples (target word, surrounding word)\n",
    "            tuple_data = [(x_, y) for x,y in batch_and_labels for x_ in x]\n",
    "        else:\n",
    "            raise ValueError('Method {} not implemented yet.'.format(method))\n",
    "            \n",
    "        # extract batch and labels\n",
    "        batch, labels = [list(x) for x in zip(*tuple_data)]\n",
    "        batch_data.extend(batch[:batch_size])\n",
    "        label_data.extend(labels[:batch_size])\n",
    "    # Trim batch and label at the end\n",
    "    batch_data = batch_data[:batch_size]\n",
    "    label_data = label_data[:batch_size]\n",
    "    \n",
    "    # Convert to numpy array\n",
    "    batch_data = np.array(batch_data)\n",
    "    label_data = np.transpose(np.array([label_data]))\n",
    "    \n",
    "    return(batch_data, label_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "batch_data, label_data = generate_batch_data(text_data, batch_size, window_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2651, 2651,  316,  316,  316,  423,  423,  423,  423,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,  948,  948,  948,  948,    0,\n",
       "          0,    0,    0,  917,  917,  917,  917,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,  902,  902,  902,  902,    0,    0,    0,\n",
       "          0,  356,  356,  356,  356,    0])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 4.6 构建模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### 1. 定义神经网络的结果和前向传播的输出结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Define Embeddings:\n",
    "embeddings = tf.Variable(tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0))\n",
    "\n",
    "# NCE loss parameters\n",
    "nce_weights = tf.Variable(tf.truncated_normal([vocabulary_size, embedding_size],\n",
    "                                               stddev=1.0 / np.sqrt(embedding_size)))\n",
    "nce_biases = tf.Variable(tf.zeros([vocabulary_size]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### 2. 定义前向传播过程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Create data/target placeholders\n",
    "x_inputs = tf.placeholder(tf.int32, shape=[batch_size])\n",
    "y_target = tf.placeholder(tf.int32, shape=[batch_size, 1])\n",
    "valid_dataset = tf.constant(valid_examples, dtype=tf.int32)\n",
    "\n",
    "# Lookup the word embedding:\n",
    "embed = tf.nn.embedding_lookup(embeddings, x_inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### 3. 定义损失函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Get loss from prediction\n",
    "loss = tf.reduce_mean(tf.nn.nce_loss(weights=nce_weights,\n",
    "                                     biases=nce_biases,\n",
    "                                     labels=y_target,\n",
    "                                     inputs=embed,\n",
    "                                     num_sampled=num_sampled,\n",
    "                                     num_classes=vocabulary_size))\n",
    "\n",
    "# Create optimizer\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=1.0).minimize(loss)\n",
    "\n",
    "# Cosine similarity between words\n",
    "norm = tf.sqrt(tf.reduce_sum(tf.square(embeddings), 1, keep_dims=True))\n",
    "normalized_embeddings = embeddings / norm\n",
    "valid_embeddings = tf.nn.embedding_lookup(normalized_embeddings, valid_dataset)\n",
    "similarity = tf.matmul(valid_embeddings, normalized_embeddings, transpose_b=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### 4. 创建一个会话来运行TensorFlow程序。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at step 500 : 63.2232780456543\n",
      "Loss at step 1000 : 3.889535903930664\n",
      "Loss at step 1500 : 18.402950286865234\n",
      "Loss at step 2000 : 22.192529678344727\n",
      "Nearest to 说: 紧, 怔, 底, 田, 露,\n",
      "Nearest to 杀: 渐, 般, 显, 养, 前,\n",
      "Nearest to 内力: 想不起, 一说, 这时候, 洞小头, 没法,\n",
      "Loss at step 2500 : 28.179908752441406\n",
      "Loss at step 3000 : 12.110696792602539\n",
      "Loss at step 3500 : 2.838730573654175\n",
      "Loss at step 4000 : 5.9628400802612305\n",
      "Nearest to 说: RARE, 紧, 底, 握, 怔,\n",
      "Nearest to 杀: RARE, 养, 渐, 虚, 前,\n",
      "Nearest to 内力: 想不起, 一说, 这时候, 洞小头, 没法,\n",
      "Loss at step 4500 : 13.232660293579102\n",
      "Loss at step 5000 : 7.305245876312256\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess.run(init)\n",
    "    # Run the skip gram model.\n",
    "    loss_vec = []\n",
    "    loss_x_vec = []\n",
    "    for i in range(generations):\n",
    "        batch_inputs, batch_labels = generate_batch_data(text_data, batch_size, window_size)\n",
    "        feed_dict = {x_inputs : batch_inputs, y_target : batch_labels}\n",
    "    \n",
    "        # Run the train step\n",
    "        sess.run(optimizer, feed_dict=feed_dict)\n",
    "    \n",
    "        # Return the loss\n",
    "        if (i+1) % print_loss_every == 0:\n",
    "            loss_val = sess.run(loss, feed_dict=feed_dict)\n",
    "            loss_vec.append(loss_val)\n",
    "            loss_x_vec.append(i+1)\n",
    "            print(\"Loss at step {} : {}\".format(i+1, loss_val))\n",
    "          \n",
    "        # Validation: Print some random words and top 5 related words\n",
    "        if (i+1) % print_valid_every == 0:\n",
    "            sim = sess.run(similarity, feed_dict=feed_dict)\n",
    "            for j in range(len(valid_words)):\n",
    "                valid_word = word_dictionary_rev[valid_examples[j]]\n",
    "                top_k = 5 # number of nearest neighbors\n",
    "                nearest = (-sim[j, :]).argsort()[1:top_k+1]\n",
    "                log_str = \"Nearest to {}:\".format(valid_word)\n",
    "                for k in range(top_k):\n",
    "                    close_word = word_dictionary_rev[nearest[k]]\n",
    "                    log_str = \"%s %s,\" % (log_str, close_word)\n",
    "                print(log_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Refrences:\n",
    "\n",
    "* [原始论文](https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf)\n",
    "* [Approximating the Softmax for Learning Word Embeddings](http://sebastianruder.com/word-embeddings-softmax/)\n",
    "* [word2vec 中的数学原理详解（多图，WIFI下阅读） - 机器学习 - 算法组](http://suanfazu.com/t/word2vec-zhong-de-shu-xue-yuan-li-xiang-jie-duo-tu-wifixia-yue-du/178)\n",
    "* [Word2Vec原理之层次Softmax算法 | 一灯@qiancy.com](http://qiancy.com/2016/08/17/word2vec-hierarchical-softmax/)\n",
    "* [Tensorflow 的Word2vec demo解析 - 阁子 - 博客园](http://www.cnblogs.com/rocketfan/p/4976806.html)\n",
    "* [Word2Vec-知其然知其所以然 - 作业部落 Cmd Markdown 编辑阅读器](https://www.zybuluo.com/Dounm/note/591752#322-使用negative-sampling优化)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
