{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 构建 word embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 思路：\n",
    "\n",
    "1. 读取数据\n",
    "2. 清理数据\n",
    "3. 创建词典、reverce_dict、count\n",
    "4. 生成随机样本\n",
    "5. 建立模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 环境说明："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scott Ming 2017-04-23 \n",
      "\n",
      "CPython 3.6.0\n",
      "IPython 6.0.0\n",
      "\n",
      "numpy 1.12.1\n",
      "pandas 0.19.2\n",
      "matplotlib 2.0.0\n",
      "tensorflow 1.0.1\n",
      "\n",
      "compiler   : GCC 4.9.2\n",
      "system     : Linux\n",
      "release    : 3.16.0-4-amd64\n",
      "machine    : x86_64\n",
      "processor  : \n",
      "CPU cores  : 4\n",
      "interpreter: 64bit\n"
     ]
    }
   ],
   "source": [
    "%load_ext watermark\n",
    "%watermark -a 'Scott Ming' -v -m -d -p numpy,pandas,matplotlib,tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import jieba\n",
    "import collections\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "import zipfile\n",
    "import string\n",
    "import numpy as np\n",
    "import urllib.request\n",
    "import tensorflow as tf\n",
    "import zhon.hanzi as zh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 读取数据\n",
    "\n",
    "为了方便，直接用官方的数据集了"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def maybe_download(filename, expected_bytes):\n",
    "  \"\"\"Download a file if not present, and make sure it's the right size.\"\"\"\n",
    "  if not os.path.exists(filename):  # 如果不存在，就下载\n",
    "    filename, _ = urllib.request.urlretrieve(url + filename, filename)\n",
    "  statinfo = os.stat(filename)\n",
    "  if statinfo.st_size == expected_bytes:\n",
    "    print('Found and verified', filename)\n",
    "  else:\n",
    "    print(statinfo.st_size)\n",
    "    raise Exception(\n",
    "        'Failed to verify ' + filename + '. Can you get to it with a browser?')\n",
    "  return filename\n",
    "\n",
    "\n",
    "def read_data(filename):\n",
    "  \"\"\"Extract the first file enclosed in a zip file as a list of words\"\"\"\n",
    "  with zipfile.ZipFile(filename) as f:\n",
    "    data = tf.compat.as_str(f.read(f.namelist()[0])).split()\n",
    "  return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found and verified text8.zip\n",
      "Data size 800000\n"
     ]
    }
   ],
   "source": [
    "filename = maybe_download('text8.zip', 31344016)\n",
    "words = read_data(filename)[:800000]  # 读取成 list\n",
    "print('Data size', len(words))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "设定高频截取及确认词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocabulary_size = 10000\n",
    "window_size = 2  # 一个词最远能联系到的距离，如果为 1，说明左右是两个\n",
    "valid_words = ['were', 'five', 'three', 'which']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 创建词典和编号及训练数据"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "创建一个词典包含所有单词的编号"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_dictionary(words, vocabulary_size):\n",
    "    count = [['RARE', -1]]\n",
    "    count.extend(collections.Counter(words).most_common(vocabulary_size-1))\n",
    "    # 创建编号字典\n",
    "    word_dict = {}\n",
    "    for word, word_count in count:\n",
    "        word_dict[word] = len(word_dict)\n",
    "    reverse_dict = dict(zip(word_dict.values(), word_dict.keys()))\n",
    "    return word_dict, reverse_dict, count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "word_dict, reverse_dict, count = build_dictionary(words, vocabulary_size)\n",
    "valid_examples = [word_dict[x] for x in valid_words]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "给语料中的所有单词编号"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def words_to_number(words, words_dict):\n",
    "    words_num = []\n",
    "    unk_count = 0\n",
    "    for word in words:\n",
    "        if word in word_dict:\n",
    "            index = word_dict[word]\n",
    "        else:\n",
    "            index = 0  # words_dict[0]\n",
    "            unk_count += 1 \n",
    "        words_num.append(index)   \n",
    "    return words_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "words_num = words_to_number(words, word_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "创建输入数据和 target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_data(words_num, window_size):\n",
    "    window_sequences = [words_num[max((ix-window_size),0):(ix+window_size+1)] for ix, x in enumerate(words_num)]\n",
    "    # 生成中心词的索引\n",
    "    label_indices = [ix if ix<window_size else window_size for ix,x in enumerate(window_sequences)]\n",
    "    # 利用 list 相加会合并成长 list 的特性把两边词和中间词分开\n",
    "    input_and_labels = [(x[:y] + x[(y+1):], x[y]) for x,y in zip(window_sequences, label_indices)]\n",
    "    # cbow 需要至少 2*windows_size 的维度\n",
    "    input_and_labels = [(x,y) for x,y in input_and_labels if len(x)==2*window_size]\n",
    "    input_data, label_data = [list(x) for x in zip(*input_and_labels)]\n",
    "    # 各自转为 array\n",
    "    input_data = np.array(input_data)  # n, 2 dim\n",
    "    label_data = np.transpose(np.array([label_data]))  # n, 1 dim\n",
    "    \n",
    "    return input_data, label_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_data, label_data = build_data(words_num, window_size)\n",
    "datasize = input_data.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 模型构建"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. 定义模型基本参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 128        # How many sets of words to train on at once.\n",
    "embedding_size = 200    # The embedding size of each word to train.\n",
    "generations = 50000     # How many iterations we will perform the training on.\n",
    "print_loss_every = 2000  # Print the loss every so many iterations\n",
    "num_sampled = int(batch_size/2)  # 负样本数量\n",
    "model_learning_rate = 0.01  # 学习率\n",
    "print_valid_every = 5000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. 定义 embedding 等参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define Embeddings:\n",
    "embeddings = tf.Variable(tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0))\n",
    "\n",
    "# NCE loss 相关参数\n",
    "nce_weights = tf.Variable(tf.truncated_normal([vocabulary_size, embedding_size],\n",
    "                                               stddev=1.0 / np.sqrt(embedding_size)))\n",
    "nce_biases = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "\n",
    "# Create data/target placeholders\n",
    "x_inputs = tf.placeholder(tf.int32, shape=[batch_size, 2*window_size])\n",
    "y_target = tf.placeholder(tf.int32, shape=[batch_size, 1])\n",
    "valid_dataset = tf.constant(valid_examples, dtype=tf.int32)\n",
    "\n",
    "# Lookup the word embedding\n",
    "# Embedding 相加\n",
    "embed = tf.zeros([batch_size, embedding_size])\n",
    "for element in range(2*window_size):\n",
    "    embed += tf.nn.embedding_lookup(embeddings, x_inputs[:, element])  # 左右两边的 words embedding 相加"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. 定义损失函数及相似度计算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get loss from prediction\n",
    "loss = tf.reduce_mean(tf.nn.nce_loss(weights=nce_weights,\n",
    "                                     biases=nce_biases,\n",
    "                                     labels=y_target,\n",
    "                                     inputs=embed,\n",
    "                                     num_sampled=num_sampled,\n",
    "                                     num_classes=vocabulary_size), name='loss')\n",
    "\n",
    "# Create optimizer\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=model_learning_rate).minimize(loss)\n",
    "\n",
    "# 计算 cosine 相似度\n",
    "norm = tf.sqrt(tf.reduce_sum(tf.square(embeddings), 1, keep_dims=True))\n",
    "normalized_embeddings = embeddings / norm\n",
    "valid_embeddings = tf.nn.embedding_lookup(normalized_embeddings, valid_dataset)\n",
    "similarity = tf.matmul(valid_embeddings, normalized_embeddings, transpose_b=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loss_summary = tf.summary.scalar('loss', loss)\n",
    "embed_summary = tf.summary.histogram('embed', embed)\n",
    "merged = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "writer = tf.summary.FileWriter('tf_log/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at step 2000 : 44.070274353027344\n",
      "Loss at step 4000 : 29.16555404663086\n",
      "Nearest to were: to, gurps, pride, tactic, contest, the, in, singer, this, descended,\n",
      "Nearest to five: RARE, the, to, a, one, that, is, of, and, as,\n",
      "Nearest to three: is, in, the, and, RARE, of, by, to, one, a,\n",
      "Nearest to which: the, RARE, by, three, and, of, s, to, in, nine,\n",
      "Loss at step 6000 : 18.81061553955078\n",
      "Loss at step 8000 : 20.462358474731445\n",
      "Loss at step 10000 : 23.814836502075195\n",
      "Nearest to were: to, in, this, and, the, by, be, of, for, also,\n",
      "Nearest to five: RARE, to, the, a, one, that, is, as, and, of,\n",
      "Nearest to three: is, in, by, the, and, of, RARE, his, to, that,\n",
      "Nearest to which: by, the, RARE, and, s, to, three, of, in, that,\n",
      "Loss at step 12000 : 13.884153366088867\n",
      "Loss at step 14000 : 11.480175018310547\n",
      "Nearest to were: to, this, in, by, be, for, and, also, the, an,\n",
      "Nearest to five: to, RARE, that, a, one, is, as, the, be, s,\n",
      "Nearest to three: is, in, by, and, his, the, that, of, to, one,\n",
      "Nearest to which: by, s, the, and, RARE, three, to, that, in, of,\n",
      "Loss at step 16000 : 14.285329818725586\n",
      "Loss at step 18000 : 8.717602729797363\n",
      "Loss at step 20000 : 13.722172737121582\n",
      "Nearest to were: to, this, by, be, for, in, also, an, and, that,\n",
      "Nearest to five: to, that, RARE, one, a, is, as, be, s, or,\n",
      "Nearest to three: is, by, in, his, and, that, the, s, which, one,\n",
      "Nearest to which: by, s, three, and, that, to, the, RARE, for, with,\n",
      "Loss at step 22000 : 12.374106407165527\n",
      "Loss at step 24000 : 13.273762702941895\n",
      "Nearest to were: to, this, by, be, for, also, an, in, and, s,\n",
      "Nearest to five: to, that, RARE, one, is, be, as, a, or, on,\n",
      "Nearest to three: is, by, in, his, that, and, which, s, are, he,\n",
      "Nearest to which: by, s, three, that, with, for, and, to, the, RARE,\n",
      "Loss at step 26000 : 11.794184684753418\n",
      "Loss at step 28000 : 7.689300060272217\n",
      "Loss at step 30000 : 13.97060775756836\n",
      "Nearest to were: to, this, be, by, also, for, an, in, that, s,\n",
      "Nearest to five: that, to, be, one, RARE, as, or, is, on, s,\n",
      "Nearest to three: is, by, his, in, that, which, are, s, and, he,\n",
      "Nearest to which: by, s, three, that, with, for, his, at, the, to,\n",
      "Loss at step 32000 : 10.668107986450195\n",
      "Loss at step 34000 : 9.543588638305664\n",
      "Nearest to were: this, to, be, by, also, for, an, in, but, that,\n",
      "Nearest to five: that, to, be, or, RARE, one, as, on, is, s,\n",
      "Nearest to three: is, by, his, in, which, that, he, are, s, this,\n",
      "Nearest to which: by, s, three, that, with, his, for, at, many, RARE,\n",
      "Loss at step 36000 : 14.819843292236328\n",
      "Loss at step 38000 : 8.74028205871582\n",
      "Loss at step 40000 : 11.190340042114258\n",
      "Nearest to were: this, to, be, also, by, an, for, but, all, which,\n",
      "Nearest to five: that, be, to, or, as, RARE, on, one, at, is,\n",
      "Nearest to three: is, by, his, which, in, that, he, are, this, s,\n",
      "Nearest to which: by, three, s, that, his, many, with, at, for, RARE,\n",
      "Loss at step 42000 : 9.226072311401367\n",
      "Loss at step 44000 : 12.255928039550781\n",
      "Nearest to were: this, be, to, also, by, an, for, all, but, which,\n",
      "Nearest to five: that, be, or, to, at, on, as, RARE, one, s,\n",
      "Nearest to three: is, by, his, which, in, that, he, are, this, s,\n",
      "Nearest to which: by, three, s, many, that, at, his, with, for, an,\n",
      "Loss at step 46000 : 5.321532726287842\n",
      "Loss at step 48000 : 7.101309299468994\n",
      "Loss at step 50000 : 8.091510772705078\n",
      "Nearest to were: this, be, also, to, by, an, for, all, but, which,\n",
      "Nearest to five: that, be, or, at, to, on, RARE, as, one, three,\n",
      "Nearest to three: is, by, his, which, in, he, that, are, this, s,\n",
      "Nearest to which: by, three, s, many, at, that, his, with, for, an,\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess.run(init)\n",
    "\n",
    "    # Run the cbow gram model.\n",
    "    loss_vec = []\n",
    "    loss_x_vec = []\n",
    "    train_step = 50000\n",
    "    for i in range(train_step):\n",
    "        start = (i*batch_size) % (datasize - (datasize % batch_size))\n",
    "        end = (i*batch_size) % (datasize - (datasize % batch_size)) + batch_size\n",
    "        feed_dict={x_inputs: input_data[start:end], y_target: label_data[start:end]}\n",
    "        # Run the train step\n",
    "        _, merged_summary = sess.run([optimizer, merged], feed_dict=feed_dict)\n",
    "        writer.add_summary(merged_summary, i)\n",
    "    \n",
    "        # Return the loss\n",
    "        if (i+1) % print_loss_every == 0:\n",
    "            loss_val = sess.run(loss, feed_dict=feed_dict)\n",
    "            loss_vec.append(loss_val)\n",
    "            loss_x_vec.append(i+1)\n",
    "            print(\"Loss at step {} : {}\".format(i+1, loss_val))\n",
    "        \n",
    "        if (i+1) % print_valid_every == 0:\n",
    "            sim = sess.run(similarity, feed_dict=feed_dict)\n",
    "            for j in range(len(valid_words)):\n",
    "                valid_word = reverse_dict[valid_examples[j]]\n",
    "                top_k = 10 # number of nearest neighbors\n",
    "                nearest = (-sim[j, :]).argsort()[1:top_k+1]\n",
    "                log_str = \"Nearest to {}:\".format(valid_word)\n",
    "                for k in range(top_k):\n",
    "                    close_word = reverse_dict[nearest[k]]\n",
    "                    log_str = \"%s %s,\" % (log_str, close_word)\n",
    "                print(log_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Refrences:\n",
    "\n",
    "### CS224n & CS224d:\n",
    "\n",
    "* 课程索引：\n",
    "    * [Stanford University CS224d: Deep Learning for Natural Language Processing](http://cs224d.stanford.edu/syllabus.html)\n",
    "    * [CS224n: Natural Language Processing with Deep Learning](http://web.stanford.edu/class/cs224n/syllabus.html)\n",
    "* 参考资料：\n",
    "    * [Word2Vec Tutorial](http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/)\n",
    "* Slides:\n",
    "    * [cs224n slide leture2 p13](http://web.stanford.edu/class/cs224n/lectures/cs224n-2017-lecture2.pdf)\n",
    "    * [cs224d slide leture2](http://cs224d.stanford.edu/lectures/CS224d-Lecture2.pdf)\n",
    "* 视频：\n",
    "    * [CS224d Lecture 2](https://www.youtube.com/watch?v=aRqn8t1hLxs&list=PLlJy-eBtNFt4CSVWYqscHDdP58M3zFHIG)\n",
    "    * [CS224d Lecture 3](https://www.youtube.com/watch?v=CP9bIt4IPVo&index=3&list=PLlJy-eBtNFt4CSVWYqscHDdP58M3zFHIG)\n",
    "* 笔记：\n",
    "    * [cs224d lecture2 note](http://www.52nlp.cn/%E6%96%AF%E5%9D%A6%E7%A6%8F%E5%A4%A7%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%8E%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%AC%E4%BA%8C%E8%AE%B2%E8%AF%8D%E5%90%91%E9%87%8F)\n",
    "\n",
    "### 其他资料:\n",
    "\n",
    "* word2vec 解析：\n",
    "    * [原始论文](https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf)\n",
    "    * [Approximating the Softmax for Learning Word Embeddings](http://sebastianruder.com/word-embeddings-softmax/)\n",
    "    * [word2vec 中的数学原理详解（多图，WIFI下阅读） - 机器学习 - 算法组](http://suanfazu.com/t/word2vec-zhong-de-shu-xue-yuan-li-xiang-jie-duo-tu-wifixia-yue-du/178)\n",
    "    * [Word2Vec原理之层次Softmax算法 | 一灯@qiancy.com](http://qiancy.com/2016/08/17/word2vec-hierarchical-softmax/)\n",
    "    * [Tensorflow 的Word2vec demo解析 - 阁子 - 博客园](http://www.cnblogs.com/rocketfan/p/4976806.html)\n",
    "    * [Word2Vec-知其然知其所以然 - 作业部落 Cmd Markdown 编辑阅读器](https://www.zybuluo.com/Dounm/note/591752#322-使用negative-sampling优化)\n",
    "    \n",
    "* 问答：\n",
    "    * [Tensorflow 的NCE-Loss的实现和word2vec - 简书](http://www.jianshu.com/p/fab82fa53e16)\n",
    "    * [word2vec - TensorFlow Embedding Lookup - Stack Overflow](http://stackoverflow.com/questions/37897934/tensorflow-embedding-lookup)\n",
    "    \n",
    "* 书籍：\n",
    "    * [TensorFlow实战 (豆瓣)](https://book.douban.com/subject/26974266/) 第7章有关 word2vec 的实现\n",
    "    * [nfmcclure/tensorflow_cookbook: Code for Tensorflow Machine Learning Cookbook](https://github.com/nfmcclure/tensorflow_cookbook) 第7章"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "235px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": false,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
