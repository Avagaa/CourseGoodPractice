{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 1. 数据预处理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 查看数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14871 ../TED_en_test.txt\n",
      "300000 ../TED_en_train.txt\n",
      "14871 ../TED_zh_test.txt\n",
      "300000 ../TED_zh_train.txt\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "for i in ../*txt\n",
    "do\n",
    "    wc -l $i\n",
    "done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train 文件足足 30 万行，所以 shuffle 一部分行数查看编码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../TED_en_test.txt\n",
      "<stdin>: ascii with confidence 1.0\n",
      "../TED_en_train.txt\n",
      "<stdin>: ascii with confidence 1.0\n",
      "../TED_zh_test.txt\n",
      "<stdin>: utf-8 with confidence 0.99\n",
      "../TED_zh_train.txt\n",
      "<stdin>: utf-8 with confidence 0.99\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "for i in ../*txt\n",
    "do\n",
    "    echo $i\n",
    "    shuf -n 50 $i | chardet\n",
    "done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "└ 英文数据是 ASCII 编码，中文数据是 UTF-8 编码，Python3 两者都支持，所以不需用 `iconv` 转码了"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../TED_en_test.txt\n",
      "---------------\n",
      "We could use sales, anything you like.\r\n",
      "There it is: after some little fluctuations at the beginning,\r\n",
      "\n",
      "../TED_en_train.txt\n",
      "---------------\n",
      "It can be a very complicated thing, the ocean.\r\n",
      "And it can be a very complicated thing, what human health is.\r\n",
      "\n",
      "../TED_zh_test.txt\n",
      "---------------\n",
      "我们还可以用销售量 什么都行\r\n",
      "看 当公司进行革新\r\n",
      "\n",
      "../TED_zh_train.txt\n",
      "---------------\n",
      "海洋是一个非常复杂的事物。\r\n",
      "人类的健康也是一件非常复杂的事情。\r\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "for i in ../*txt\n",
    "do\n",
    "    echo $i\n",
    "    echo '---------------'\n",
    "    head -n 2 $i\n",
    "    echo ''\n",
    "done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "└ 英文符号有时对应的是空格，为了方便翻译，可以把这些先去除，提升文字翻译准确性"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 数据清理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "先创建个小数据文件方便测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "mkdir -p ../data && \\\n",
    "for i in ../*txt\n",
    "do\n",
    "    head -n 50 $i > \"../data/tst_${i#../}\"\n",
    "done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tst_TED_en_test.txt   tst_TED_zh_test.txt\r\n",
      "tst_TED_en_train.txt  tst_TED_zh_train.txt\r\n"
     ]
    }
   ],
   "source": [
    "!ls ../data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "读取数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import itertools\n",
    "import operator\n",
    "import string\n",
    "from collections import deque, Counter\n",
    "\n",
    "import jieba\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import zhon.hanzi as zh\n",
    "\n",
    "from tensorflow.contrib.legacy_seq2seq import basic_rnn_seq2seq, embedding_rnn_seq2seq, sequence_loss\n",
    "from tensorflow.python.ops import variable_scope"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`exec` 可把 str 转为 variable name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "basepath = '../data'\n",
    "for s in ('en_train', 'en_test', 'zh_train', 'zh_test'):\n",
    "    path = os.path.join(basepath, 'tst_TED_' + s + '.txt')\n",
    "    with open(path, 'r') as file:\n",
    "        exec(s + \" = file.read().splitlines()\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "中文数据先做分词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cut_chinese_data(sequences):\n",
    "    jieba.setLogLevel(20)                                                                       \n",
    "    jieba.enable_parallel(4)\n",
    "    for sequence in sequences:\n",
    "        data = jieba.cut(sequence)\n",
    "        yield ' '.join(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean_data(sequences):\n",
    "    sequences = (''.join(c for c in x if c not in string.punctuation) \n",
    "                           for x in sequences)\n",
    "    sequences = (''.join(c for c in x if c not in zh.punctuation) \n",
    "                           for x in sequences)\n",
    "    sequences = (x.split() for x in sequences)\n",
    "    sequences = [[c for c in x if c] for x in sequences]  # clean 之后多次复用，所以用 list\n",
    "    return sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 数据转置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "zh_train = cut_chinese_data(zh_train)\n",
    "zh_test = cut_chinese_data(zh_test)\n",
    "\n",
    "zh_train = clean_data(zh_train)\n",
    "en_train = clean_data(en_train)\n",
    "zh_test = clean_data(zh_test)\n",
    "en_test = clean_data(en_test)\n",
    "\n",
    "data = itertools.chain(zh_train, en_train, zh_test, en_test)  # Concat all data\n",
    "words = itertools.chain.from_iterable(data)  # Flat data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_common_words(words, n):\n",
    "    count = Counter(words)\n",
    "    count_dict = {key: value for key, value in count.items() if value > n}\n",
    "    word_counts = sorted(count_dict.items(), \n",
    "                         key=operator.itemgetter(1), reverse=True)\n",
    "    return word_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_dict(word_counts):\n",
    "    count = [['<UNK>', -1]]\n",
    "    count.extend(word_counts)\n",
    "    vocab2ix = {key: ix for ix, (key, _) in enumerate(count)}\n",
    "    vocab2ix['<GO>'] = max(vocab2ix.values()) + 1\n",
    "    vocab2ix['<EOS>'] = max(vocab2ix.values()) + 1\n",
    "    ix2vocab = {value: key for key, value in vocab2ix.items()}\n",
    "    return vocab2ix, ix2vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def word_to_number(sequences, word_dict):\n",
    "    data = []\n",
    "    for sequence in sequences:\n",
    "        sequence_data = []\n",
    "        for word in sequence:\n",
    "            try:\n",
    "                sequence_data.append(word_dict[word])\n",
    "            except:\n",
    "                sequence_data.append(0)\n",
    "        data.append(sequence_data)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def append_go_eos(nested_list):\n",
    "    # 因为生成器只能**遍历一次**，而遍历 deque_ 后，\n",
    "    #     没有新的 references，所以第一行不能使用生成器\n",
    "    nested_list = [deque(list_) for list_ in nested_list]\n",
    "    for deque_ in nested_list:\n",
    "        deque_.appendleft('<GO>')\n",
    "        deque_.append('<EOS>')\n",
    "    nested_list = [list(deque_) for deque_ in nested_list]\n",
    "    return nested_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_counts = get_common_words(words, 0)\n",
    "vocab2ix, ix2vocab = build_dict(word_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "zh_train = append_go_eos(zh_train)\n",
    "zh_test  = append_go_eos(zh_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def seq2seq_pad(encoder_inputs, encoder_length, decoder_inputs, decoder_length, vocab, pad_symbol='<UNK>'):\n",
    "    \"\"\"\n",
    "    - encoder_input: A nested list of symbol str for encoding, length: batch_size\n",
    "    - encoder_length: max length of encoder input\n",
    "    - decoder_input: A nested list of symbol str for decoding, length: batch_size\n",
    "    - decoder_length: max length of decoder input\n",
    "    - vocab: vocabulary index, symbol (str) -> index (int)\n",
    "    \n",
    "    Example: \n",
    "    [\"hello\", \"world\"] -> [\"hi\", \"<EOS>\"]\n",
    "    [\"cover\", \"me\"] -> [\"roger\", \"<EOS>\"]\n",
    "    \n",
    "    seq2seq_pad([['hello', 'world'], ['cover', 'me']], 4, [['hi', '<EOS>'], ['roger', '<EOS>']], 4, vocab)\n",
    "    \n",
    "    Assume that index of \"<PAD>\" is 0\n",
    "\n",
    "    Output:\n",
    "    [[0, 0, <index of 'hello'>, <index of 'world'>], [0, 0, <index of 'cover'>, <index of 'me'>]],\n",
    "    [[<index of 'hi'>, <index of 'EOS'>, 0, 0], [<index of 'roger'>, <index of 'EOS'>, 0, 0]]\n",
    "    \"\"\"\n",
    "    pad_index = vocab[pad_symbol]\n",
    "    def to_index(inputs, length, pad_from_start=True):\n",
    "        inputs_to_index = []\n",
    "        for cur_input in inputs:\n",
    "            cur_input_to_index = [pad_index] * length\n",
    "            l = len(cur_input)\n",
    "            if l < length:\n",
    "                if pad_from_start:\n",
    "                    cur_input_to_index[(length - l):] = [vocab[i] for i in cur_input]\n",
    "                else:\n",
    "                    cur_input_to_index[:l] = [vocab[i] for i in cur_input]\n",
    "            else:\n",
    "                cur_input_to_index = [vocab[i] for i in cur_input[:length]]\n",
    "            inputs_to_index.append(cur_input_to_index)    \n",
    "        return inputs_to_index\n",
    "    return to_index(encoder_inputs, encoder_length, True), to_index(decoder_inputs, decoder_length, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 构建模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "en_train 13\n",
      "zh_train 14\n",
      "en_test 13\n",
      "zh_test 12\n"
     ]
    }
   ],
   "source": [
    "for i, j in zip('en_train zh_train en_test zh_test'.split(),\n",
    "                (en_train, zh_train, en_test, zh_test)):\n",
    "    print(i, max(len(x) for x in j))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "encoder_length = max(len(x) for x in en_train)\n",
    "decoder_length = max(len(x) for x in zh_train)\n",
    "\n",
    "cell = tf.contrib.rnn.BasicRNNCell(128)\n",
    "num_encoder_symbols = len(vocab2ix)\n",
    "num_decoder_symbols = len(vocab2ix)\n",
    "batch_size = 5\n",
    "embedding_size = 128\n",
    "epochs = 50\n",
    "print_loss_every = 5\n",
    "\n",
    "\n",
    "encoder_placeholders = [tf.placeholder(tf.int32, shape=[None],\n",
    "                                       name=\"encoder_%d\" % i) for i in range(encoder_length)]\n",
    "decoder_placeholders = [tf.placeholder(tf.int32, shape=[None],\n",
    "                                       name=\"decoder_%d\" % i) for i in range(decoder_length)]\n",
    "target_placeholders = [tf.placeholder(tf.int32, shape=[None],\n",
    "                                       name=\"target_%d\" % i) for i in range(decoder_length)]\n",
    "target_weights_placeholders = [tf.placeholder(tf.float32, shape=[None],\n",
    "                                       name=\"decoder_weight_%d\" % i) for i in range(decoder_length)]\n",
    "outputs, states = embedding_rnn_seq2seq(\n",
    "    encoder_placeholders, decoder_placeholders, cell,\n",
    "    num_encoder_symbols, num_decoder_symbols,\n",
    "    embedding_size, output_projection=None,\n",
    "    feed_previous=False)\n",
    "\n",
    "loss = sequence_loss(outputs, target_placeholders, target_weights_placeholders)\n",
    "train_step = tf.train.AdamOptimizer(0.01).minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**数据准备步骤：**\n",
    "\n",
    "1. 先做 padding\n",
    "2. 通过 batch size 获取 mini-batch data\n",
    "3. left_shift \n",
    "4. 获取 feed_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_batch_data(en, zh, batch_size):\n",
    "    en = np.asarray(en)\n",
    "    zh = np.asarray(zh)\n",
    "    idx = np.random.randint(np.asarray(en).shape[0], size=batch_size)\n",
    "    return en[idx], zh[idx], idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def left_shift(decoder_inputs, pad_idx):\n",
    "    # for generating targets\n",
    "    return [list(input_[1:]) + [pad_idx] for input_ in decoder_inputs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_feed_dict(encoder_inputs, decoder_inputs):\n",
    "    encoder_inputs = list(zip(*encoder_inputs))\n",
    "    target_inputs = list(zip(*left_shift(decoder_inputs, vocab2ix['<UNK>'])))\n",
    "    decoder_inputs = list(zip(*decoder_inputs))\n",
    "    \n",
    "    feed_dict = dict()\n",
    "    # Prepare input data    \n",
    "    for (i, placeholder) in enumerate(encoder_placeholders):\n",
    "        # 这里用 placeholder 或者 placeholder.name 都可以\n",
    "        feed_dict[placeholder.name] = np.asarray(encoder_inputs[i], dtype=int)\n",
    "    for i in range(len(decoder_placeholders)):\n",
    "        feed_dict[decoder_placeholders[i].name] = np.asarray(decoder_inputs[i], dtype=int)\n",
    "        feed_dict[target_placeholders[i].name] = np.asarray(target_inputs[i], dtype=int)        \n",
    "        # 这里使用 weights 把 <PAD> 的损失屏蔽了\n",
    "        feed_dict[target_weights_placeholders[i].name] = np.asarray(\n",
    "            [float(idx != vocab2ix['<UNK>'])\n",
    "             for idx in target_inputs[i]],dtype=float)\n",
    "    return feed_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "en_train_data, zh_train_data = seq2seq_pad(en_train, encoder_length, \n",
    "                                           zh_train, decoder_length, vocab2ix)\n",
    "\n",
    "en_test_data, zh_test_data = seq2seq_pad(en_test, encoder_length, \n",
    "                                           en_test, decoder_length, vocab2ix)\n",
    "\n",
    "# For test prediction\n",
    "en_test_batch_data, zh_test_batch_data, idx = get_batch_data(en_test_data, zh_test_data, batch_size=5)\n",
    "test_feed_dict = get_feed_dict(en_test_batch_data, zh_test_batch_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_train_batch_input_target(idx):\n",
    "    test_batch_array = np.asarray(zh_train)[idx]\n",
    "    test_target_list = [x[1:][:-1] for x in test_batch_array]  # Remove <GO> and <EOS>\n",
    "    test_target_list = [' '.join(x) for x in test_target_list]\n",
    "    test_input_list = [' '.join(x) for x in np.asarray(en_train)[idx]]\n",
    "    return test_input_list, test_target_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_test_batch_input_target(idx):\n",
    "    test_batch_array = np.asarray(zh_test)[idx]\n",
    "    test_target_list = [x[1:][:-1] for x in test_batch_array]  # Remove <GO> and <EOS>\n",
    "    test_target_list = [' '.join(x) for x in test_target_list]\n",
    "    test_input_list = [' '.join(x) for x in np.asarray(en_test)[idx]]\n",
    "    return test_input_list, test_target_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_out_sequences(outputs_list):\n",
    "    test_out_array = np.asarray(outputs_list).T\n",
    "    test_out_list = [[ix2vocab[words] for words in sublist]\n",
    "                                      for sublist in test_out_array]\n",
    "    # 删除 <EOS> 之后的词\n",
    "    test_out_list = [list(itertools.takewhile(lambda x: x != '<EOS>', sublist))\n",
    "                     for sublist in test_out_list]\n",
    "    test_out_list = [' '.join(i) for i in test_out_list]\n",
    "    return test_out_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After    0 steps, cost is 5.5506\n",
      "After    5 steps, cost is 4.7260\n",
      "After   10 steps, cost is 4.7547\n",
      "After   15 steps, cost is 5.1344\n",
      "After   20 steps, cost is 3.5311\n",
      "After   25 steps, cost is 3.7506\n",
      "After   30 steps, cost is 2.5597\n",
      "After   35 steps, cost is 2.2508\n",
      "After   40 steps, cost is 3.2460\n",
      "After   45 steps, cost is 1.8039\n",
      "\n",
      "---Deocding---\n",
      "\n",
      "Input: You smell money\n",
      "Target: 你 闻到 的 是 钱 的 气味\n",
      "Output: 你 闻到 的 是 钱 的 气味\n",
      "-=--=--=--=--=--=--=--=--=--=-\n",
      "Input: And bringing those two together might seem a very daunting task\n",
      "Target: 将 两者 统一 起来 看起来 是 一件 艰巨 的 任务\n",
      "Output: 将 两者 还 不再 健康\n",
      "-=--=--=--=--=--=--=--=--=--=-\n",
      "Input: And were making the ocean pretty unhappy in a lot of different ways\n",
      "Target: 我们 正在 通过 许多 不同 的 方法 惹怒 海洋\n",
      "Output: 我们 正在 通过 浮沫\n",
      "-=--=--=--=--=--=--=--=--=--=-\n",
      "Input: and absorbed it into their skin and into their bodies\n",
      "Target: 并 把 污染 吸入 了 它们 的 皮肤 和 身体\n",
      "Output: 在 西海岸 最大 的\n",
      "-=--=--=--=--=--=--=--=--=--=-\n",
      "Input: The pyramid of ocean life\n",
      "Target: 海洋生物 的 食物链\n",
      "Output: 海洋生物 的 是\n",
      "-=--=--=--=--=--=--=--=--=--=-\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    encoder_inputs, decoder_inputs = seq2seq_pad(en_train, encoder_length, \n",
    "                                                 zh_train, decoder_length, vocab2ix)\n",
    "    for i in range(epochs):\n",
    "        en_train_batch_data, zh_train_batch_data, _ = get_batch_data(\n",
    "            en_train_data, zh_train_data, batch_size)\n",
    "        feed_dict = get_feed_dict(en_train_batch_data, zh_train_batch_data)\n",
    "        sess.run(train_step, feed_dict)\n",
    "        if i % print_loss_every == 0:\n",
    "            print('After {:4d} steps, cost is {:.4f}'.format(i, sess.run(loss, feed_dict)))\n",
    "\n",
    "    print(\"\\n---Deocding---\\n\")\n",
    "    \n",
    "    # Decoding\n",
    "    with variable_scope.variable_scope(variable_scope.get_variable_scope(), reuse=True):\n",
    "        outputs, states = embedding_rnn_seq2seq(\n",
    "            encoder_placeholders, decoder_placeholders, cell,\n",
    "            num_encoder_symbols, num_decoder_symbols,\n",
    "            embedding_size, output_projection=None,\n",
    "            feed_previous=True)\n",
    "\n",
    "        outputs_list = []\n",
    "        for o in outputs:\n",
    "            # 注意这里也需要提供 feed_dict\n",
    "            m = np.argmax(o.eval(feed_dict), axis=1)\n",
    "            outputs_list.append(m)\n",
    "        \n",
    "        outputs_ = get_out_sequences(outputs_list)\n",
    "        inputs_, targets_ = get_train_batch_input_target(_)\n",
    "        for i, o, t in zip(inputs_, outputs_, targets_):\n",
    "            print('Input:', i)\n",
    "            print('Target:', t)\n",
    "            print('Output:', o)\n",
    "            print('-=-' * 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "└ 对比 Input, Target, Output 三者，发现在训练集上验证是没问题的"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "12px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": false,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": true,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
