{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import operator\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import multiprocessing\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 导入数据并做数据清理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "直接导入之前清理好的数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_pickle('data/cleared_data.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 35124 entries, 0 to 35123\n",
      "Data columns (total 6 columns):\n",
      "review           35124 non-null object\n",
      "sentiment        35124 non-null object\n",
      "cut_words        35124 non-null object\n",
      "cleared_words    35124 non-null object\n",
      "counter          35124 non-null object\n",
      "words_count      35124 non-null int64\n",
      "dtypes: int64(1), object(5)\n",
      "memory usage: 1.6+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def other_multiprocessing(df, func, workers):\n",
    "\n",
    "    chunk_size = int(df.shape[0] / workers)\n",
    "    chunks = (df.ix[df.index[i:i + chunk_size]] for i in range(0, df.shape[0], chunk_size))\n",
    "\n",
    "    pool = multiprocessing.Pool(processes=4)\n",
    "    result = pool.map(func, chunks)\n",
    "    return result\n",
    "\n",
    "\n",
    "def sum_func(d):\n",
    "    return d.counter.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.08 s, sys: 468 ms, total: 2.55 s\n",
      "Wall time: 2min 13s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "result = other_multiprocessing(df, sum_func, workers=4)\n",
    "counter_sum = np.sum(np.asarray(result))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "高频截断，出现次数 10 次以下的词作为 unknown 词，同时用此计算 vocabulary_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "d = {i: j for i, j in counter_sum.items() if j > 10}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sorted_dict = sorted(d.items(), key=operator.itemgetter(1), reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocab_size = len(sorted_dict) + 1  # 加一个未登录词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentences = df.cleared_words.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_dict(word_counts):\n",
    "    # Build word dictionary\n",
    "    count = [['UNK', -1]]\n",
    "    count.extend(word_counts)\n",
    "    word_dict = {}\n",
    "    for word, _ in count:\n",
    "        word_dict[word] = len(word_dict)\n",
    "\n",
    "    # Build reversed dictionary\n",
    "    reversed_dict = {j: i for i, j in word_dict.items()}\n",
    "\n",
    "    return word_dict, reversed_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_dict, reversed_dict = build_dict(sorted_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def word_to_number(sentences, word_dict):\n",
    "    # Word to number\n",
    "    data = []\n",
    "    for sentence in sentences:\n",
    "        sentence_data = []\n",
    "        for word in sentence:\n",
    "            if word in word_dict:\n",
    "                index = word_dict[word]\n",
    "            else:\n",
    "                index = 0\n",
    "            sentence_data.append(index)\n",
    "        data.append(sentence_data)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = word_to_number(sentences, word_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df['word_to_number'] = np.asarray(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 设定数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = df[df.words_count > 2]  # 至少能形成一个 windows_size\n",
    "train = df[(df.sentiment == 1) | (df.sentiment == 0)]\n",
    "test  = df[~df.index.isin(train.index)].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test['sentiment'] = test.sentiment.map(lambda x: int(x[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text_data_train = train.word_to_number.values\n",
    "text_data_test = test.word_to_number.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    34051.000000\n",
       "mean        24.038648\n",
       "std         24.401450\n",
       "min          3.000000\n",
       "25%          9.000000\n",
       "50%         16.000000\n",
       "75%         31.000000\n",
       "max        894.000000\n",
       "Name: words_count, dtype: float64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.words_count.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "从上方的词长分布设定句子长度\n",
    "\n",
    "* 利用 list 相加会合并的特性\n",
    "* array + list 也有上述特性"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_words = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text_data_train = np.array([x[0:max_words] for x in [y+[0]*max_words for y in text_data_train]])\n",
    "text_data_test = np.array([x[0:max_words] for x in [y+[0]*max_words for y in text_data_test]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "target 转为 2 维表示 `(0, 1)`, 方便测试 softmax_cross_entropy_with_logits：\n",
    "```\n",
    "0 -> (1, 0); \n",
    "1 -> (0, 1)```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "target_train = train['sentiment'].map(int).values\n",
    "target_test = test['sentiment'].map(int).values\n",
    "\n",
    "target_data_train = np.zeros(shape=(target_train.shape[0], 2))\n",
    "target_data_test = np.zeros(shape=(target_test.shape[0], 2))\n",
    "\n",
    "# row 即第一维\n",
    "target_train_row_idx = np.arange(target_train.shape[0])\n",
    "target_test_row_idx = np.arange(target_test.shape[0])\n",
    "\n",
    "# target_train 的每个值其实也是 target 的方位\n",
    "target_data_train[target_train_row_idx, target_train] = 1\n",
    "target_data_test[target_test_row_idx, target_test] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 构建模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_embedding_dim = 128\n",
    "data_size = text_data_train.shape[0]\n",
    "batch_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_data = tf.placeholder(tf.int32, shape=[None, 20], name='input_data')\n",
    "labels = tf.placeholder(tf.int32, shape=[None, 2], name='labels')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_embedding = tf.Variable(\n",
    "    tf.random_uniform([vocab_size, word_embedding_dim])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_embeds = tf.nn.embedding_lookup(word_embedding, input_data)\n",
    "context_embeds = tf.reduce_mean(input_embeds, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 计算前向传播结果\n",
    "raw_output = tf.layers.dense(context_embeds, 2)\n",
    "output = tf.nn.softmax(raw_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# cost = tf.nn.softmax_cross_entropy_with_logits(logits=raw_output, labels=labels)\n",
    "# 一个正确答案的时候，其实用 tf.nn.sparse_softmax_cross_entropy_with_logits 就好了，但为了测试上面的\n",
    "#     tf.nn.softmax_cross_entropy_with_logits 函数，labels 已被转为 2 维，所以需要重新获取下编号\n",
    "cost = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=raw_output, labels=tf.arg_max(labels, 1))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 判断两个张亮的每一维度是否相等\n",
    "correct_prediction = tf.equal(tf.argmax(output, 1), tf.argmax(labels, 1))\n",
    "# 先将布尔型的数值转为实数型，然后计算平均值\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "# 设定两者的 feed_dict 方便计算 accuracy\n",
    "train_feed_dict = {\n",
    "    input_data: text_data_train, \n",
    "    labels: target_data_train,\n",
    "}\n",
    "test_feed_dict = {\n",
    "    input_data: text_data_test, \n",
    "    labels: target_data_test,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After 0 training step(s), cross entropy on batch data is 0.287002, trian accuracy is 0.53, test accuracy is 0.53\n",
      "After 500 training step(s), cross entropy on batch data is 0.048961, trian accuracy is 0.47, test accuracy is 0.47\n",
      "After 1000 training step(s), cross entropy on batch data is 0.013362, trian accuracy is 0.53, test accuracy is 0.53\n",
      "After 1500 training step(s), cross entropy on batch data is 0.185217, trian accuracy is 0.53, test accuracy is 0.53\n",
      "After 2000 training step(s), cross entropy on batch data is 0.031848, trian accuracy is 0.47, test accuracy is 0.47\n",
      "After 2500 training step(s), cross entropy on batch data is 0.015394, trian accuracy is 0.53, test accuracy is 0.53\n",
      "After 3000 training step(s), cross entropy on batch data is 0.088409, trian accuracy is 0.53, test accuracy is 0.53\n",
      "After 3500 training step(s), cross entropy on batch data is 0.023557, trian accuracy is 0.47, test accuracy is 0.47\n",
      "After 4000 training step(s), cross entropy on batch data is 0.019733, trian accuracy is 0.53, test accuracy is 0.53\n",
      "After 4500 training step(s), cross entropy on batch data is 0.046071, trian accuracy is 0.53, test accuracy is 0.53\n",
      "After 5000 training step(s), cross entropy on batch data is 0.023530, trian accuracy is 0.47, test accuracy is 0.47\n",
      "After 5500 training step(s), cross entropy on batch data is 0.388256, trian accuracy is 0.47, test accuracy is 0.47\n",
      "After 6000 training step(s), cross entropy on batch data is 0.033540, trian accuracy is 0.53, test accuracy is 0.53\n",
      "After 6500 training step(s), cross entropy on batch data is 0.014822, trian accuracy is 0.47, test accuracy is 0.47\n",
      "After 7000 training step(s), cross entropy on batch data is 0.071137, trian accuracy is 0.47, test accuracy is 0.47\n",
      "After 7500 training step(s), cross entropy on batch data is 0.022303, trian accuracy is 0.53, test accuracy is 0.53\n",
      "After 8000 training step(s), cross entropy on batch data is 1.950799, trian accuracy is 0.47, test accuracy is 0.47\n",
      "After 8500 training step(s), cross entropy on batch data is 0.041424, trian accuracy is 0.47, test accuracy is 0.47\n",
      "After 9000 training step(s), cross entropy on batch data is 0.014101, trian accuracy is 0.53, test accuracy is 0.53\n",
      "After 9500 training step(s), cross entropy on batch data is 0.117577, trian accuracy is 0.53, test accuracy is 0.53\n"
     ]
    }
   ],
   "source": [
    "train_step = tf.train.GradientDescentOptimizer(0.0001).minimize(cost)\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    STEP = 10000\n",
    "    for i in range(STEP):\n",
    "        start = (i * batch_size) % (data_size - (data_size % batch_size))\n",
    "        end = (i * batch_size) % (data_size - (data_size % batch_size)) + batch_size\n",
    "        feed_dict = {\n",
    "            input_data: text_data_train[start:end], \n",
    "            labels: target_data_train[start:end],\n",
    "        }\n",
    "        sess.run(train_step, feed_dict=feed_dict)\n",
    "        if i % 500 == 0:\n",
    "            total_cross_entropy = cost.eval(feed_dict=feed_dict)[0]\n",
    "            train_accuracy = accuracy.eval(feed_dict=train_feed_dict)\n",
    "            test_accuracy = accuracy.eval(feed_dict=test_feed_dict)\n",
    "            print(\"After %d training step(s), cross entropy on batch data is \"\n",
    "                  \"%f, trian accuracy is %.2f, test accuracy is %.2f\" % (\n",
    "                      i, total_cross_entropy, train_accuracy, test_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 试试 mini-batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After 0 training step(s), cross entropy on batch data is 0.515902, trian accuracy is 0.53, test accuracy is 0.53\n",
      "After 500 training step(s), cross entropy on batch data is 0.774015, trian accuracy is 0.57, test accuracy is 0.57\n",
      "After 1000 training step(s), cross entropy on batch data is 0.617369, trian accuracy is 0.54, test accuracy is 0.54\n",
      "After 1500 training step(s), cross entropy on batch data is 0.692724, trian accuracy is 0.56, test accuracy is 0.56\n",
      "After 2000 training step(s), cross entropy on batch data is 0.608744, trian accuracy is 0.63, test accuracy is 0.63\n",
      "After 2500 training step(s), cross entropy on batch data is 0.743858, trian accuracy is 0.63, test accuracy is 0.63\n",
      "After 3000 training step(s), cross entropy on batch data is 0.613086, trian accuracy is 0.67, test accuracy is 0.66\n",
      "After 3500 training step(s), cross entropy on batch data is 0.247247, trian accuracy is 0.68, test accuracy is 0.68\n",
      "After 4000 training step(s), cross entropy on batch data is 0.301995, trian accuracy is 0.67, test accuracy is 0.66\n",
      "After 4500 training step(s), cross entropy on batch data is 0.505117, trian accuracy is 0.66, test accuracy is 0.65\n",
      "After 5000 training step(s), cross entropy on batch data is 0.578155, trian accuracy is 0.71, test accuracy is 0.70\n",
      "After 5500 training step(s), cross entropy on batch data is 0.388136, trian accuracy is 0.71, test accuracy is 0.71\n",
      "After 6000 training step(s), cross entropy on batch data is 0.451530, trian accuracy is 0.72, test accuracy is 0.71\n",
      "After 6500 training step(s), cross entropy on batch data is 0.773971, trian accuracy is 0.72, test accuracy is 0.71\n",
      "After 7000 training step(s), cross entropy on batch data is 0.712487, trian accuracy is 0.73, test accuracy is 0.72\n",
      "After 7500 training step(s), cross entropy on batch data is 0.877815, trian accuracy is 0.72, test accuracy is 0.71\n",
      "After 8000 training step(s), cross entropy on batch data is 0.803861, trian accuracy is 0.72, test accuracy is 0.70\n",
      "After 8500 training step(s), cross entropy on batch data is 1.013564, trian accuracy is 0.72, test accuracy is 0.72\n",
      "After 9000 training step(s), cross entropy on batch data is 0.016111, trian accuracy is 0.74, test accuracy is 0.73\n",
      "After 9500 training step(s), cross entropy on batch data is 1.907670, trian accuracy is 0.74, test accuracy is 0.73\n"
     ]
    }
   ],
   "source": [
    "train_step = tf.train.GradientDescentOptimizer(0.001).minimize(cost)\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    STEP = 10000\n",
    "    # 先把 input_data 和 labels 按列拼接，方便打乱两者索引\n",
    "    data = np.concatenate((text_data_train, target_data_train), axis=1) \n",
    "    for i in range(STEP):\n",
    "        batch_data = data[np.random.randint(data.shape[0], size=50), :]\n",
    "        X = batch_data[:, :-2]\n",
    "        Y = batch_data[:, -2:]\n",
    "        feed_dict={input_data: X, labels: Y}\n",
    "        sess.run(train_step, feed_dict=feed_dict)\n",
    "        if i % 500 == 0:\n",
    "            total_cross_entropy = cost.eval(feed_dict=feed_dict)[0]\n",
    "            train_accuracy = accuracy.eval(feed_dict=train_feed_dict)\n",
    "            test_accuracy = accuracy.eval(feed_dict=test_feed_dict)\n",
    "            print(\"After %d training step(s), cross entropy on batch data is \"\n",
    "                  \"%f, trian accuracy is %.2f, test accuracy is %.2f\" % (\n",
    "                      i, total_cross_entropy, train_accuracy, test_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After 0 training step(s), cross entropy on batch data is 0.809544, trian accuracy is 0.55, test accuracy is 0.55\n",
      "After 500 training step(s), cross entropy on batch data is 0.534802, trian accuracy is 0.82, test accuracy is 0.80\n",
      "After 1000 training step(s), cross entropy on batch data is 0.568108, trian accuracy is 0.85, test accuracy is 0.82\n",
      "After 1500 training step(s), cross entropy on batch data is 0.418481, trian accuracy is 0.87, test accuracy is 0.84\n",
      "After 2000 training step(s), cross entropy on batch data is 0.045808, trian accuracy is 0.89, test accuracy is 0.85\n",
      "After 2500 training step(s), cross entropy on batch data is 0.164741, trian accuracy is 0.90, test accuracy is 0.85\n",
      "After 3000 training step(s), cross entropy on batch data is 0.384198, trian accuracy is 0.91, test accuracy is 0.86\n",
      "After 3500 training step(s), cross entropy on batch data is 0.372669, trian accuracy is 0.92, test accuracy is 0.86\n",
      "After 4000 training step(s), cross entropy on batch data is 0.745511, trian accuracy is 0.92, test accuracy is 0.86\n",
      "After 4500 training step(s), cross entropy on batch data is 0.081253, trian accuracy is 0.93, test accuracy is 0.87\n",
      "After 5000 training step(s), cross entropy on batch data is 0.004539, trian accuracy is 0.93, test accuracy is 0.87\n",
      "After 5500 training step(s), cross entropy on batch data is 0.140764, trian accuracy is 0.93, test accuracy is 0.87\n",
      "After 6000 training step(s), cross entropy on batch data is 0.000012, trian accuracy is 0.94, test accuracy is 0.87\n",
      "After 6500 training step(s), cross entropy on batch data is 1.527730, trian accuracy is 0.94, test accuracy is 0.87\n",
      "After 7000 training step(s), cross entropy on batch data is 0.347197, trian accuracy is 0.95, test accuracy is 0.88\n",
      "After 7500 training step(s), cross entropy on batch data is 0.057389, trian accuracy is 0.95, test accuracy is 0.88\n",
      "After 8000 training step(s), cross entropy on batch data is 0.068131, trian accuracy is 0.95, test accuracy is 0.88\n",
      "After 8500 training step(s), cross entropy on batch data is 0.055854, trian accuracy is 0.95, test accuracy is 0.88\n",
      "After 9000 training step(s), cross entropy on batch data is 0.207919, trian accuracy is 0.95, test accuracy is 0.88\n",
      "After 9500 training step(s), cross entropy on batch data is 0.031566, trian accuracy is 0.96, test accuracy is 0.88\n"
     ]
    }
   ],
   "source": [
    "train_step = tf.train.AdamOptimizer(0.001).minimize(cost)\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    STEP = 10000\n",
    "    # 先把 input_data 和 labels 按列拼接，方便打乱两者索引\n",
    "    data = np.concatenate((text_data_train, target_data_train), axis=1) \n",
    "    for i in range(STEP):\n",
    "        batch_data = data[np.random.randint(data.shape[0], size=50), :]\n",
    "        X = batch_data[:, :-2]\n",
    "        Y = batch_data[:, -2:]\n",
    "        feed_dict={input_data: X, labels: Y}\n",
    "        sess.run(train_step, feed_dict=feed_dict)\n",
    "        if i % 500 == 0:\n",
    "            total_cross_entropy = cost.eval(feed_dict=feed_dict)[0]\n",
    "            train_accuracy = accuracy.eval(feed_dict=train_feed_dict)\n",
    "            test_accuracy = accuracy.eval(feed_dict=test_feed_dict)\n",
    "            print(\"After %d training step(s), cross entropy on batch data is \"\n",
    "                  \"%f, trian accuracy is %.2f, test accuracy is %.2f\" % (\n",
    "                      i, total_cross_entropy, train_accuracy, test_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "同样是 反向传播 + minibatch，但不知道为啥，AdamOptimizer 相比梯度要好很多。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
