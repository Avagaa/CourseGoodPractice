{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 不同 batch_size 模型预测时间对比"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 环境说明："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scott Ming 2017-04-23 \n",
      "\n",
      "CPython 3.6.0\n",
      "IPython 6.0.0\n",
      "\n",
      "numpy 1.12.1\n",
      "pandas 0.19.2\n",
      "matplotlib 2.0.0\n",
      "tensorflow 1.0.1\n",
      "\n",
      "compiler   : GCC 4.9.2\n",
      "system     : Linux\n",
      "release    : 3.16.0-4-amd64\n",
      "machine    : x86_64\n",
      "processor  : \n",
      "CPU cores  : 4\n",
      "interpreter: 64bit\n"
     ]
    }
   ],
   "source": [
    "%load_ext watermark\n",
    "%watermark -a 'Scott Ming' -v -m -d -p numpy,pandas,matplotlib,tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from mlxtend.plotting import plot_confusion_matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from text_helpers import build_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 创建数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = np.loadtxt('data/train_data.txt', dtype=int)\n",
    "test = np.loadtxt('data/test_data.txt', dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train = train[:, :-1]\n",
    "y_train = train[:, -1:].reshape((-1,))\n",
    "x_test = test[:, :-1]\n",
    "y_test = test[:, -1:].reshape((-1,))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 构建模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess = tf.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocab_size = 80000\n",
    "sequence_length = x_train.shape[1]\n",
    "word_embed_size = 128\n",
    "data_size = train.shape[0]\n",
    "batch_size = 50\n",
    "num_filters = 3\n",
    "num_classes = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class TextCNN(object):\n",
    "    \"\"\"A CNN for text classification.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self, sequence_length, vocab_size, word_embed_size,\n",
    "            num_classes):\n",
    "\n",
    "        # Placeholders for input, output\n",
    "        self.input_x = tf.placeholder(\n",
    "            tf.int32, shape=[None, sequence_length], name='input_x')\n",
    "        self.input_y = tf.placeholder(\n",
    "            tf.int32, shape=[None, ], name='input_y')\n",
    "\n",
    "        # Embedding layer\n",
    "        with tf.name_scope('embedding'):\n",
    "            self.W = tf.get_variable('word_embedding', [vocab_size, word_embed_size],\n",
    "                                     tf.float32, tf.random_normal_initializer())\n",
    "            self.embeds = tf.nn.embedding_lookup(self.W, self.input_x)\n",
    "            self.embeds_expanded = tf.expand_dims(self.embeds, -1)\n",
    "\n",
    "        # Convolution + maxpool layer\n",
    "        with tf.name_scope('conv-maxpool'):\n",
    "            filter_num = 64\n",
    "            window_size = 3\n",
    "            filter_shape = [window_size, word_embed_size, 1, filter_num]\n",
    "            W = tf.get_variable(\"W\", filter_shape, \n",
    "                                initializer=tf.truncated_normal_initializer(stddev=0.1))\n",
    "            b = tf.get_variable(\"b\", [filter_num], initializer=tf.constant_initializer(0.0))\n",
    "            conv = tf.nn.conv2d(\n",
    "                self.embeds_expanded,\n",
    "                W,\n",
    "                strides=[1, 1, 1, 1],\n",
    "                padding='VALID',\n",
    "                name='conv')\n",
    "            conv_hidden = tf.nn.tanh(tf.add(conv, b), name='tanh')\n",
    "            pool = tf.nn.max_pool(\n",
    "                conv_hidden,\n",
    "                ksize=[1, sequence_length - window_size + 1, 1, 1],\n",
    "                strides=[1, 1, 1, 1],\n",
    "                padding='VALID',\n",
    "                name='pool')\n",
    "            pool_shape = pool.get_shape().as_list()\n",
    "            # pool_shape[0] 为一个 batch 中数据的个数，即评论条数\n",
    "            nodes = pool_shape[1] * pool_shape[2] * pool_shape[3]\n",
    "            # 通过 tf.reshape 函数把 pool 层的输出编程一个 batch 的向量\n",
    "            self.pool_flat = tf.reshape(pool, [-1, nodes])  # -1 表示尽可能的展平\n",
    "\n",
    "        # Final scores and predictions\n",
    "        with tf.name_scope('output'):\n",
    "            softmax_w = tf.get_variable('softmax_w', [nodes, num_classes], \n",
    "                                tf.float32, tf.random_normal_initializer())\n",
    "            softmax_b = tf.get_variable('softmax_b', [num_classes], tf.float32, \n",
    "                                tf.constant_initializer(0.0))\n",
    "            self.logits = tf.matmul(self.pool_flat, softmax_w) + softmax_b\n",
    "            self.y = tf.nn.softmax(self.logits)\n",
    "\n",
    "        # CalculateMean cross-entropy loss\n",
    "        with tf.name_scope('loss'):\n",
    "            losses = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "                logits=self.logits, labels=self.input_y)\n",
    "            self.loss = tf.reduce_mean(losses)\n",
    "\n",
    "        # Accuracy\n",
    "        with tf.name_scope('accuracy'):\n",
    "            # 计算预测值\n",
    "            self.pred = tf.argmax(self.y, 1)\n",
    "            # 判断两个张亮的每一维度是否相等\n",
    "            correct_prediction = tf.equal(tf.cast(self.pred, tf.int32), self.input_y)\n",
    "            # 先将布尔型的数值转为实数型，然后计算平均值\n",
    "            self.accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "cnn = TextCNN(sequence_length, vocab_size, \n",
    "              word_embed_size, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_feed_dict = {cnn.input_x: x_train, cnn.input_y: y_train,}\n",
    "test_feed_dict  = {cnn.input_x: x_test, cnn.input_y: y_test,}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After 0 training step(s), cross entropy on batch data is 10.103700, trian accuracy is 0.47, test accuracy is 0.47\n",
      "After 50 training step(s), cross entropy on batch data is 5.291380, trian accuracy is 0.49, test accuracy is 0.49\n",
      "After 100 training step(s), cross entropy on batch data is 2.060752, trian accuracy is 0.52, test accuracy is 0.51\n",
      "After 150 training step(s), cross entropy on batch data is 0.602189, trian accuracy is 0.58, test accuracy is 0.57\n",
      "After 200 training step(s), cross entropy on batch data is 0.867739, trian accuracy is 0.62, test accuracy is 0.60\n",
      "After 250 training step(s), cross entropy on batch data is 0.643813, trian accuracy is 0.66, test accuracy is 0.63\n",
      "After 300 training step(s), cross entropy on batch data is 0.710485, trian accuracy is 0.68, test accuracy is 0.64\n",
      "After 350 training step(s), cross entropy on batch data is 0.694074, trian accuracy is 0.69, test accuracy is 0.64\n",
      "After 400 training step(s), cross entropy on batch data is 0.673931, trian accuracy is 0.71, test accuracy is 0.66\n",
      "After 450 training step(s), cross entropy on batch data is 0.541885, trian accuracy is 0.73, test accuracy is 0.68\n"
     ]
    }
   ],
   "source": [
    "train_step = tf.train.AdamOptimizer(0.001).minimize(cnn.loss)\n",
    "sess.run(tf.global_variables_initializer())\n",
    "STEP = 500\n",
    "for i in range(STEP):\n",
    "    batch_data = train[np.random.randint(train.shape[0], size=batch_size), :]\n",
    "    X = batch_data[:, :-1]\n",
    "    Y = batch_data[:, -1:].reshape((-1,))\n",
    "    feed_dict = {cnn.input_x: X, cnn.input_y: Y}\n",
    "    sess.run(train_step, feed_dict=feed_dict)\n",
    "    if i % 50 == 0:\n",
    "        total_cross_entropy = sess.run(cnn.loss, feed_dict=feed_dict)\n",
    "        train_accuracy = sess.run(cnn.accuracy, feed_dict=train_feed_dict)\n",
    "        test_accuracy = sess.run(cnn.accuracy, feed_dict=test_feed_dict)\n",
    "        test_prediction = sess.run(cnn.pred, feed_dict=test_feed_dict)\n",
    "        print(\"After %d training step(s), cross entropy on batch data is \"\n",
    "              \"%f, trian accuracy is %.2f, test accuracy is %.2f\" % (\n",
    "                  i, total_cross_entropy, train_accuracy, test_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. 复用模型并查看预测时间"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "为了增强测试时间函数的可复用性，写成装饰器的形式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from functools import wraps\n",
    "\n",
    "def timethis(func):\n",
    "    '''\n",
    "    Decorator that reports the execution time.\n",
    "    '''\n",
    "    @wraps(func)\n",
    "    def wrapper(*args, **kwargs):\n",
    "        start = time.time()\n",
    "        result = func(*args, **kwargs)\n",
    "        end = time.time()\n",
    "        print('{:.5f}'.format(end-start))\n",
    "        return result\n",
    "    return wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "@timethis\n",
    "def test_prediction(feed_dict):\n",
    "    sess.run(cnn.pred, feed_dict=feed_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_batch_data(data, batch_size):\n",
    "    batch_data = data[np.random.randint(data.shape[0], size=batch_size), :]\n",
    "    X = batch_data[:, :-1]\n",
    "    Y = batch_data[:, -1:].reshape((-1,))\n",
    "    feed_dict = {cnn.input_x: X, cnn.input_y: Y}\n",
    "    return feed_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feed_dict_batch1 = get_batch_data(test, 1)\n",
    "feed_dict_batch128 = get_batch_data(test, 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When batch size is   1, it takes:\n",
      "0.00081\n",
      "---------------\n",
      "When batch size is 128, it takes:\n",
      "0.00518\n"
     ]
    }
   ],
   "source": [
    "with tf.variable_scope(tf.get_variable_scope(), reuse=True):\n",
    "    print('When batch size is {:3d}, it takes:'.format(1))\n",
    "    test_prediction(feed_dict_batch1)\n",
    "    print('---' * 5)\n",
    "    print('When batch size is {:3d}, it takes:'.format(128))\n",
    "    test_prediction(feed_dict_batch128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "从上面的时间测算可以看出，后者消耗差不多是前者的 6 倍，但感觉还是区别不大啊"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "138px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": false,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
