{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 运行环境"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-04-09T09:06:42+08:00\n",
      "\n",
      "CPython 3.5.2\n",
      "IPython 5.3.0\n",
      "\n",
      "compiler   : GCC 4.9.2\n",
      "system     : Linux\n",
      "release    : 3.16.0-4-amd64\n",
      "machine    : x86_64\n",
      "processor  : \n",
      "CPU cores  : 4\n",
      "interpreter: 64bit\n"
     ]
    }
   ],
   "source": [
    "%load_ext watermark\n",
    "%watermark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0.1\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 构建一个词的 Word Embedding\n",
    "在构建统计语言模型的时候，我们把所有看到的词都放入了 dict 里面，这时候影响不大，因为每个词占用的内存较小。\n",
    "\n",
    "在使用神经网络做自然语言处理的时候，我们一般都会对词表做一个截断操作，取最高频的 n 个（也有人按词频阈值做截断）。这样有两个好处：\n",
    "1. 减少模型的内存使用。\n",
    "2. 只出现过一两次的词，在整个优化过程中往往也很难学好。不如把这些词直接全看成未登录词。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 决定了 embedding 的维度 （隐层节点数）\n",
    "word_embedding_dim = 128\n",
    "# 决定了词表数量, 预留一个未登录词\n",
    "vocab_size = 80000 + 1\n",
    "UNK_IDX = 0\n",
    "\n",
    "# 这里需要把 Word embedding 放到 Variable 里面。因为 Word embedding 是要随机初始化，跟着数据不断变化的。\n",
    "# 它相当于普通神经网络中的权重。\n",
    "\n",
    "# 在梯度下降时， tensorflow 的 Optimizer 会自动找到 Graph 中的 Variable，计算梯度并进行更新。\n",
    "word_embedding = tf.Variable(tf.random_uniform([vocab_size, word_embedding_dim]))\n",
    "\n",
    "# placeholder 和 variable 基本都可以当做 Tensor 来用\n",
    "# 注意这里的输入是 int32 类型，表示一个词 ID。这里我们需要对数据进行预处理，以把高频词映射到 [1, 80000] 之间，不在词表里面的词设置成 UNK, ID 为 0\n",
    "# 这里我们假设输入是两个词\n",
    "\n",
    "# 这里 Shape 的第一维我们指定为 None，是表示第一维可以根据数据进行变化，因此同样一个程序可以适应梯度下降时不同的 batch_size\n",
    "input_data = tf.placeholder(tf.int32, shape=[None, 2], name='input_data')\n",
    "\n",
    "input_embeds = tf.nn.embedding_lookup(word_embedding, input_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "因为 input_data 是一个二维矩阵，lookup 之后得到的其实是一个『三维的矩阵』\n",
    "\n",
    "怎么理解呢？如果我们一个样本有两个词，拿到的就是矩阵的两行，因此是一个矩阵\n",
    "\n",
    "但是我们同时有多个样本，因此需要对这个矩阵再『扩张』一个维度。而这个维度因为数据还未给出，大小是未知的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'embedding_lookup:0' shape=(?, 2, 128) dtype=float32>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_embeds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 两个词的向量做相加"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# reduce 开头的函数一般有一个 axis 参数，决定按行、按列或者按整个矩阵进行 reduce\n",
    "context_embeds = tf.reduce_sum(input_embeds, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Sum:0' shape=(?, 128) dtype=float32>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 注意观察 context_embds 的 shape\n",
    "# 因为 placeholder 第一位的维度是 None，这里 TF 没法确切知道第一维最后的 shape\n",
    "context_embeds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 相加的词向量再映射到 N 个词的概率分布"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 激活之前的输出\n",
    "raw_output = tf.layers.dense(context_embeds, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 加 softmax 之后的输出\n",
    "output = tf.nn.softmax(raw_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Softmax_1:0' shape=(?, 2) dtype=float32>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.99105352  0.0089465 ]]\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    # 因为引入了 variable，所以需要进行初始化\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    # 输出的矩阵比较大，我们只看前 10 列\n",
    "    print(sess.run(output, feed_dict={input_data: np.asarray([[1, 2]])})[:, :10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 同第三讲示例代码中的类似 (sigmoid + cross entropy)，softmax 配合 cross entropy 的时候，在求导时两个连着看，也可以做分母的消除，因此在计算 cost 的时候我们要把 raw_output 喂给 tf 的这个损失函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 样本的 labels 也需要用 placeholder 放置\n",
    "labels = tf.placeholder(tf.int32, shape=[None, 2], name='labels')\n",
    "\n",
    "# 因为我们每个样本的 label 只有一个，使用稠密的 softmax 算 cost 及求导太浪费了。这里使用 sparse 版本即可。\n",
    "# 如果你的 label 是完整的 N 个词上的概率分布，这时候可以使用 tf.nn.softmax_cross_entropy_with_logits\n",
    "# cost = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=raw_output, labels=labels)\n",
    "cost = tf.nn.softmax_cross_entropy_with_logits(logits=raw_output, labels=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'dense_2/BiasAdd:0' shape=(?, 2) dtype=float32>"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'labels_5:0' shape=(?, 2) dtype=int32>"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0\n",
      "Cost: 0.293557\n",
      "Probability: 0.745607\n",
      "Probability: 0.291226\n",
      "------\n",
      "Iteration 20\n",
      "Cost: 0.083625\n",
      "Probability: 0.919776\n",
      "Probability: 0.086740\n",
      "------\n",
      "Iteration 40\n",
      "Cost: 0.046348\n",
      "Probability: 0.954709\n",
      "Probability: 0.048951\n",
      "------\n",
      "Iteration 60\n",
      "Cost: 0.031678\n",
      "Probability: 0.968818\n",
      "Probability: 0.033695\n",
      "------\n",
      "Iteration 80\n",
      "Cost: 0.023928\n",
      "Probability: 0.976356\n",
      "Probability: 0.025547\n",
      "------\n"
     ]
    }
   ],
   "source": [
    "train_step = tf.train.GradientDescentOptimizer(0.01).minimize(cost)\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    dummy_feed_dict = {input_data: np.asarray([[1, 2], [4, 3]]),\n",
    "                       labels: np.asarray([[1, 0], [0, 1]])}\n",
    "    for i in range(100):\n",
    "        sess.run(train_step, feed_dict=dummy_feed_dict)\n",
    "        if i % 20 == 0:\n",
    "            print(\"Iteration %d\" % i)\n",
    "            print(\"Cost: %f\" % cost.eval(feed_dict=dummy_feed_dict)[0])\n",
    "            # 查看输出中 ID == 3 的概率\n",
    "            print(\"Probability: %f\" % output.eval(feed_dict=dummy_feed_dict)[0, 0])\n",
    "            print(\"Probability: %f\" % output.eval(feed_dict=dummy_feed_dict)[1, 0])\n",
    "            output_tst = output.eval(feed_dict=dummy_feed_dict)\n",
    "#             print(\"The ouput shape is {}\".format(output.eval(feed_dict=dummy_feed_dict).shape))\n",
    "            print(\"------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.97635603,  0.02364396],\n",
       "       [ 0.02554693,  0.97445315]], dtype=float32)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_tst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.02180748,  0.97819251], dtype=float32)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_a.argmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.94072884"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_a[5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 作业"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "请根据这个基本的框架程序进行扩展，使用你的语料库进行训练。并完成 3 个名词各自最相近的 Top 10 个词的检索"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 2]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.asarray([[1, 2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.arange(16).reshape((1, 16))[0, 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3],\n",
       "       [4]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.asarray([[3], [4]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
