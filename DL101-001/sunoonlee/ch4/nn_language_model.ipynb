{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## word-embedding\n",
    "\n",
    "在构建 count-based 语言模型的时候，我们把所有看到的词都放入了 dict 里面，这时候影响不大，因为每个词占用的内存较小。\n",
    "\n",
    "在使用神经网络做自然语言处理的时候，我们一般都会对词表做一个截断操作，取最高频的 n 个（也有人按词频阈值做截断）。这样有两个好处：\n",
    "1. 减少模型的内存使用。\n",
    "2. 只出现过一两次的词，在整个优化过程中往往也很难学好。不如把这些词直接全看成未登录词。\n",
    "\n",
    "### 运行环境\n",
    "\n",
    "docker image `tensorflow/tensorflow`, Python 3.4.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0.1\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "from collections import Counter\n",
    "import time\n",
    "import jieba\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 读取语料，生成训练数据\n",
    "\n",
    "简便起见，忽略首尾词，否则需要设置文首文末的 token。  \n",
    "context words 取一左一右。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Loading model cost 0.757 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('天色', 3), ('恐怖', 3), ('活该', 3), ('手续', 3), ('光阴', 3)]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "read_size = 100000\n",
    "vocab_size = 2000 + 1  # 预留一个未登录词\n",
    "\n",
    "with open('../ch1/ZhangAiLing.txt', encoding='utf-8') as f:\n",
    "    corpus = f.read(read_size)\n",
    "\n",
    "words = [word for word in jieba.cut(corpus) if word not in ' \\n']\n",
    "word_cnt = Counter(words)\n",
    "vocab = [i[0] for i in word_cnt.most_common(vocab_size - 1)]  # 词表(高频截断)\n",
    "vocab.insert(0, 'UNK')\n",
    "\n",
    "# 将语料序列映射到 [0, vocab_size - 1] 内的整数, 未登录词为 0\n",
    "word_ids = [vocab.index(word) if (word in vocab) else 0 \n",
    "            for word in words]\n",
    "\n",
    "# 生成训练数据\n",
    "inputs_train = np.asarray([[word_ids[i-1], word_ids[i+1]] \n",
    "                           for i in range(1, len(word_ids) - 1)])\n",
    "labels_train = np.asarray(word_ids[1:-1])\n",
    "\n",
    "# 查看截断位置的词频\n",
    "word_cnt.most_common(vocab_size)[-5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100000, 67488, 2001, (67486, 2), (67486,))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(corpus), len(words), len(vocab), inputs_train.shape, labels_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 定义神经网络\n",
    "\n",
    "#### 定义 Variable 和 placeholder\n",
    "\n",
    "Word embeddings 定义为 Variable，需要做随机初始化。它相当于普通神经网络中的权重。\n",
    "\n",
    "inputs 是 int32 类型，每行为一条样本中输入词的 ID。这里我们需要对数据进行预处理，把高频词映射到 [1, vocab_size - 1] 之间，不在词表里面的词设置成 UNK, ID 为 0。\n",
    "\n",
    "inputs 的 Shape，第一维我们指定为 None，可以根据数据而变化，因此同样一个程序可以适应梯度下降时不同的 batch_size。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "word_embedding_dim = 50\n",
    "word_embeddings = tf.Variable(tf.random_uniform([vocab_size, word_embedding_dim]))\n",
    "inputs = tf.placeholder(tf.int32, shape=[None, 2], name='inputs')\n",
    "labels = tf.placeholder(tf.int32, shape=[None], name='labels')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### lookup 得到 input 词的 embedding\n",
    "\n",
    "对矩阵 inputs 作 lookup 之后，得到的是一个 rank-3 的张量。其第1个维度大小未知，取决于样本数量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'embedding_lookup:0' shape=(?, 2, 50) dtype=float32>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_embeds = tf.nn.embedding_lookup(word_embeddings, inputs)\n",
    "input_embeds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### 词向量相加，并映射到输出层（N 个词的概率分布）\n",
    "\n",
    "接下来对两个 context 词的 word embedding 求和。\n",
    "\n",
    "reduce 开头的函数一般有一个 axis 参数，决定按行、按列或者按整个矩阵进行 reduce\n",
    "\n",
    "相加得到的 context_embds 的第一维仍然是未知的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Sum:0' shape=(?, 50) dtype=float32>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context_embeds = tf.reduce_sum(input_embeds, axis=1)\n",
    "context_embeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorShape([Dimension(None), Dimension(2001)]), (67486,))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_output = tf.layers.dense(context_embeds, vocab_size)\n",
    "output = tf.nn.softmax(raw_output)\n",
    "raw_output.shape, labels_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### cost function\n",
    "\n",
    "与 sigmoid 类似, softmax 配合 cross entropy 的时候，在求导时两个连着看，也可以做分母的消除，因此在计算 cost 的时候我们要把 raw_output 喂给 tf 的这个损失函数.\n",
    "\n",
    "因为我们每个样本的 label 只有一个，使用稠密的 softmax 算 cost 及求导太浪费了。这里使用 sparse 版本即可。如果你的 label 是完整的 N 个词上的概率分布，这时候可以使用 tf.nn.softmax_cross_entropy_with_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "cost = tf.reduce_mean(\n",
    "    tf.nn.sparse_softmax_cross_entropy_with_logits(logits=raw_output, labels=labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 训练\n",
    "\n",
    "最初用全量梯度下降, 后改用随机梯度下降(v2), 速度明显加快.  \n",
    "再由随机梯度下降改为小批量梯度下降(v4)\n",
    "\n",
    "不过不大理解的是, 为什么 cost 在训练后期反而上升?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorShape([Dimension(None)]),\n",
       " (67486,),\n",
       " TensorShape([Dimension(None), Dimension(2)]),\n",
       " (67486, 2))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels.shape, labels_train.shape, inputs.shape, inputs_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "learning_rate = 3\n",
    "epochs = 1000\n",
    "print_cost_every = 10\n",
    "batch_size = 100\n",
    "feed = {inputs: inputs_train, labels: labels_train}  # full-batch feed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Round 000 - Cost: 7.63822\n",
      "Round 010 - Cost: 3.32822\n",
      "Round 020 - Cost: 3.07603\n",
      "Round 030 - Cost: 3.02027\n",
      "Round 040 - Cost: 3.02089\n",
      "Round 050 - Cost: 3.03762\n",
      "Round 060 - Cost: 3.05802\n",
      "Round 070 - Cost: 3.07785\n",
      "Round 080 - Cost: 3.09582\n",
      "KeyboardInterrupt\n",
      "\n",
      "time: 452.78 s\n"
     ]
    }
   ],
   "source": [
    "# writer = tf.summary.FileWriter('/tensorflow/tf')\n",
    "# cost_summary = tf.summary.scalar('cost_', cost)\n",
    "# embedding_summary = tf.summary.histogram('embeddings_', word_embeddings)\n",
    "# merged = tf.summary.merge_all()\n",
    "\n",
    "train_step = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost)\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "costs = []\n",
    "\n",
    "# 训练集的随机序号\n",
    "num_inputs = len(labels_train)\n",
    "order = np.arange(num_inputs)\n",
    "np.random.shuffle(order)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "try:\n",
    "    for i in range(epochs):\n",
    "        if i % print_cost_every == 0:\n",
    "            cost_value, merged_value = sess.run([cost, merged], feed_dict=feed)\n",
    "            print(\"Epoch {:03d} - Cost: {:.5f}\".format(i, cost_value))\n",
    "            costs.append(cost_value)\n",
    "            writer.add_summary(merged_value, i)\n",
    "        for j in range(0, num_inputs, batch_size):\n",
    "            batch_index = order[j: j + batch_size]\n",
    "            batch_inputs = inputs_train[batch_index]\n",
    "            batch_labels = labels_train[batch_index]\n",
    "            batch_feed = {inputs: batch_inputs, labels: batch_labels}\n",
    "            sess.run(train_step, feed_dict=batch_feed)\n",
    "except KeyboardInterrupt:\n",
    "    print('KeyboardInterrupt')\n",
    "finally:\n",
    "    end_time = time.time()\n",
    "    print('\\ntime: {:.2f} s'.format(end_time - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAW4AAAD8CAYAAABXe05zAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f74cc57ea58>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(costs);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 比较词向量相似性\n",
    "\n",
    "完成 3 个名词各自最相近的 Top 10 个词的检索\n",
    "\n",
    "原先用 for 循环的方式, 后按童老师建议修改为矩阵乘法方式, 参考 [tensorflow_cookbook/05_Working_With_CBOW.ipynb](https://github.com/nfmcclure/tensorflow_cookbook/blob/master/07_Natural_Language_Processing/05_Working_With_CBOW_Embeddings/05_Working_With_CBOW.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "validate_words = ['叔惠', '家里', '茶杯', '手套']\n",
    "validate_ids = [vocab.index(word) for word in validate_words]\n",
    "validate_inputs = tf.constant(validate_ids, dtype=tf.int32)\n",
    "\n",
    "norm = tf.sqrt(tf.reduce_sum(tf.square(word_embeddings), 1, keep_dims=True))\n",
    "normalized_embeddings = word_embeddings / norm\n",
    "validate_embeddings = tf.nn.embedding_lookup(normalized_embeddings, validate_inputs)\n",
    "similarity = tf.matmul(validate_embeddings, normalized_embeddings, transpose_b=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "similar to 叔惠:\n",
      "['世钧', '慕瑾', '他', '曼桢', '顾太太', '她', '我', '一鹏', '嘴里', '翠芝']\n",
      "similar to 家里:\n",
      "['一幢', '身上', '妹妹', '对于', '这儿', '一块儿', '将来', '和世钧', '乡下', '房里']\n",
      "similar to 茶杯:\n",
      "['鞋子', '公馆', '听筒', '鞋', '无线电', '使劲', '抽屉', '裤袋', '这间', '里头']\n",
      "similar to 手套:\n",
      "['白', '钟', '茶叶', '报纸', '头发', '镜子', '背', '枕头', '包车', '桂花']\n"
     ]
    }
   ],
   "source": [
    "sim_values = sess.run(similarity, feed_dict=feed)\n",
    "for i in range(len(validate_words)):\n",
    "    word = validate_words[i]\n",
    "    similar_ids = (-sim_values[i, :]).argsort()[1: 11]\n",
    "    similar_words = [vocab[j] for j in similar_ids]\n",
    "    print('similar to {}:'.format(word))\n",
    "    print(similar_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### ch6 task1: 比较不同 batch_size 的预测运算时间"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def batch_predict_time(size):\n",
    "    batch_index = np.random.choice(num_inputs, size, replace=False)\n",
    "    batch_inputs = inputs_train[batch_index]        \n",
    "    predictions = tf.argmax(output, 1)\n",
    "    start = time.time()\n",
    "    predictions_val = sess.run(predictions, feed_dict={inputs: batch_inputs})\n",
    "    end = time.time()\n",
    "    return end - start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 loops, best of 3: 38.9 ms per loop\n"
     ]
    }
   ],
   "source": [
    "%timeit batch_predict_time(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 loops, best of 3: 41.5 ms per loop\n"
     ]
    }
   ],
   "source": [
    "%timeit batch_predict_time(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 loops, best of 3: 64.2 ms per loop\n"
     ]
    }
   ],
   "source": [
    "%timeit batch_predict_time(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.05188274383544922, 0.05747032165527344, 0.06839132308959961)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_predict_time(1), batch_predict_time(100), batch_predict_time(1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "结果: \n",
    "* 预测的 batch_size 从 1 增大到 100, 耗费的时间竟然没怎么增加!\n",
    "* 从 1 到 1000, 耗费的时间也没有超过两倍."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### ChangeLog\n",
    "\n",
    "\n",
    "`v4a`\n",
    "* 学习率由 0.1 改为 3, 训练时间从 1h+ 减少至 不到10分钟...\n",
    "* 教训: 训练前, **确保学习率在能收敛的前提下取较大的值!** 特别是在模型变更之后, 学习率要重新选取\n",
    "\n",
    "`v4`\n",
    "* 改用 mini-batch 梯度下降. (batch size 100, 学习率 0.1)\n",
    "* 奇怪, 训练速度并没有加快. 用时 5000+ s.\n",
    "  * 学习率取得太小了!!\n",
    "\n",
    "`v3`\n",
    "* 样本加大到 10w 词(有效词 6w+), 词向量维度 30->50, vocab_size 400->2000\n",
    "* 结果的相似性明显提高\n",
    "* 按 ch5 task2 增加 summary 输出\n",
    "\n",
    "`v2`\n",
    "* 采用 SGD, 速度明显加快\n",
    "* 用向量化方法计算相似度, 取代低效的 for 循环\n",
    "\n",
    "`v0~v1`  \n",
    "* 结果大体上有一点 sense。人名对人名，物件对物件。  \n",
    "* 考虑到这些前提，结果大致可以接受：\n",
    "    * 样本不到 10000 词，算非常小了\n",
    "    * context 只取了一左一右两个词，并不能很准确地捕捉上下文\n",
    "    * 并未做什么优化"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
