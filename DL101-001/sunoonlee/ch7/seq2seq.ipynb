{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 概述\n",
    "神经网络翻译模型. 利用 `tf.contrib.legacy_seq2seq.model_with_buckets` 实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPython 3.4.3\n",
      "IPython 5.3.0\n",
      "\n",
      "tensorflow 1.0.1\n",
      "numpy 1.12.0\n",
      "\n",
      "compiler   : GCC 4.8.4\n",
      "system     : Linux\n",
      "release    : 4.9.13-moby\n",
      "machine    : x86_64\n",
      "processor  : x86_64\n",
      "CPU cores  : 4\n",
      "interpreter: 64bit\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "import re\n",
    "import time\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.ops import variable_scope\n",
    "import jieba\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "%load_ext watermark\n",
    "%watermark -p tensorflow,numpy -v -m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# 模型相关参数\n",
    "cell_size = 100\n",
    "embedding_size = 100\n",
    "buckets = [(8, 8), (12, 12), (20, 20)]\n",
    "# num_sampled = 300\n",
    "# TODO: 怎样更合理地设置 buckets?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 读入语料并处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "basedir = '^data/'\n",
    "files = {'train': {'en': basedir + 'TED_en_train.txt',\n",
    "                   'zh': basedir + 'TED_zh_train.txt'},\n",
    "         'test': {'en': basedir + 'TED_en_test.txt',\n",
    "                  'zh': basedir + 'TED_zh_test.txt'}}\n",
    "lines_to_read = 100\n",
    "VOCAB_MIN_FREQ = 1  # 计入词表的最小词频, 小于此值则记为 UNK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 读取文本文件, tokenize 得到 sentences (word lists)\n",
    "\n",
    "decoder inputs 在句子首尾分别添加特殊 token\n",
    "* 句首的 <GO> Token 是为了喂给 decoder 的第一个 cell, 以便生成第一个词.  \n",
    "* 句末的 <EOS> Token 相当于人为给模型提供一个句子结束的信号, 让模型自己学会何时结束句子."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "_PAD, _GO, _EOS, _UNK = '<PAD>', '<GO>', '<EOS>', '<UNK>' \n",
    "_PAD_ID, _GO_ID, _EOS_ID, _UNK_ID = 0, 1, 2, 3\n",
    "_START_VOCAB = (_PAD, _GO, _EOS, _UNK)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Loading model cost 0.777 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    }
   ],
   "source": [
    "def en_tokenizer(sentence):\n",
    "    \"\"\"英文句子分词\n",
    "    source: https://github.com/tensorflow/models/blob/master/tutorials/rnn/translate/data_utils.py\n",
    "    \"\"\"\n",
    "    _word_split = re.compile(\"([.,!?\\\"':;)(])\")\n",
    "    words = []\n",
    "    for space_separated_fragment in sentence.strip().split():\n",
    "        words.extend(_word_split.split(space_separated_fragment))\n",
    "    return [w for w in words if w]\n",
    "\n",
    "def zh_tokenizer(sentence):\n",
    "    '''中文句子分词'''\n",
    "    return [w for w in jieba.cut(sentence.strip()) if w not in (' ', '\\t', '\\n')]\n",
    "    # return [char for char in sentence.strip() if char not in (' ', '\\t', '\\n')]\n",
    "\n",
    "\n",
    "def read_sentences(encoder_file, decoder_file, lines_to_read=None):\n",
    "    \"\"\"默认从英文到中文\"\"\"\n",
    "    \n",
    "    # 读取文件. lines_to_read 参数可用于小规模试验.\n",
    "    with open(encoder_file, encoding='utf-8') as f:\n",
    "        enc_lines = f.readlines()[:lines_to_read] if lines_to_read else f.readlines()\n",
    "    with open(decoder_file, encoding='utf-8') as f:\n",
    "        dec_lines = f.readlines()[:lines_to_read] if lines_to_read else f.readlines()\n",
    "    assert len(enc_lines) == len(dec_lines)\n",
    "\n",
    "    num_lines = len(enc_lines)\n",
    "    \n",
    "    encoder_sentences = []\n",
    "    decoder_sentences = []\n",
    "    \n",
    "    for i in range(num_lines):\n",
    "        enc_sentence = en_tokenizer(enc_lines[i])\n",
    "        dec_sentence = zh_tokenizer(dec_lines[i])\n",
    "        if enc_sentence and dec_sentence:\n",
    "            encoder_sentences.append(enc_sentence)\n",
    "            decoder_sentences.append(dec_sentence)\n",
    "    return encoder_sentences, decoder_sentences\n",
    "\n",
    "\n",
    "encoder_sentences, decoder_sentences = read_sentences(\n",
    "    files['train']['en'], files['train']['zh'], lines_to_read=lines_to_read)\n",
    "encoder_sentences_test, decoder_sentences_test = read_sentences(\n",
    "    files['train']['en'], files['train']['zh'], lines_to_read=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD8CAYAAABn919SAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f0e3f9b5eb8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def plot_length_histogram(sentences):\n",
    "    lengths = np.asarray([len(s) for s in sentences])\n",
    "    plt.hist(lengths, bins=range(30));\n",
    "\n",
    "plot_length_histogram(encoder_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD8CAYAAABn919SAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\niVIAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f0e38eb5b38>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_length_histogram(decoder_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# def print_(lst):\n",
    "#     for i in lst:\n",
    "#         print(i)\n",
    "\n",
    "# print_(encoder_sentences)\n",
    "# print_(decoder_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 由训练文件的 word lists 得到 vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "307 353\n"
     ]
    }
   ],
   "source": [
    "def build_vocab(sentences):\n",
    "    \"\"\"生成词表. 前几个位置留给 _START_VOCAB 的特殊 token \"\"\"\n",
    "    vocab = list(_START_VOCAB)\n",
    "    words_flat = [w for s in sentences for w in s]\n",
    "    word_cnt = Counter(words_flat)\n",
    "    for word, count in word_cnt.most_common():\n",
    "        if count >= VOCAB_MIN_FREQ:\n",
    "            vocab.append(word)\n",
    "    return vocab\n",
    "\n",
    "# vocab_xxx 相当于 demo 代码里的 reversed_vocab\n",
    "vocab_enc = build_vocab(encoder_sentences)  # list: id -> word\n",
    "vocab_dec = build_vocab(decoder_sentences)\n",
    "num_encoder_symbols = len(vocab_enc)\n",
    "num_decoder_symbols = len(vocab_dec)\n",
    "\n",
    "# wod2id_xxx 相当于 demo 代码里的 vocab\n",
    "word2id_enc = {w: i for i, w in enumerate(vocab_enc)}  # dict: word -> id\n",
    "word2id_dec = {w: i for i, w in enumerate(vocab_dec)}\n",
    "\n",
    "print(num_encoder_symbols, num_decoder_symbols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### decoder inputs 句子首尾加 token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def add_tokens(sentences):\n",
    "    \"\"\"为 decoder 的输入语句增加首尾 token\"\"\"\n",
    "    for i in range(len(sentences)):\n",
    "        sentences[i] = [_GO] + sentences[i] + [_EOS]\n",
    "    return sentences\n",
    "\n",
    "decoder_sentences = add_tokens(decoder_sentences)\n",
    "decoder_sentences_test = add_tokens(decoder_sentences_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# print_(encoder_sentences)\n",
    "# print_(decoder_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# print_(encoder_sentences_test)\n",
    "# print_(decoder_sentences_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### tokens -> ids -> bucketing -> padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([29, 47, 23], [208, 218, 70])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def bucket_and_pad(enc_sentences, dec_sentences, buckets, word2id_enc, word2id_dec):\n",
    "    \"\"\"\n",
    "    - enc_sentences: A nested list of symbol str for encoding, length: batch_size\n",
    "    - dec_sentences: A nested list of symbol str for decoding, length: batch_size\n",
    "    - word2id_enc, word2id_dec: dict. symbol (str) -> index (int)\n",
    "    \n",
    "    Example: \n",
    "    [\"hello\", \"world\"] -> [\"hi\", \"<EOS>\"]\n",
    "    [\"cover\", \"me\"] -> [\"roger\", \"<EOS>\"]\n",
    "        \n",
    "    Assume that index of \"<PAD>\" is 0\n",
    "\n",
    "    Output:\n",
    "    [[0, 0, <index of 'hello'>, <index of 'world'>], [0, 0, <index of 'cover'>, <index of 'me'>]],\n",
    "    [[<index of 'hi'>, <index of 'EOS'>, 0, 0], [<index of 'roger'>, <index of 'EOS'>, 0, 0]]\n",
    "    \"\"\"\n",
    "    def to_index(sentence, length, word2id, pad_from_start=True):\n",
    "        ids = [_PAD_ID] * length\n",
    "        l = len(sentence)\n",
    "        if l < length:\n",
    "            if pad_from_start:\n",
    "                ids[(length - l):] = [word2id.get(w, _UNK_ID) for w in sentence]\n",
    "            else:\n",
    "                ids[:l] = [word2id.get(w, _UNK_ID) for w in sentence]\n",
    "        else:\n",
    "            ids = [word2id.get(w, _UNK_ID) for w in sentence[:length]]\n",
    "        return ids\n",
    "    \n",
    "    num_sentences = len(enc_sentences)\n",
    "    \n",
    "    encoder_data = [[] for _ in range(len(buckets))]\n",
    "    decoder_data = [[] for _ in range(len(buckets))]\n",
    "    \n",
    "    # bucketing. 此时 decoder_sentences 已加首尾 token.\n",
    "    for i in range(num_sentences):\n",
    "        for bucket_id, (encoder_size, decoder_size) in enumerate(buckets):\n",
    "            if len(enc_sentences[i]) <= encoder_size and len(dec_sentences[i]) <= decoder_size:\n",
    "                encoder_data[bucket_id].append(\n",
    "                    to_index(enc_sentences[i], encoder_size, word2id_enc, True))\n",
    "                decoder_data[bucket_id].append(\n",
    "                    to_index(dec_sentences[i], decoder_size, word2id_dec, False))\n",
    "                break\n",
    "    \n",
    "    return encoder_data, decoder_data\n",
    "\n",
    "\n",
    "encoder_data, decoder_data = bucket_and_pad(\n",
    "    encoder_sentences, decoder_sentences, buckets, word2id_enc, word2id_dec)\n",
    "encoder_data_test, decoder_data_test = bucket_and_pad(\n",
    "    encoder_sentences_test, decoder_sentences_test, buckets, word2id_enc, word2id_dec)\n",
    "\n",
    "data_sizes = [len(encoder_data[i]) for i in range(len(buckets))]\n",
    "data_sizes_dec = [len(decoder_data[i]) for i in range(len(buckets))]\n",
    "assert data_sizes == data_sizes_dec\n",
    "\n",
    "data_sizes_test = [len(encoder_data_test[i]) for i in range(len(buckets))]\n",
    "\n",
    "data_sizes, data_sizes_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# encoder_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# decoder_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 构建 seq2seq 模型\n",
    "\n",
    "\n",
    "训练和生成(decoding) 这两个阶段需要共享变量\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "cell = tf.contrib.rnn.GRUCell(cell_size)\n",
    "# 也可以试试 多层 rnn cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 按最大 bucket 的长度创建 placeholder 列表\n",
    "\n",
    "注意在 bucketing 时, decoder inputs 已经做了首尾的 token: <GO> 和 <EOS>.\n",
    "\n",
    "decoder_placeholder 长度取为 max_encoder_length. 即, 认为 decoder_inputs 包含首尾的 token?\n",
    "*(或者, decoder_inputs 不应包含 EOS token?)*\n",
    "\n",
    "target_placeholder 应该不包含 GO token, 所以长度应该减1 ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "max_encoder_length, max_decoder_length = buckets[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "encoder_placeholders = [\n",
    "    tf.placeholder(tf.int32, shape=[None], name=\"encoder_%d\" % i)\n",
    "    for i in range(max_encoder_length)]\n",
    "decoder_placeholders = [\n",
    "    tf.placeholder(tf.int32, shape=[None], name=\"decoder_%d\" % i)\n",
    "    for i in range(max_decoder_length)]\n",
    "target_placeholders = [\n",
    "    tf.placeholder(tf.int32, shape=[None], name=\"target_%d\" % i)\n",
    "    for i in range(max_decoder_length)]\n",
    "target_weights_placeholders = [\n",
    "    tf.placeholder(tf.float32, shape=[None], name=\"decoder_weight_%d\" % i)\n",
    "    for i in range(max_decoder_length)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 利用 `model_with_buckets` 接口构建模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def seq2seq_f(encoder_placeholders, decoder_placeholders, do_decode):\n",
    "    # 可以在这里设置不同的 seq2seq 接口, 比如 embedding_attention_seq2seq\n",
    "    return tf.contrib.legacy_seq2seq.embedding_rnn_seq2seq(\n",
    "        encoder_placeholders,\n",
    "        decoder_placeholders,\n",
    "        cell,\n",
    "        num_encoder_symbols=num_encoder_symbols,\n",
    "        num_decoder_symbols=num_decoder_symbols,\n",
    "        embedding_size=embedding_size,\n",
    "        output_projection=None,\n",
    "        feed_previous=do_decode)\n",
    "\n",
    "# def sampled_loss(labels, logits):\n",
    "#     \"\"\"参考: https://github.com/tensorflow/models/blob/master/tutorials/rnn/translate/data_utils.py\"\"\"\n",
    "#     labels = tf.reshape(labels, [-1, 1])\n",
    "#     # We need to compute the sampled_softmax_loss using 32bit floats to\n",
    "#     # avoid numerical instabilities.\n",
    "#     local_w_t = tf.cast(w_t, tf.float32)\n",
    "#     local_b = tf.cast(b, tf.float32)\n",
    "#     local_inputs = tf.cast(logits, tf.float32)\n",
    "#     return tf.nn.sampled_softmax_loss(\n",
    "#         weights=proj_w_t,\n",
    "#         biases=proj_b,\n",
    "#         labels=labels,\n",
    "#         inputs=logits,\n",
    "#         num_sampled=num_sampled,\n",
    "#         num_classes=num_decoder_symbols)\n",
    "\n",
    "# proj_w_t = tf.get_variable(\"proj_w\", [num_decoder_symbols, cell_size], dtype=tf.float32)\n",
    "# proj_w = tf.transpose(proj_w_t)\n",
    "# proj_b = tf.get_variable(\"proj_b\", [num_decoder_symbols], dtype=tf.float32)\n",
    "# output_projection = (proj_w, proj_b)\n",
    "\n",
    "outputs, losses = tf.contrib.legacy_seq2seq.model_with_buckets(\n",
    "    encoder_placeholders, decoder_placeholders, target_placeholders,\n",
    "    target_weights_placeholders, buckets, lambda x, y: seq2seq_f(x, y, False),\n",
    "    softmax_loss_function=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "losses 是个列表, 各元素分别为不同 bucket 的 loss. 用 losses 求和后传给 train_step 会报错, 原因可能是: 较大 bucket 的 losses 需要 feed 更多的 placeholder, 在用较小 bucket 的数据来训练时, 无法提供.\n",
    "\n",
    "实际上不同 bucket 的训练是相对独立的, 因此可以用不同的 loss 传给 optimizer 用于不同 bucket 的训练. 参见 [demon386/tf_bucket_seq2seq/bucketmodel.py](https://github.com/demon386/tf_bucket_seq2seq/blob/master/bucketmodel.py) 之 [L173](https://github.com/demon386/tf_bucket_seq2seq/blob/master/bucketmodel.py#L173), [L243](https://github.com/demon386/tf_bucket_seq2seq/blob/master/bucketmodel.py#L243)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 准备 feed 数据\n",
    "\n",
    "注意 embedding_rnn_seq2seq 接收的 encoder_inputs 形状为 `A list of 1D int32 Tensors of shape [batch_size]`. 为此, 可以用 `list(zip(*lst))` 来对 nested list 进行\"转置\", 得到需要的形状."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def left_shift(decoder_inputs):\n",
    "    \"\"\"generate targets grom decoder_inputs\"\"\"\n",
    "    return [list(input_[1:]) + [_PAD_ID] for input_ in decoder_inputs]\n",
    "\n",
    "def get_bucket_inputs(encoder_data, decoder_data, bucket_id):\n",
    "    encoder_inputs = encoder_data[bucket_id]\n",
    "    decoder_inputs = decoder_data[bucket_id]\n",
    "    return (encoder_inputs, decoder_inputs)\n",
    "\n",
    "def get_batch_inputs(encoder_data, decoder_data, bucket_id, batch_start, batch_size):\n",
    "    encoder_inputs = encoder_data[bucket_id][batch_start : batch_start+batch_size]\n",
    "    decoder_inputs = decoder_data[bucket_id][batch_start : batch_start+batch_size]\n",
    "    return (encoder_inputs, decoder_inputs)\n",
    "\n",
    "def generate_feed_dict(inputs_tuple, encoder_size, decoder_size):\n",
    "    \"\"\"对 inputs 做转置, 并喂给 placeholder 列表, 得到 feed_dict\"\"\"\n",
    "    encoder_inputs, decoder_inputs = inputs_tuple\n",
    "    encoder_inputs = list(zip(*encoder_inputs))\n",
    "    target_inputs = list(zip(*left_shift(decoder_inputs)))\n",
    "    decoder_inputs = list(zip(*decoder_inputs)) \n",
    "    \n",
    "    feed_dict = dict()\n",
    "    # Prepare input data\n",
    "    for i in range(encoder_size):\n",
    "        # 这里用 placeholder 或者 placeholder.name 都可以\n",
    "        feed_dict[encoder_placeholders[i].name] = np.asarray(encoder_inputs[i], dtype=int)\n",
    "    for i in range(decoder_size):\n",
    "        feed_dict[decoder_placeholders[i].name] = np.asarray(decoder_inputs[i], dtype=int)\n",
    "        feed_dict[target_placeholders[i].name] = np.asarray(target_inputs[i], dtype=int)        \n",
    "        # 这里使用 weights 把 <PAD> 的损失屏蔽了\n",
    "        feed_dict[target_weights_placeholders[i].name] = np.asarray(\n",
    "            [float(idx != _PAD_ID) for idx in target_inputs[i]], dtype=float)\n",
    "    return feed_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 训练\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# 训练相关参数\n",
    "epochs = 500\n",
    "print_loss_every = 5\n",
    "learning_rate = 3\n",
    "batch_size = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch bucket-0 bucket-1 bucket-2 \n",
      "\n",
      "   0   5.8737   5.4940   7.9868 \n",
      "   5  16.6137  18.6926  16.6111 \n",
      "  10   8.4404  12.8896  12.0718 \n",
      "  15   5.9407   7.1941   4.9982 \n",
      "  20   4.8046   5.4349   4.6590 \n",
      "  25   2.9296   3.7810   2.8583 \n",
      "  30   1.5386   2.7664   3.4946 \n",
      "  35   1.0132   1.3530   1.6074 \n",
      "  40   0.5756   1.0957   0.6527 \n",
      "  45   0.1591   0.4443   0.3440 \n",
      "  50   0.3563   0.6183   0.3331 \n",
      "  55   0.0729   0.2131   0.2162 \n",
      "  60   0.0763   0.1663   0.1700 \n",
      "  65   0.0556   0.1272   0.1328 \n",
      "  70   0.0520   0.1062   0.1209 \n",
      "  75   0.0447   0.0918   0.0964 \n",
      "  80   0.0397   0.0787   0.0949 \n",
      "  85   0.0386   0.0749   0.0812 \n",
      "  90   0.0378   0.0669   0.0807 \n",
      "  95   0.0379   0.0647   0.0717 \n",
      " 100   0.0364   0.0590   0.0719 \n",
      " 105   0.0365   0.0576   0.0654 \n",
      " 110   0.0350   0.0534   0.0655 \n",
      " 115   0.0353   0.0526   0.0609 \n",
      " 120   0.0337   0.0493   0.0601 \n",
      " 125   0.0342   0.0487   0.0574 \n",
      " 130   0.0327   0.0459   0.0559 \n",
      " 135   0.0334   0.0456   0.0544 \n",
      " 140   0.0319   0.0433   0.0522 \n",
      " 145   0.0327   0.0432   0.0518 \n",
      " 150   0.0313   0.0414   0.0491 \n",
      " 155   0.0323   0.0414   0.0494 \n",
      " 160   0.0308   0.0398   0.0465 \n",
      " 165   0.0320   0.0397   0.0471 \n",
      " 170   0.0304   0.0383   0.0443 \n",
      " 175   0.0317   0.0382   0.0451 \n",
      " 180   0.0300   0.0370   0.0425 \n",
      " 185   0.0314   0.0369   0.0433 \n",
      " 190   0.0297   0.0358   0.0410 \n",
      " 195   0.0311   0.0358   0.0417 \n",
      " 200   0.0294   0.0347   0.0397 \n",
      " 205   0.0308   0.0348   0.0404 \n",
      " 210   0.0292   0.0338   0.0387 \n",
      "KeyboardInterrup\n",
      "Train time: 31.92808771133423 s\n"
     ]
    }
   ],
   "source": [
    "# 把不同 bucket 的 loss 分别传给 optimizer, 得到不同的 train_step. 不知可否?\n",
    "train_steps = [tf.train.AdagradOptimizer(learning_rate).minimize(losses[i]) \n",
    "               for i in range(len(buckets))]\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# Training\n",
    "start_time = time.time()\n",
    "try:\n",
    "    print('Epoch ' + ''.join(['bucket-{} '.format(i) for i in range(len(buckets))]))\n",
    "    for i in range(epochs):\n",
    "        if i % print_loss_every == 0:\n",
    "            print('\\n{: 4d}'.format(i), end=' ')\n",
    "\n",
    "        for bucket_id in range(len(buckets)):\n",
    "            cur_data_size = data_sizes[bucket_id]\n",
    "            if cur_data_size == 0:\n",
    "                continue  # 某个 bucket 为空的特殊情形\n",
    "            encoder_size, decoder_size = buckets[bucket_id]\n",
    "            \n",
    "            # 输出 loss 过程信息\n",
    "            if i % print_loss_every == 0:\n",
    "                bucket_inputs = get_bucket_inputs(encoder_data, decoder_data, bucket_id)\n",
    "                bucket_feed = generate_feed_dict(bucket_inputs, encoder_size, decoder_size)\n",
    "                loss_val = sess.run(losses[bucket_id], feed_dict=bucket_feed)\n",
    "                print('{: 8.4f}'.format(loss_val), end=' ')\n",
    "            \n",
    "            # 训练\n",
    "            for batch_start in range(0, cur_data_size, batch_size):\n",
    "                batch_inputs = get_batch_inputs(\n",
    "                    encoder_data, decoder_data, bucket_id, batch_start, batch_size)\n",
    "                batch_feed = generate_feed_dict(batch_inputs, encoder_size, decoder_size)\n",
    "                sess.run(train_steps[bucket_id], feed_dict=batch_feed)\n",
    "except KeyboardInterrupt:\n",
    "    print('\\nKeyboardInterrup')\n",
    "\n",
    "print('Train time: {} s'.format(time.time() - start_time))\n",
    "# plt.plot(range(0, i, print_loss_every), loss_history);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 测试/decode\n",
    "\n",
    "decoding 过程中, 因为 feed_previous 为 true, 所以 `decoder_inputs` 只用到第一个元素, 后面的都不需要."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "** bucket 0\n",
      " input:  And those simple themes aren ' t really\n",
      "output:  这些 简单 的 话题 确实 不是 <EOS>\n",
      "target:  这些 简单 的 话题 确实 不是 <EOS>\n",
      "\n",
      " input:  and we build from there ,\n",
      "output:  从 这里 出发 ， <EOS>\n",
      "target:  从 这里 出发 ， <EOS>\n",
      "\n",
      " input:  Cannery Row , at the time ,\n",
      "output:  那时 的 坎纳里 鲁夫 ， <EOS>\n",
      "target:  那时 的 坎纳里 鲁夫 ， <EOS>\n",
      "\n",
      " input:  had the biggest industrial\n",
      "output:  有着 西海岸 最大 的 <EOS>\n",
      "target:  有着 西海岸 最大 的 <EOS>\n",
      "\n",
      " input:  canning operation on the west coast .\n",
      "output:  工业化 罐头 工厂 。 <EOS>\n",
      "target:  工业化 罐头 工厂 。 <EOS>\n",
      "\n",
      " input:  We piled enormous amounts of pollution\n",
      "output:  我们 堆积 了 大量 的 污染物 <EOS>\n",
      "target:  我们 堆积 了 大量 的 污染物 <EOS>\n",
      "\n",
      " input:  into the air and into the water .\n",
      "output:  在 空气 中 和 水中 <EOS>\n",
      "target:  在 空气 中 和 水中 <EOS>\n",
      "\n",
      " input:  wrote in the 1940s that\n",
      "output:  在 二十世纪 40 年代 指出 ， <EOS>\n",
      "target:  在 二十世纪 40 年代 指出 ， <EOS>\n",
      "\n",
      " input:  were so bad they turned\n",
      "output:  特别 难闻 <EOS>\n",
      "target:  特别 难闻 <EOS>\n",
      "\n",
      " input:  People working in these canneries\n",
      "output:  工作 在 罐头厂 的 人们 <EOS>\n",
      "target:  工作 在 罐头厂 的 人们 <EOS>\n",
      "\n",
      "\n",
      "** bucket 1\n",
      " input:  It can be a very complicated thing , the ocean .\n",
      "output:  海洋 是 一个 非常复杂 的 事物 。 <EOS>\n",
      "target:  海洋 是 一个 非常复杂 的 事物 。 <EOS>\n",
      "\n",
      " input:  but what I ' m going to try to say is that\n",
      "output:  但 我 但 我 但 我 但 我 但 我 但 我\n",
      "target:  但 我 想要 试图 去 说明 的 是 <EOS>\n",
      "\n",
      " input:  even in that complexity ,\n",
      "output:  即使 是 如此 复杂 的 情况 ， <EOS>\n",
      "target:  即使 是 如此 复杂 的 情况 ， <EOS>\n",
      "\n",
      " input:  there ' s some simple themes that I think ,\n",
      "output:  也 存在 一些 我 认为 简单 的 话题 ， <EOS>\n",
      "target:  也 存在 一些 我 认为 简单 的 话题 ， <EOS>\n",
      "\n",
      " input:  but things that we all pretty well know .\n",
      "output:  而是 一些 我们 都 恰好 知道 的 事情 。 <EOS>\n",
      "target:  而是 一些 我们 都 恰好 知道 的 事情 。 <EOS>\n",
      "\n",
      " input:  And I ' m going to start with this one :\n",
      "output:  接下来 我 就 来说 一个 。 <EOS>\n",
      "target:  接下来 我 就 来说 一个 。 <EOS>\n",
      "\n",
      " input:  And if we just take that\n",
      "output:  接下来 如果 我们 能 理解 这 一点 <EOS>\n",
      "target:  接下来 如果 我们 能 理解 这 一点 <EOS>\n",
      "\n",
      " input:  then we can go to the next step ,\n",
      "output:  可以 得出 下 一步 的 ， <EOS>\n",
      "target:  可以 得出 下 一步 的 ， <EOS>\n",
      "\n",
      " input:  which is that if the ocean ain ' t happy ,\n",
      "output:  那 就是 如果 海洋 不 高兴 了 <EOS>\n",
      "target:  那 就是 如果 海洋 不 高兴 了 <EOS>\n",
      "\n",
      " input:  ain ' t nobody happy .\n",
      "output:  大家 也 都 别 想 开心 。 <EOS>\n",
      "target:  大家 也 都 别 想 开心 。 <EOS>\n",
      "\n",
      "\n",
      "** bucket 2\n",
      " input:  And it can be a very complicated thing , what human health is .\n",
      "output:  人类 的 健康 也 是 一件 非常复杂 的 事情 。 <EOS>\n",
      "target:  人类 的 健康 也 是 一件 非常复杂 的 事情 。 <EOS>\n",
      "\n",
      " input:  And bringing those two together might seem a very daunting task ,\n",
      "output:  将 两者 统一 起来 看起来 是 一件 艰巨 的 任务 。 <EOS>\n",
      "target:  将 两者 统一 起来 看起来 是 一件 艰巨 的 任务 。 <EOS>\n",
      "\n",
      " input:  if we understand , we can really move forward .\n",
      "output:  一些 如果 我们 能 理解 ， 就 很 容易 向前 发展 的 话题 。 <EOS>\n",
      "target:  一些 如果 我们 能 理解 ， 就 很 容易 向前 发展 的 话题 。 <EOS>\n",
      "\n",
      " input:  themes about the complex science of what ' s going on ,\n",
      "output:  有关 那 复杂 的 科学 有 了 怎样 的 发展 ， <EOS>\n",
      "target:  有关 那 复杂 的 科学 有 了 怎样 的 发展 ， <EOS>\n",
      "\n",
      " input:  If momma ain ' t happy , ain ' t nobody happy .\n",
      "output:  如果 老妈 不 高兴 了 ， 大家 都 别 想 开心 。 <EOS>\n",
      "target:  如果 老妈 不 高兴 了 ， 大家 都 别 想 开心 。 <EOS>\n",
      "\n",
      " input:  We know that , right ? We ' ve experienced that .\n",
      "output:  我们 都 知道 ， 不是 吗 ？ 我们 都 经历 过 。 <EOS>\n",
      "target:  我们 都 知道 ， 不是 吗 ？ 我们 都 经历 过 。 <EOS>\n",
      "\n",
      " input:  And we ' re making the ocean pretty unhappy in a lot of different ways .\n",
      "output:  我们 正在 通过 许多 不同 的 方法 惹怒 海洋 。 <EOS>\n",
      "target:  我们 正在 通过 许多 不同 的 方法 惹怒 海洋 。 <EOS>\n",
      "\n",
      " input:  They say , \" You know what you smell ?\n",
      "output:  他们 说 ： “ 你 知道 你 闻到 的 是 什么 吗 ？ <EOS>\n",
      "target:  他们 说 ： “ 你 知道 你 闻到 的 是 什么 吗 ？ <EOS>\n",
      "\n",
      " input:  We made the ocean unhappy ; we made people very unhappy ,\n",
      "output:  让 海洋 不 高兴 ， 人们 也 特别 不 高兴 ， <EOS>\n",
      "target:  让 海洋 不 高兴 ， 人们 也 特别 不 高兴 ， <EOS>\n",
      "\n",
      " input:  Now , when an ecologist looks at the ocean -- I have to tell you --\n",
      "output:  … … 当 一位 生态 学者 看待 海洋 的 时候 — — 我 承认 — — <EOS>\n",
      "target:  现在 … … 当 一位 生态 学者 看待 海洋 的 时候 — — 我 承认 — — <EOS>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def cut_at_eos(sentence):\n",
    "    if _EOS in sentence:        \n",
    "        return sentence[:sentence.index(_EOS)+1]\n",
    "    else:\n",
    "        return sentence\n",
    "\n",
    "def no_prepending_pad(sentence):\n",
    "    for i in range(len(sentence)):\n",
    "        if sentence[i] != _PAD:\n",
    "            return sentence[i:]\n",
    "\n",
    "with variable_scope.variable_scope(variable_scope.get_variable_scope(), reuse=True):\n",
    "    outputs, _ = tf.contrib.legacy_seq2seq.model_with_buckets(\n",
    "        encoder_placeholders, decoder_placeholders, target_placeholders,\n",
    "        target_weights_placeholders, buckets, lambda x, y: seq2seq_f(x, y, True),\n",
    "        softmax_loss_function=None)\n",
    "    \n",
    "    for bucket_id in range(len(buckets)):\n",
    "        print('\\n** bucket {}'.format(bucket_id))\n",
    "        cur_data_size = data_sizes_test[bucket_id]\n",
    "        if cur_data_size == 0:\n",
    "            continue  # 某个 bucket 为空的特殊情形\n",
    "\n",
    "        encoder_size, decoder_size = buckets[bucket_id]\n",
    "        bucket_inputs_test = get_bucket_inputs(encoder_data_test, decoder_data_test, bucket_id)\n",
    "        bucket_feed = generate_feed_dict(bucket_inputs_test, encoder_size, decoder_size)\n",
    "        \n",
    "        output_bucket = np.zeros((cur_data_size, decoder_size), dtype=int)  \n",
    "        # output_bucket 用于记录当前bucket输出值, 形状是 outputs 的\"转置\"\n",
    "    \n",
    "        for i in range(decoder_size):\n",
    "            prob = outputs[bucket_id][i]  # 第i个词的概率输出\n",
    "            output_bucket[:, i] = np.argmax(sess.run(prob, feed_dict=bucket_feed), axis=1)\n",
    "        \n",
    "        # for j in range(cur_data_size):\n",
    "        for j in range(10):  # 只看 bucket 里的前几个句子\n",
    "            sen = [vocab_dec[output_bucket[j, k]] for k in range(decoder_size)]\n",
    "            input_ = [vocab_enc[i] for i in encoder_data_test[bucket_id][j]]\n",
    "            target_ = [vocab_dec[i] for i in decoder_data_test[bucket_id][j][1:]]\n",
    "            input_ = no_prepending_pad(input_)\n",
    "            sen = cut_at_eos(sen)\n",
    "            target_ = cut_at_eos(target_)\n",
    "            print(' input: ', ' '.join(input_))\n",
    "            print('output: ', ' '.join(sen))\n",
    "            print('target: ', ' '.join(target_))\n",
    "            print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 试验\n",
    "\n",
    "读入 1000 句. learning_rate = 3. 约 170 步时, cost 稳定到 0.1-0.2  \n",
    "耗时 1500 * 170/520 = 500 s\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Note\n",
    "\n",
    "难点: \n",
    "- seq2seq 接收的输入形式很特别, 与之前遇到的问题有很大区别:\n",
    "  + 两种输入: encoder_inputs, decoer_inputs.\n",
    "  + decoder_inputs 在训练和生成时用法不同.\n",
    "  + 输入都是不同时间步的 list of tensors, 需要做一次转置.\n",
    "- bucketing 让代码的复杂度增加了一个数量级\n",
    "\n",
    "todo:\n",
    "- [x] 生成时遇到 EOS 就停下来.\n",
    "- 如何准确处理 decoder_input 的长度? +1/-1/shift 之类.\n",
    "- UNK 的处理\n",
    "    - [此文](http://suriyadeepan.github.io/2016-12-31-practical-seq2seq/) 建议 UNK 在语料(不是词表)中的比例控制在 5% 以内.\n",
    "- 计算效率.\n",
    "    - 尝试参照 [models/seq2seq_model.py](https://github.com/tensorflow/models/blob/master/tutorials/rnn/translate/seq2seq_model.py) 实施 sampled loss, 未成功\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## ChangeLog\n",
    "\n",
    "* v.pre\n",
    "    * 感觉大的框架没啥问题. 但翻译结果惨不忍睹. 改进之路漫漫...\n",
    "    * UNK 太多, 需要处理一下.\n",
    "* maybe\n",
    "    * 试试多层 cell\n",
    "    * 试试字模型\n",
    "    * 用 class 组织代码"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "30px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_position": {
    "height": "1351px",
    "left": "0px",
    "right": "1038px",
    "top": "106.989px",
    "width": "135px"
   },
   "toc_section_display": "block",
   "toc_window_display": true,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
